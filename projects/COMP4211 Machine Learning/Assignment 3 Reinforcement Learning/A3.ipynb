{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQhsIwolkbl_"
      },
      "source": [
        "## Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUujcEBVhV_l"
      },
      "source": [
        "The general Bellman equation is:\n",
        "\n",
        "$V^\\pi(s)=r(s,a)+\\gamma V^\\pi(\\delta (s,a))$\n",
        "\n",
        "For our policy $\\pi$ as defined in the question:\n",
        "\n",
        "$r(FACEBOOK,quit)=-1$\n",
        "\n",
        "$r(CLASS1,study)=-2$\n",
        "\n",
        "$r(CLASS2,study)=-2$\n",
        "\n",
        "$r(CLASS3,pub)=1$\n",
        "\n",
        "$V^\\pi(FACEBOOK)=-1+\\gamma V^\\pi(CLASS1)$\n",
        "\n",
        "$V^\\pi(CLASS1)=-2+\\gamma V^\\pi(CLASS2)$\n",
        "\n",
        "$V^\\pi(CLASS2)=-2+\\gamma V^\\pi(CLASS3)$\n",
        "\n",
        "$V^\\pi(CLASS3)=0.2(1+\\gamma V^\\pi(CLASS1))+0.4(1+\\gamma V^\\pi(CLASS2))+0.4(1+\\gamma V^\\pi(CLASS3))$\n",
        "\n",
        "Solving this linear system gives us: $V^\\pi(FACEBOOK)=-6.15041, V^\\pi(CLASS1)=-5.72268, V^\\pi(CLASS2)=-4.13631, V^\\pi(CLASS3)=-2.37368$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5htorVfokRih"
      },
      "source": [
        "## Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIb69Q4hnSmj"
      },
      "source": [
        "Bellman optimality condition is:\n",
        "\n",
        "$V^*(FACEBOOK)= \\max{[r(FACEBOOK,a)+\\gamma V^*(\\delta (FACEBOOK,a))]} = max{[-1+\\gamma V^*(CLASS1), -1+\\gamma V^*(FACEBOOK)]}$\n",
        "\n",
        "$V^*(CLASS1)= \\max{[r(CLASS1,a)+\\gamma V^*(\\delta (CLASS1,a))]} = max{[-2+\\gamma V^*(CLASS2), -1+\\gamma V^*(FACEBOOK)]}$\n",
        "\n",
        "$V^*(CLASS2)= \\max{[r(CLASS2,a)+\\gamma V^*(\\delta (CLASS2,a))]} = max{[-2+\\gamma V^*(CLASS3), \\gamma V^*(SLEEP)]}$\n",
        "\n",
        "$V^*(CLASS3)= \\max{[r(CLASS3,a)+\\gamma V^*(\\delta (CLASS3,a))]} = max{[10+\\gamma V^*(SLEEP), 0.2(1+\\gamma V^*(CLASS1))+0.4(1+\\gamma V^*(CLASS2))+0.4(1+\\gamma V^*(CLASS3))]}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdns5atHhVAR"
      },
      "source": [
        "%matplotlib inline\n",
        "# tells matplotlib to display plots in notebook\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8C0fsPpV1Ln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad9ad7ba-04f9-4209-cfb9-72be5666856f"
      },
      "source": [
        "STATES = ['FACEBOOK','CLASS1','CLASS2','CLASS3','SLEEP'] # we don't include state 'SLEEP' since there are no actions that can be taken at the goal state\n",
        "ACTIONS = ['facebook','quit','study','pub','sleep']\n",
        "\n",
        "# if k actions are available at state S, then this returns an array of size k, where each element consists of:\n",
        "# the action name, the immediate reward of taking that action, and a list of resultant states and the corresponding probabilities for them\n",
        "def get_state_profile(state):\n",
        "  if state=='FACEBOOK':\n",
        "    return [['quit', -1, 'CLASS1', 1], ['facebook', -1, 'FACEBOOK', 1]]\n",
        "  if state=='CLASS1':\n",
        "    return [['study', -2, 'CLASS2', 1], ['facebook', -1, 'FACEBOOK', 1]]\n",
        "  if state=='CLASS2':\n",
        "    return [['study', -2, 'CLASS3', 1], ['sleep', 0, 'SLEEP', 1]]\n",
        "  if state=='CLASS3':\n",
        "    return [['study', 10, 'SLEEP', 1], ['pub', 1, 'CLASS1', 0.2, 'CLASS2', 0.4, 'CLASS3', 0.4]]\n",
        "  if state=='SLEEP':\n",
        "    return [[]]\n",
        "\n",
        "def fix_default_q(q_table):\n",
        "  val = np.NINF # just refers to invalid values\n",
        "  q_table[0,2:]=val # disable all actions besides 'facebook', 'quit' for state FACEBOOK\n",
        "\n",
        "  q_table[1,1]=val # disable all actions besides 'facebook', 'study' for state CLASS1\n",
        "  q_table[1,3:]=val\n",
        "\n",
        "  q_table[2,0:2]=val # disable all actions besides 'sleep', 'study' for state CLASS2\n",
        "  q_table[2,3]=val\n",
        "\n",
        "  q_table[3,0:2]=val\n",
        "  q_table[3,4]=val # disable all actions besides 'pub', 'study' for state CLASS3\n",
        "\n",
        "  q_table[4][:]=0\n",
        "\n",
        "  return q_table\n",
        "\n",
        "\n",
        "V_k = np.ones(shape=(len(STATES))) # randomly initialise as all 0's\n",
        "gamma = 0.9 # discount factor\n",
        "threshold = 0.01\n",
        "counter = 0\n",
        "while True:\n",
        "  V_next = np.zeros(shape=(len(STATES)))\n",
        "  Q = np.zeros(shape=(len(STATES), len(ACTIONS))) # Q(s, a) table stored\n",
        "\n",
        "  for state_num in range(len(STATES)): # iterate through all states\n",
        "    state_profile = get_state_profile(STATES[state_num])\n",
        "\n",
        "    for action_num in range(len(state_profile)): # iterate through all possible actions at each state\n",
        "      if STATES[state_num]=='SLEEP': # don't do computations for state 'SLEEP'\n",
        "        break\n",
        "      this_action_profile = state_profile[action_num]\n",
        "      action = this_action_profile[0]\n",
        "      reward = this_action_profile[1] # since reward is deterministic\n",
        "\n",
        "      for index in range(2,len(state_profile[action_num]),2):\n",
        "        # Q(s,a) = sum (R(s, a) + gamma*V_k) over all possible resultant states\n",
        "        prob = this_action_profile[index+1]\n",
        "        nextstate = STATES.index(this_action_profile[index])\n",
        "        Q[state_num][ACTIONS.index(action)] += prob*(reward+gamma*V_k[nextstate])\n",
        "    V_next[state_num] = np.max(Q[state_num])\n",
        "  \n",
        "  counter += 1\n",
        "  delta = np.linalg.norm(V_k - V_next, ord=1)\n",
        "  V_k = V_next\n",
        "  print(\"No. of iterations: {} | V_k: {} | delta: {}\" .format(counter, V_k, delta))\n",
        "  if delta <= threshold:\n",
        "    break\n",
        "    \n",
        "# output policy\n",
        "print(\"\\n=================\\nFinal Q table:\")\n",
        "Q = fix_default_q(Q)\n",
        "print(Q)\n",
        "print(\"\\n=================\\nOptimal Policy:\")\n",
        "best_actions = np.argmax(Q, axis=1)\n",
        "# optimal policy should be quit, study, study, study \n",
        "for state_num in range(len(STATES)-1):\n",
        "  print(\"STATE: {} | ACTION: {}\" .format(STATES[state_num], ACTIONS[best_actions[state_num]]))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of iterations: 1 | V_k: [ 0.   0.   0.9 10.9  0. ] | delta: 13.0\n",
            "No. of iterations: 2 | V_k: [ 0.    0.    7.81 10.    0.  ] | delta: 7.8100000000000005\n",
            "No. of iterations: 3 | V_k: [ 0.     5.029  7.    10.     0.   ] | delta: 5.839000000000001\n",
            "No. of iterations: 4 | V_k: [ 3.5261  4.3     7.     10.      0.    ] | delta: 4.255100000000001\n",
            "No. of iterations: 5 | V_k: [ 2.87  4.3   7.   10.    0.  ] | delta: 0.6561000000000003\n",
            "No. of iterations: 6 | V_k: [ 2.87  4.3   7.   10.    0.  ] | delta: 0.0\n",
            "\n",
            "=================\n",
            "Final Q table:\n",
            "[[ 1.583  2.87    -inf   -inf   -inf]\n",
            " [ 1.583   -inf  4.3     -inf   -inf]\n",
            " [  -inf   -inf  7.      -inf  0.   ]\n",
            " [  -inf   -inf 10.     7.894   -inf]\n",
            " [ 0.     0.     0.     0.     0.   ]]\n",
            "\n",
            "=================\n",
            "Optimal Policy:\n",
            "STATE: FACEBOOK | ACTION: quit\n",
            "STATE: CLASS1 | ACTION: study\n",
            "STATE: CLASS2 | ACTION: study\n",
            "STATE: CLASS3 | ACTION: study\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wsn8U9EykkOB"
      },
      "source": [
        "The policy our value iteration algorithm produced indeed matches that of the intuitive optimal policy (intuitively, we should try to get to CLASS3 as quickly as possible to claim the +10 reward, and since every connection between the other states involves a negative number, we want to do it as fast as possible. (While CLASS2 -> SLEEP has an immediate reward of 0, the discounted reward of 10 available at CLASS3 far outweighs it. Same with taking action pub at CLASS3 - while the reward of +1 can be claimed infinite times, it is outweighed by the fact if we don't end up in CLASS3, it requires at minimum another -2 to get back to CLASS3.)\n",
        "\n",
        "It can be verified that $V^\\pi(FACEBOOK)=2.87, V^\\pi(CLASS1)=4.3, V^\\pi(CLASS2)=7, V^\\pi(CLASS3)=10$ (assuming $V^\\pi(SLEEP)=0$) is indeed the exact solution to the policy we obtained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZePBSpSJYoD"
      },
      "source": [
        "## Task 3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RdqNhdXd9a_",
        "outputId": "1a2764a8-4a48-4636-cf4c-027082f47bb4"
      },
      "source": [
        "Q_count = 0\n",
        "Q_new = np.array([[-1, -1, 0, 0, 0],\n",
        "                  [-1.9, 0, -2, 0, 0],\n",
        "                  [0, 0, -1.1, 0, 0],\n",
        "                  [0, 0, 10, 1, 0],\n",
        "                  [0, 0, 0, 0, 0]]).astype(\"float32\")\n",
        "\n",
        "Q_new = fix_default_q(Q_new)\n",
        "print(\"==== Initial table: ====\")\n",
        "print(Q_new)\n",
        "\n",
        "def update_q(state, next_state, action, alpha, gamma):\n",
        "  global Q_new\n",
        "  global Q_count\n",
        "  state_profile = get_state_profile(state)\n",
        "  for transition in state_profile:\n",
        "    if transition[0]==action:\n",
        "      rsa = transition[1]\n",
        "      qsa = Q_new[STATES.index(state), ACTIONS.index(action)] # old Q(s, a) value\n",
        "      new_q = qsa + alpha * (rsa + gamma * max(Q_new[STATES.index(next_state), :]) - qsa) # update rule\n",
        "      Q_new[STATES.index(state), ACTIONS.index(action)] = new_q\n",
        "      break\n",
        "  Q_count+=1\n",
        "  print(\"===== Q table after {} iterations ====\" .format(Q_count))\n",
        "  print(Q_new)\n",
        "  \n",
        "\n",
        "update_q('CLASS3','CLASS3','pub',1,gamma)\n",
        "update_q('CLASS3','CLASS2','pub',1,gamma)\n",
        "update_q('CLASS2','CLASS3','study',1,gamma)\n",
        "update_q('CLASS3','CLASS1','pub',1,gamma)\n",
        "update_q('CLASS1','CLASS2','study',1,gamma)\n",
        "update_q('CLASS2','SLEEP','sleep',1,gamma)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==== Initial table: ====\n",
            "[[-1.  -1.  -inf -inf -inf]\n",
            " [-1.9 -inf -2.  -inf -inf]\n",
            " [-inf -inf -1.1 -inf  0. ]\n",
            " [-inf -inf 10.   1.  -inf]\n",
            " [ 0.   0.   0.   0.   0. ]]\n",
            "===== Q table after 1 iterations ====\n",
            "[[-1.  -1.  -inf -inf -inf]\n",
            " [-1.9 -inf -2.  -inf -inf]\n",
            " [-inf -inf -1.1 -inf  0. ]\n",
            " [-inf -inf 10.  10.  -inf]\n",
            " [ 0.   0.   0.   0.   0. ]]\n",
            "===== Q table after 2 iterations ====\n",
            "[[-1.  -1.  -inf -inf -inf]\n",
            " [-1.9 -inf -2.  -inf -inf]\n",
            " [-inf -inf -1.1 -inf  0. ]\n",
            " [-inf -inf 10.   1.  -inf]\n",
            " [ 0.   0.   0.   0.   0. ]]\n",
            "===== Q table after 3 iterations ====\n",
            "[[-1.  -1.  -inf -inf -inf]\n",
            " [-1.9 -inf -2.  -inf -inf]\n",
            " [-inf -inf  7.  -inf  0. ]\n",
            " [-inf -inf 10.   1.  -inf]\n",
            " [ 0.   0.   0.   0.   0. ]]\n",
            "===== Q table after 4 iterations ====\n",
            "[[-1.   -1.    -inf  -inf  -inf]\n",
            " [-1.9   -inf -2.    -inf  -inf]\n",
            " [ -inf  -inf  7.    -inf  0.  ]\n",
            " [ -inf  -inf 10.   -0.71  -inf]\n",
            " [ 0.    0.    0.    0.    0.  ]]\n",
            "===== Q table after 5 iterations ====\n",
            "[[-1.   -1.    -inf  -inf  -inf]\n",
            " [-1.9   -inf  4.3   -inf  -inf]\n",
            " [ -inf  -inf  7.    -inf  0.  ]\n",
            " [ -inf  -inf 10.   -0.71  -inf]\n",
            " [ 0.    0.    0.    0.    0.  ]]\n",
            "===== Q table after 6 iterations ====\n",
            "[[-1.   -1.    -inf  -inf  -inf]\n",
            " [-1.9   -inf  4.3   -inf  -inf]\n",
            " [ -inf  -inf  7.    -inf  0.  ]\n",
            " [ -inf  -inf 10.   -0.71  -inf]\n",
            " [ 0.    0.    0.    0.    0.  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}