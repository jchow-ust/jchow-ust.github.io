{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"COMP4211 Assignment 1 Q4 modified.ipynb","provenance":[],"authorship_tag":"ABX9TyMJjca0IoI1jymW/53IaWVe"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"bdns5atHhVAR","executionInfo":{"status":"ok","timestamp":1602742255175,"user_tz":-480,"elapsed":32929,"user":{"displayName":"Jasper Chow","photoUrl":"https://lh3.googleusercontent.com/-k0QDg-eIJ0g/AAAAAAAAAAI/AAAAAAAAAKo/x9I5CoddE3s/s64/photo.jpg","userId":"06566271520968338357"}},"outputId":"cc1db7b6-6e2b-468a-fa73-2968ba808070","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#%matplotlib inline\n","# IMPORTANT: need this line to show matplotlib plots!\n","\n","# import necessary dependencies and MNIST dataset\n","#import matplotlib.pyplot as plt\n","from sklearn.datasets import fetch_openml\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.model_selection import train_test_split\n","import warnings\n","from sklearn.exceptions import ConvergenceWarning\n","\n","with warnings.catch_warnings():\n","  warnings.filterwarnings(\"ignore\", category=ConvergenceWarning, module=\"sklearn\")\n","\n","\n","# load MNIST dataset\n","X_train, y_train = fetch_openml(name=\"mnist_784\", return_X_y=True) # newest version is implicit. return_X_y parameter specifies returning labels as a separate vector.\n","\n","\n","testSize = 0.2 # could write trainSize = 0.8 as well. This value refers to the percent of data which is partitioned off into the test set and NOT used by the model.\n","randomState = 0  # ensure reproducibility of randomness\n","\n","# Q4a. Here we split the data into 80% training, 20% training, using our random state \"randomState\" to ensure consistency of results\n","X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=testSize, random_state=randomState)\n","\n","\n","print(X_train[0]) # Q4b(i) display first sample\n","print(\"Training set size: {}\" .format(len(y_train))) # Q4b(ii) display number of samples in training set\n","print(\"Testing set size: {}\" .format(len(y_test))) # Q4b(iii) display number of samples in testing set\n","\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.  86. 254. 202.   6.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.  42. 181. 253. 132.   5.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0. 148. 253. 207.  58.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  40.  30.   0.\n","   0.   0.   0.   0.   0. 123. 234. 253. 108.   8.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 139. 240. 226.  17.\n","   0.   0.   0.   0.  35. 233. 253. 192.  10.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.  14.  73. 211. 253. 253.  82.\n","   0.   0.   0.  11. 108. 253. 253. 174.  17.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.  71. 215. 253. 253. 225. 123.   5.\n","   0.   0.   0.  78. 253. 253. 229.  90.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.  56. 194. 253. 253. 179. 105.   0.   0.\n","   0.   0.  10. 136. 253. 253. 134.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   6. 134. 253. 253. 175.  92.   0.   0.   0.\n","   0.   0.  77. 253. 253. 227.  92.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.  71. 253. 253. 253. 187.   0.   0.   0.   0.\n","   0.  70. 249. 253. 230.  43.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.  82. 244. 254. 254. 254. 214. 133. 133.\n"," 133. 198. 254. 254. 208.   4.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.  16. 131. 235. 253. 253. 253. 253.\n"," 253. 254. 253. 253. 214.  31.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.  23.  96. 136. 113. 222.\n"," 253. 254. 253. 173.  32.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 167.\n"," 253. 254. 253. 136.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  99. 212.\n"," 253. 246.  72.  28.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 100. 245. 253.\n"," 253. 207.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  66. 164. 253. 253.\n"," 213.  81.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.  24. 158. 253. 253. 253.\n"," 143.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.  32. 225. 253. 253. 103.\n","   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   3. 196. 213.  86.   1.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n","Training set size: 56000\n","Testing set size: 14000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GwErRZ1NJaBU"},"source":["In some instances (eg importing the same dataset from Keras) we may have to reshape the data, which is 3-dimensional (number of samples * image width * image height) into a 2-dimensional representation. Vectors of size (image_width * image_height) are constructed. In our dataset, both image_width and image_height are equal to 28, so the vectors will have size 784, with each entry in each vector carrying a value from 0-255.\n","\n","This is because the MLPClassifier.fit() function only takes an array of shape (number of samples) * (number of features) and cannot parse 3-dimensional data. Here I will let number of features = number of pixels = 28*28 = 784\n","\n","If we use scikit-learn for this dataset, however, this step is not needed."]},{"cell_type":"code","metadata":{"id":"1v-1cSjAJRvo","executionInfo":{"status":"ok","timestamp":1602742274779,"user_tz":-480,"elapsed":1125,"user":{"displayName":"Jasper Chow","photoUrl":"https://lh3.googleusercontent.com/-k0QDg-eIJ0g/AAAAAAAAAAI/AAAAAAAAAKo/x9I5CoddE3s/s64/photo.jpg","userId":"06566271520968338357"}},"outputId":"98036169-c34c-46ea-e282-a1c803e89686","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["print(\"X_train {}\" .format(X_train.shape))\n","print(\"y_train {}\" .format(y_train.shape))\n","print(\"X_test {}\" .format(X_test.shape))\n","print(\"y_test {}\" .format(y_test.shape))\n","\n","#nsamples, nx, ny = X_train.shape\n","#X_train = X_train.reshape((nsamples,nx*ny))\n","\n","#nsamples_t, nx_t, ny_t = X_test.shape\n","#X_test = X_test.reshape((nsamples_t,nx_t*ny_t))\n","\n","#print(\"New X_train shape {}\" .format(X_train.shape))\n","#print(\"New X_test shape {}\" .format(X_test.shape))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["X_train (56000, 784)\n","y_train (56000,)\n","X_test (14000, 784)\n","y_test (14000,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kp0WzrhxKzHj"},"source":["Here we create our model, based off the Multi-Layer-Perceptron (MLP)."]},{"cell_type":"code","metadata":{"id":"1cBMfg1HKv6q","executionInfo":{"status":"ok","timestamp":1602742348080,"user_tz":-480,"elapsed":71360,"user":{"displayName":"Jasper Chow","photoUrl":"https://lh3.googleusercontent.com/-k0QDg-eIJ0g/AAAAAAAAAAI/AAAAAAAAAKo/x9I5CoddE3s/s64/photo.jpg","userId":"06566271520968338357"}},"outputId":"55762028-de7d-474e-92b9-7958df1ffe18","colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["# we can use different types of functions as activation function for hidden layer, including ReLU, Sigmoid, hyperbolic tan, etc.\n","# I arbitrarily chose for there to be 2 hidden layers: 100 perceptrons in 1st, 30 in 2nd\n","# learning rate decreases slightly with each step if we choose learning_rate=\"invscaling\", and constant as default\n","# max_iter denotes number of epochs (max number of passes through the entire set of data.) Too small and you underfit, too big and you overfit. (also, too big and the model takes too much time.)\n","maxIters=20\n","model = MLPClassifier(hidden_layer_sizes=(200,60), max_iter=maxIters, \n","                      activation=\"relu\", learning_rate=\"constant\", \n","                      random_state=1)\n","\n","# fit data to the model\n","model.fit(X_train, y_train)\n","\n","# Q4c(i) and Q4c(ii) display training/testing accuracy\n","print(\"Training accuracy: {}\" .format(model.score(X_train, y_train)))\n","print(\"Testing accuracy: {}\" .format(model.score(X_test, y_test)))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Training accuracy: 0.9859285714285714\n","Testing accuracy: 0.96\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Yos_PB87Gne6"},"source":["Here we double the number of perceptrons in each hidden layer."]},{"cell_type":"code","metadata":{"id":"FsvHI1pvGmgk","executionInfo":{"status":"ok","timestamp":1602742504612,"user_tz":-480,"elapsed":139834,"user":{"displayName":"Jasper Chow","photoUrl":"https://lh3.googleusercontent.com/-k0QDg-eIJ0g/AAAAAAAAAAI/AAAAAAAAAKo/x9I5CoddE3s/s64/photo.jpg","userId":"06566271520968338357"}},"outputId":"b027c15e-cb9f-4221-ad6f-88a3808ada2e","colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["model2 = MLPClassifier(hidden_layer_sizes=(400,120), max_iter=maxIters, \n","                      activation=\"relu\", learning_rate=\"constant\", \n","                      random_state=1)\n","\n","# fit data to the model\n","model2.fit(X_train, y_train)\n","\n","# display training/testing accuracy\n","print(\"Training accuracy: {}\" .format(model2.score(X_train, y_train)))\n","print(\"Testing accuracy: {}\" .format(model2.score(X_test, y_test)))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Training accuracy: 0.9924464285714286\n","Testing accuracy: 0.9690714285714286\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SYmEjsM1G0vd"},"source":["Here we halve the number of perceptrons in each hidden layer."]},{"cell_type":"code","metadata":{"id":"7TbDsOLHG0W3","executionInfo":{"status":"ok","timestamp":1602742551501,"user_tz":-480,"elapsed":41671,"user":{"displayName":"Jasper Chow","photoUrl":"https://lh3.googleusercontent.com/-k0QDg-eIJ0g/AAAAAAAAAAI/AAAAAAAAAKo/x9I5CoddE3s/s64/photo.jpg","userId":"06566271520968338357"}},"outputId":"6ec71b8d-7800-46cb-b455-0166cd1d276e","colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["model3 = MLPClassifier(hidden_layer_sizes=(100,30), max_iter=maxIters, \n","                      activation=\"relu\", learning_rate=\"constant\", \n","                      random_state=1)\n","\n","# fit data to the model\n","model3.fit(X_train, y_train)\n","\n","# display training/testing accuracy\n","print(\"Training accuracy: {}\" .format(model3.score(X_train, y_train)))\n","print(\"Testing accuracy: {}\" .format(model3.score(X_test, y_test)))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n","  % self.max_iter, ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Training accuracy: 0.9802142857142857\n","Testing accuracy: 0.9565714285714285\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XxDlCr5hHl44"},"source":["From keeping all other parameters the same and only changing the number of nodes in each hidden layer, we can see that **increasing the number of nodes in each hidden layer raises the testing and training accuracy**, while **decreasing the number of nodes in each hidden layer lowers the training and testing accuracy.**"]}]}