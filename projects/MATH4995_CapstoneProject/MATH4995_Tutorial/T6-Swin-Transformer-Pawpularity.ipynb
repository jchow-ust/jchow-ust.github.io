{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# T6 - Swin-Transformer - Pawpularity","metadata":{}},{"cell_type":"markdown","source":"This is an ongoing CV-focused kaggle contest (3 months to go from now, Oct, 2021). And you are getting the chance to win a cash prize!\n\nIn this contest, you will help the website [PetFinder.my](https://petfinder.my/) to give \"Pawpularity\" scores to pet photos, which will help them find their homes faster.\n\nThe \"Pawpularity\" scores in the trainning set is derived from each pet profile's page view statistics at the listing pages, using an algorithm that normalizes the traffic data across different pages, platforms and various metrics.\n\n`Metadata`\n* For each image, you are provided optional metadata, manually labeling each photo for key visual quality and composition parameters.\n\n* These labels are not used for deriving our Pawpularity score, but it may be beneficial for better understanding the content and co-relating them to a photo's attractiveness. Our end goal is to deploy AI solutions that can generate intelligent recommendations (i.e. show a closer frontal pet face, add accessories, increase subject focus, etc) and automatic enhancements (i.e. brightness, contrast) on the photos, so we are hoping to have predictions that are more easily interpretable.\n\n* You may use these labels as you see fit, and optionally build an intermediate / supplementary model to predict the labels from the photos. If your supplementary model is good, we may integrate it into our AI tools as well.\n\n* In our production system, new photos that are dynamically scored will not contain any photo labels. If the Pawpularity prediction model requires photo label scores, we will use an intermediary model to derive such parameters, before feeding them to the final model.\n\n`Evaluation Metrics`\n\n$$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}$$\n\n[PetFinder.my - Pawpularity Contest](https://www.kaggle.com/c/petfinder-pawpularity-score/overview/description)\n\n[Reference](https://www.kaggle.com/phalanx/train-swin-t-pytorch-lightning/notebook)","metadata":{}},{"cell_type":"markdown","source":"In this tutorial, I will build the pipeline and use AlexNet as a demo. When you submit the code to Kaggle, you may encounter error even though you can successfully run and save the notebook. Please refer to Discussion for more information.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## key updates:","metadata":{}},{"cell_type":"markdown","source":"### 1. use new model: swin-transformer\n\n2021 ICCV Best Paper: https://arxiv.org/abs/2103.14030\n\nAn explanation for Swin-Transformer: https://medium.com/codex/swin-transformers-the-most-powerful-tool-in-computer-vision-659f78744871\n\nFor loading the pretrained model, please include the dataset: https://www.kaggle.com/liucong12601/timmswin\n\nThe initial ViT(Vision Transformer) showed promising performance in vision problem but adapting Transformers to fully supplement convolutions was still considered a challenge. Swin transformers on the other hand can model the differences between the two domains such as variations in the scale of objects and the high resolution of pixels in images more efficiently and can serve as a general-purpose pipeline for vision.\n\nThe paper describes Swin Transformers as a hierarchical Transformer whose representation is computed with Shifted WINdows.\n\n![Swin-Transformer](https://miro.medium.com/max/1400/1*KYN2Xg7IUE_YP0ieE6nysw.png)\n\n##### Hierarchical representation\n1.  the input RGB image is split into patches by the patch partition layer. Each patch is 4 x 4 x 3(3 for RGB channels) and is considered a “token”. The patch is subject to a linear embedding layer which projects it to a C dimensional token as in ViT.\n\n2. The main architecture is composed of multiple stages(4 stages for Swin-T) which again is built by connecting a patch merging layer and multiple Swin transformer blocks. The Swin transformer block is based on a modified self-attention. \n\n3. A hierarchical representation is implemented through the patch merging layers. This layer concatenates the features of 2 × 2 neighboring patches which reduce the number of tokens and applies a linear transformation that sets the output dimension by a factor of 2(relative to the input).\n\n![hierachical representation](https://miro.medium.com/max/682/1*KSNFDRw_C-PVt-Hg8ugn8g.png)\n\n##### Shifted Windows\n\n1. The shifted windows approach is based on the observation that standard vision Transformers conduct self-attention on a global receptive field. Therefore, vision transformers have quadratic computational complexity to the number of tokens.\n\n2. The shifted window aims to compute self-attention within local windows. A window contains M × M non-overlapping patches(M=7), and self-attention is calculated in the window. \n\n3.  As illustrated in the figure below, the first module uses standard window configuration to compute self-attention locally from evenly separated windows starting from the top-left pixel. The next Swin transformer block adopts a window configuration that is shifted by (M/2, M/2) pixels from the preceding layer. During the Swin transformer blocks, the network alternates between standard window configuration(W-MSA) and shited window configuration(SW-MSA). This approach introduces connections between neighboring overlapping windows just like how deep convolutions work. \n\n![shifted window approach](https://miro.medium.com/max/828/1*tmXrQOcPcpSjOqx-iQZAXw.png)\n\n\n##### Efficient Shifting\n\n1. For efficient processing of edge windows smaller than M × M, the paper applies cyclic-shifting before computing self-attention as illustrated in the figure below. A masking mechanism is applied to the partitions so that computation is limited within each original window. (This part is hard to understand. Please refer to the official code for better understanding)\n\n![efficient shifting](https://miro.medium.com/max/844/1*7_GOjYTuuJPJPOby6zr1OA.png)","metadata":{}},{"cell_type":"markdown","source":"### 2. new optimizer: AdamW\n\nPytorch doc for AdamW: https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html#torch.optim.AdamW\n\nA very good comparison for Adam and AdamW: https://towardsdatascience.com/why-adamw-matters-736223f31b5d\n\nTry to imagine minimizing a cost function f of a neural network like walking down a hillside in the mountains: You initialize the weights of your network randomly which translates to starting at a random point on the mountain. Your goal is to reach a good minimum of the cost function (the valley) as quickly as possible. Before each step you calculate the gradient ∇ f (determine in which direction the hillside inclines the most) and take a step in the opposite direction\n\n* Adam: Take bigger, more daring steps when walking down a meadow where the gradient does not change much — or smaller steps when climbing down rocks where the gradient constantly changes.\n\n    Adam keeps track of (exponential moving) averages of the gradient (called the first moment, from now on denoted as $m$) and the square of the gradients (called raw second moment, from now on denoted as $v$). When the gradients do not change much ($m$ is close to $\\sqrt{v}$) and “we do not have to be careful walking down the hill”, the step size is of the order of α, if they do ($\\sqrt{v} >> m$) and “we need to be careful not to walk in the wrong direction”, the step size is much smaller.\n    \n* Problem of Adam: The violet term in line 6 shows L2 regularization in Adam (not AdamW) as it is usually implemented in deep learning libraries. The regularization term is added to the cost function which is then derived to calculate the gradients $g$. However, if one adds the weight decay term at this point, the moving averages of the gradient and its square ($m$ and $v$) keep track not only of the gradients of the loss function but also of the regularization term! This means that L2 regularization does not work as intended and is not as effective as with SGD which is why SGD yields models that generalize better and has been used for most state-of-the-art results.\n\n* AdamW: the weight decay is performed only after controlling the parameter-wise step size (see the green term in line 12). The weight decay or regularization term does not end up in the moving averages and is thus only proportional to the weight itself. The authors show experimentally that AdamW yields better training loss and that the models generalize much better than models trained with Adam allowing the new version to compete with stochastic gradient descent with momentum.","metadata":{}},{"cell_type":"markdown","source":"![adam-adamW](https://miro.medium.com/max/1296/1*BOPnuP6VP0JVnJsoCdTo-g.png)","metadata":{}},{"cell_type":"markdown","source":"### 3. use new learning rate scheduler: CosineAnnealingWarmRestarts\n\n* T_0 (int) – Number of iterations for the first restart.\n* T_mult (int, optional) – A factor increases T_{i} after a restart. Default: 1.\n\n* eta_min (float, optional) – Minimum learning rate. Default: 0.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\neta_min = 1e-6\nT_0 = 20\nT_mult = 1\ndemo_model = [torch.tensor([0., 1.])] # I randomly create some parameter for demo\noptimizer = optim.SGD(demo_model, lr=1e-5)\nscheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, eta_min=eta_min, T_0=T_0, T_mult=T_mult)\nlrs = []\nx = range(20)\nfor epoch in x:\n    lrs.append(scheduler.get_lr())\n    ## or we can use:\n    #lrs.append(optimizer.param_groups[0]['lr'])\n    scheduler.step()\nplt.plot(x, lrs)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T12:53:29.506834Z","iopub.execute_input":"2021-11-08T12:53:29.507748Z","iopub.status.idle":"2021-11-08T12:53:31.429699Z","shell.execute_reply.started":"2021-11-08T12:53:29.507645Z","shell.execute_reply":"2021-11-08T12:53:31.428661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. use new loss:\n* Instead of using MSELoss as we did in the last tutorial, here we use BCEWithLogitsLoss. This loss is for `binary classififation` originally. Here we use it for our `regression` prblom. During the training, we firstly transform our targets from [0,100] to [0,1], which make it `like` a binary classification problem: we would like to predict the `pobability that our score is 1`, then we can used the BCEWithLogitsLoss like a binary classification problem.\n    \n* This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.\n\n* During the training, our model will return a numerical value for a given image, then the Sigmod function will transform this value to [0,1]. Then it will calculate the BCELoss bewteen the output of the Sigmod function and the targets. For validation and testing, we not longer need to BCELoss part. We only need to transform the output of the model by the Sigmoid function (between [0,1]), representing \"the probability that this image is of score 1\". Then we multiple it by 100, then we get the Pawpularity in [0,100].\n    \n* The advantange of this loss function over MSELoss is that, if we directly use the output of the model as our predicted Pawpularity, then this value can be negative (which is not reasonable). However, if we use BCEWithLogitsLoss, its embedded Sigmoid function will transform the model output to [0,1], which is a reasonable region for Pawpularity Score (normalized by 100).\n    \n* In the evaluation of the validation set, we still use RMSE to measure our loss function, which is used by the contest.\n    \n    \n","metadata":{}},{"cell_type":"markdown","source":"### 5. mixture:\n\nDuring the training, here we will try to mixing up the training data within a batch to further create \"diversity\" to improve the generalization ability of the model.\n\nGiven a batch of data $(x,y)$ of size (batchsize, channel, image_size, image_size), we do the mixup with probability $p$. During the mixup, we firstly sample a ratio $\\lambda \\sim \\text{Beta}(\\alpha, \\alpha)$. Then we get the mixup results:\n\n$$x_{mix} = \\lambda x + (1-\\lambda) x[\\text{random permutation}, \\text{channel}, \\text{image_size}, \\text{image_size}]$$\n\n$$y_a = y$$\n\n$$y_b = y[\\text{random permutation}]$$\n\nThen we set the loss to be:\n\n$$\\text(loss)_{mix}(x_{mix} ,y_a, y_b) = \\lambda \\text{loss}(M(x_{mix}), y_a) + (1-\\lambda) \\text{loss}(M(x_{mix}), y_b)$$","metadata":{}},{"cell_type":"markdown","source":"### 6. GradCam\n\ngrad_cam: https://github.com/jacobgil/pytorch-grad-cam\n\nexplanation: \n\nhttps://research.fb.com/wp-content/uploads/2017/09/iccv-grad-cam.pdf\n\nhttps://towardsdatascience.com/demystifying-convolutional-neural-networks-using-gradcam-554a85dd4e48\n\n\n\nWhile CNN enable superior performance, their lack of decomposability into intuitive and understandable components makes them hard to interpret. Interpretability of deep learning models matters to build trust and move towards their successful integration in our daily lives. To achieve this goal the model transparency is useful to explain why they predict what they predict.\n\nGrad-Cam, uses the gradient information flowing into the last convolutional layer of the CNN to understand each neuron for a decision of interest.\n\nStep 1. First compute the gradient of the score for the class $c$, $yc$ (before the softmax) with respect to feature maps $A_k$ of a convolutional layer. These gradient flowing back are global average-pooled to obtain the neuron importance weights $a_k$ for the target class.\n\n![calculating_weights_ak](https://miro.medium.com/max/796/1*RE3V1anNLUuYd18NbBuQjg.png)\n\nStep 2. After calculating $a_k$ for the target class $c$, we perform a weighted combination of activation maps and follow it by ReLU.\n\n![Linear Combination](https://miro.medium.com/max/397/1*FqE04KDQukS5h6doLszqNQ.png)\n\nThis results in a coarse heatmap of the same size as that of the convolutional feature maps. We apply ReLU to the linear combination because we are only interested in the features that have a positive influence on the class of interest. Without ReLU, the class activation map highlights more than that is required and hence achieve low localization performance.\n\n($L^c_{Grad-CAM}$ is first up-sampled to the input image resolution using bi-linear interpolation.)\n","metadata":{}},{"cell_type":"markdown","source":"![grad_cam](https://www.statworx.com/wp-content/uploads/CAM_orig_paper-2048x919.png)","metadata":{"execution":{"iopub.status.busy":"2021-11-09T02:03:30.859891Z","iopub.execute_input":"2021-11-09T02:03:30.860229Z","iopub.status.idle":"2021-11-09T02:03:31.541704Z","shell.execute_reply.started":"2021-11-09T02:03:30.860136Z","shell.execute_reply":"2021-11-09T02:03:31.540886Z"}}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# install package by adding dataset\n# you can find the torchsummary package in dataset: https://www.kaggle.com/truthr/torchsummary\n# pytorch_grad_cam dataset in https://www.kaggle.com/zhicongliang/gradcam131\n# ttah 0.0.3: https://www.kaggle.com/dmitrykonovalov/ttach003\n# and I also upload the python-box package in dataset: https://www.kaggle.com/zhicongliang/pythonbox\n# and you can find timm in dataset: https://www.kaggle.com/kozodoi/timm-pytorch-image-models\n# then you can add these datasets to your notebook\n!pip install ../input/torchsummary/torchsummary-1.5.1-py3-none-any.whl\n!pip install ../input/pythonbox/python_box-5.4.1-py3-none-any.whl\n!pip install ../input/timm-pytorch-image-models/pytorch-image-models-master\n!pip install ../input/gradcam131/grad-cam-1.3.1\n!pip install ../input/ttach003/ttach-0.0.3-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:00:42.19281Z","iopub.execute_input":"2021-11-08T13:00:42.193488Z","iopub.status.idle":"2021-11-08T13:00:42.216043Z","shell.execute_reply.started":"2021-11-08T13:00:42.193451Z","shell.execute_reply":"2021-11-08T13:00:42.204991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ../input/gradcam131/grad-cam-1.3.1\n!pip install ../input/ttach003/ttach-0.0.3-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:09:01.3803Z","iopub.execute_input":"2021-11-08T13:09:01.380666Z","iopub.status.idle":"2021-11-08T13:09:31.535517Z","shell.execute_reply.started":"2021-11-08T13:09:01.380627Z","shell.execute_reply":"2021-11-08T13:09:31.533515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tqdm\nfrom PIL import Image\nimport copy\n\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom box import Box\n\nfrom pytorch_grad_cam import GradCAMPlusPlus\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as T\n# https://rwightman.github.io/pytorch-image-models/\nimport timm\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:09:35.688247Z","iopub.execute_input":"2021-11-08T13:09:35.688535Z","iopub.status.idle":"2021-11-08T13:09:41.374701Z","shell.execute_reply.started":"2021-11-08T13:09:35.688501Z","shell.execute_reply":"2021-11-08T13:09:41.373454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 0. Configuration","metadata":{"execution":{"iopub.execute_input":"2021-10-08T11:29:07.628386Z","iopub.status.busy":"2021-10-08T11:29:07.627346Z","iopub.status.idle":"2021-10-08T11:29:07.631719Z","shell.execute_reply":"2021-10-08T11:29:07.630854Z","shell.execute_reply.started":"2021-10-08T11:29:07.628334Z"}}},{"cell_type":"markdown","source":"Here we define a dictionary to store our parameters.","metadata":{}},{"cell_type":"code","source":"config = {\n    'root': '../input/petfinder-pawpularity-score/',\n    'device': 'cuda', # 'cpu' for cpu, 'cuda' for gpu\n    'n_splits': 5,\n    'seed': 2021,\n    'train_batchsize': 64,\n    'val_batchsize': 64,\n    'epoch': 20,\n    'learning_rate': 1e-5,\n    'logger_interval': 1,\n    'model_name': 'swin_tiny_patch4_window7_224',\n    'pretrain_path': '../input/timmswin/swin_tiny_patch4_window7_224.pth',\n    'eta_min': 1e-4,\n    'T_0': 20\n}\n\n# transform key to attribute. it will be easier for us to refer to these parameters later\nconfig = Box(config)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:09:43.212686Z","iopub.execute_input":"2021-11-08T13:09:43.213004Z","iopub.status.idle":"2021-11-08T13:09:43.222423Z","shell.execute_reply.started":"2021-11-08T13:09:43.212967Z","shell.execute_reply":"2021-11-08T13:09:43.220194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 1. Load the data","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"}},{"cell_type":"markdown","source":"If we are using dataset like cifar10, mnist, svhn and etc., we can directly use torchvision.datasets. However, if you would like to use our own data, we need to constrcut a custom Dataset that will help us load the data and perform some basic transformations.\n\nThe most important functions of a custom Dataset is `__len__` and `__getitem__`.\n\nThe `__len__` function will return the number of elements in this dataset, while `__getitem__` will return an image-label pair that can be accepted by pytorch given an index.","metadata":{}},{"cell_type":"code","source":"# define Custom Dataset with pytorch\nclass PetfinderDataset(Dataset):\n\n    def __init__(self, df, image_size=224, transform=None):\n        self._X = df[\"Id\"].values\n        self._y = None\n        if \"Pawpularity\" in df.keys():\n            self._y = df[\"Pawpularity\"].values\n        if not transform:\n            # we resize all the image to the same size\n            self._transform = T.Compose([\n                T.Resize([image_size, image_size]),\n                T.ToTensor(), # transform the PIL image type to torch.tensor\n            ])\n        else:\n            self._transform = transform\n\n    def __len__(self):\n        return len(self._X)\n\n    def __getitem__(self, idx):\n        image_path = self._X[idx]\n        # given the index(path), read the raw image, and then transform it\n        # image = read_image(image_path)  # this require the latest torchvision version\n        image = Image.open(image_path)\n        image = self._transform(image)\n        # if we have label, then we return the image-label pair (for training)\n        # if not, we directly return the image (for testing)\n        if self._y is not None:\n            label = self._y[idx]\n            return image, label\n        return image","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:09:44.118829Z","iopub.execute_input":"2021-11-08T13:09:44.119251Z","iopub.status.idle":"2021-11-08T13:09:44.13076Z","shell.execute_reply.started":"2021-11-08T13:09:44.119207Z","shell.execute_reply":"2021-11-08T13:09:44.12959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(os.path.join(config.root, 'train.csv'))\ndf['Id'] = df[\"Id\"].apply(lambda x: os.path.join(config.root, \"train\", x + \".jpg\")) # we transform the Id to its image path\n\ntrain_val_set = PetfinderDataset(df)\n\nprint('# of data:', len(df))\nprint('range of label [{}, {}]'.format(df['Pawpularity'].min(), df['Pawpularity'].max()))","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:09:44.63917Z","iopub.execute_input":"2021-11-08T13:09:44.641123Z","iopub.status.idle":"2021-11-08T13:09:44.744101Z","shell.execute_reply.started":"2021-11-08T13:09:44.641074Z","shell.execute_reply":"2021-11-08T13:09:44.742743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we show some images here\nplt.figure(figsize=(12, 12))\nfor idx  in range(16):\n    image, label = train_val_set.__getitem__(idx)\n    plt.subplot(4, 4, idx+1)\n    plt.imshow(image.permute(1, 2, 0));\n    plt.axis('off')\n    plt.title('Pawpularity: {}'.format(label))","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:09:44.971385Z","iopub.execute_input":"2021-11-08T13:09:44.971845Z","iopub.status.idle":"2021-11-08T13:09:46.488121Z","shell.execute_reply.started":"2021-11-08T13:09:44.971789Z","shell.execute_reply":"2021-11-08T13:09:46.487164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2. Define Swin-Transformer\n\n","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, name):\n        super(Model, self).__init__()\n        self.backbone = timm.create_model(name, \n                                          pretrained=False, # it would be very easy to set it to true\n                                                            # but in kaggle we could not use internet to download it\n                                          num_classes=0, \n                                          in_chans=3)\n        \n        state_dict = torch.load(config.pretrain_path, map_location=config.device)['model']\n        del state_dict['head.weight'] # in the model, we don't have these two parameters actually\n        del state_dict['head.bias']\n        \n        self.backbone.load_state_dict(state_dict)\n        num_features = self.backbone.num_features\n        self.fc = nn.Sequential(nn.Dropout(0.5),\n                                nn.Linear(num_features, 1)\n                               )\n        \n    def forward(self, x):\n        f = self.backbone(x)\n        out = self.fc(f)\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:09:47.863063Z","iopub.execute_input":"2021-11-08T13:09:47.863347Z","iopub.status.idle":"2021-11-08T13:09:47.873357Z","shell.execute_reply.started":"2021-11-08T13:09:47.863316Z","shell.execute_reply":"2021-11-08T13:09:47.871754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchsummary\n# here we show the summary of the model\nmodel = Model(config.model_name)\ntorchsummary.summary(model, (3,224,224), device='cpu')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-11-08T13:09:49.142272Z","iopub.execute_input":"2021-11-08T13:09:49.142552Z","iopub.status.idle":"2021-11-08T13:09:57.433866Z","shell.execute_reply.started":"2021-11-08T13:09:49.142522Z","shell.execute_reply":"2021-11-08T13:09:57.432911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3. Train Our Model with Cross Validation","metadata":{}},{"cell_type":"code","source":"def test(model, test_loader):\n    model.eval() # turn model into evaluation mode\n    test_loss = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(config.device), target.float().to(config.device)/100.\n            output = model(data)\n            test_loss += F.mse_loss(output.sigmoid().view(-1), \n                                    target.view(-1), reduction='sum').item()  # sum up batch loss\n\n    test_loss /= len(test_loader.dataset)\n    return np.sqrt(test_loss) # RMSE ","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:09:57.436088Z","iopub.execute_input":"2021-11-08T13:09:57.436403Z","iopub.status.idle":"2021-11-08T13:09:57.445046Z","shell.execute_reply.started":"2021-11-08T13:09:57.436359Z","shell.execute_reply":"2021-11-08T13:09:57.443825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mixup(x, y, alpha=1):\n    assert alpha > 0, \"alpha should be larger than 0\"\n    assert x.size(0) > 1, \"Mixup cannot be applied to a single instance.\"\n    \n    lam = np.random.beta(alpha, alpha)\n#     for the shape of lam, run the following two lines\n#     import seaborn as sns\n#     sns.distplot(np.random.beta(0.5,0.5, 1000), bins=100)\n    rand_index = torch.randperm(x.size()[0]) # random permutation of images in the batch x\n    mixed_x = lam * x + (1-lam) * x[rand_index, :]\n    target_a, target_b = y, y[rand_index]\n    return mixed_x, target_a, target_b, lam\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:09:57.446559Z","iopub.execute_input":"2021-11-08T13:09:57.447213Z","iopub.status.idle":"2021-11-08T13:09:57.458194Z","shell.execute_reply.started":"2021-11-08T13:09:57.447167Z","shell.execute_reply":"2021-11-08T13:09:57.456829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we split the dataset into for cross-validation\n# here we treat the label \"Pawpularity\" as categorical data, and use the StratifiedKfol Function\n# actually it is numerical data\nskf = StratifiedKFold(\n    n_splits=config.n_splits, shuffle=True, random_state=config.seed\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:09:57.461002Z","iopub.execute_input":"2021-11-08T13:09:57.46167Z","iopub.status.idle":"2021-11-08T13:09:57.473032Z","shell.execute_reply.started":"2021-11-08T13:09:57.461622Z","shell.execute_reply":"2021-11-08T13:09:57.471853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we keep record of the training in each fold\ntrain_losses_fold = []\nval_losses_fold = []\nbest_model_fold = []\nlearning_rate_fold = []\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(df[\"Id\"], df[\"Pawpularity\"])):\n    \n    print('================================ CV fold {} ================================'.format(fold))\n    \n    train_df = df.loc[train_idx].reset_index(drop=True)\n    val_df = df.loc[val_idx].reset_index(drop=True)\n    \n    # we would like to do some random transformation to our training data such that\n    # our model can be more rubost against different patterns in out-of-sample data\n    train_transform = T.Compose([\n        T.Resize([224, 224]), # crop the image size to 3*224*224\n        T.RandomHorizontalFlip(), # random flip the image horizontally\n        T.RandomVerticalFlip(), # random flip the image vertically\n        T.RandomAffine(15, translate=(0.1, 0.1), scale=(0.9, 1.1)), # Random affine transformation of the image keeping center invariant.\n        T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1), # randomly changes the brightness, saturation, and other properties of an image\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    # in validation set, we only convert our data to torch.float and do a normalization\n    val_transform = T.Compose([\n        T.Resize([224, 224]),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    train_set = PetfinderDataset(train_df, transform=train_transform)\n    val_set = PetfinderDataset(val_df, transform=val_transform)\n    \n    # then we define the dataloader for training and validation\n    # it tells the machine how to sample from our training/validation set\n    train_loader = DataLoader(train_set, batch_size=config.train_batchsize, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_set, batch_size=config.val_batchsize, num_workers=4)\n    \n    model = Model(config.model_name).to(config.device) # use GPU to accelerate the training. Kaggle gives us 30h every week.\n    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate)\n    # we decay the learning rate by factor gamma=0.1 when we reach each milestone epoch\n    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, eta_min=config.eta_min, T_0=config.T_0)\n    # https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n    criterion = nn.BCEWithLogitsLoss()\n    \n    train_losses = []\n    val_losses = []\n    learning_rates = []\n    \n    best_val_loss = np.inf\n    best_model = None\n    \n    for epoch in range(config.epoch):\n        print('\\t=================== Epoch {} ==================='.format(epoch))\n        \n        model.train() # turn model into training mode\n        batch_train_loss = []\n        \n        # iterate each bactch to update the model\n        for batch_idx, (data, target) in tqdm.tqdm(enumerate(train_loader), total=len(train_loader)):\n            data, target = data.to(config.device), target.float().to(config.device) / 100. # we transform the label to [0,1]\n            optimizer.zero_grad() # very important. without this step, grad will accumulate\n            \n            if torch.rand(1)[0] < 0.5:\n                mix_images, target_a, target_b, lam = mixup(data, target, alpha=0.5)\n                output = model(mix_images)\n                loss = lam * criterion(output.view(-1), target_a.view(-1)) + (1-lam) * criterion(output.view(-1), target_b)\n            else:\n                output = model(data)\n                loss = criterion(output.view(-1), target.view(-1))\n                \n            loss.backward()\n            optimizer.step() # update the model by the gradient\n            \n            batch_train_loss.append(loss.item())\n        \n        if epoch % config.logger_interval == 0:\n            train_loss = np.sum(batch_train_loss)/len(train_loader) # BCEWithLogitsLoss loss\n            val_loss = test(model, val_loader) * 100 # RMSE loss\n            \n            train_losses.append(train_loss)\n            val_losses.append(val_loss)\n            \n            print('\\t\\t train loss: {:.4f}'.format(train_loss))\n            print('\\t\\t val loss: {:.4f} -- best loss: {:.4f}'.format(val_loss, best_val_loss))\n            \n            # if we get a lower validation loss, then we record the model\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                best_model = copy.deepcopy(model)\n\n            learning_rates.append(optimizer.param_groups[0]['lr'])\n                  \n        scheduler.step()\n    \n    train_losses_fold.append(train_losses)\n    val_losses_fold.append(val_losses)\n    learning_rate_fold.append(learning_rates)\n    best_model_fold.append(best_model)\n             \n    ","metadata":{"execution":{"iopub.execute_input":"2021-10-12T06:15:58.59171Z","iopub.status.busy":"2021-10-12T06:15:58.591387Z","iopub.status.idle":"2021-10-12T06:19:33.825179Z","shell.execute_reply":"2021-10-12T06:19:33.823727Z","shell.execute_reply.started":"2021-10-12T06:15:58.59167Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### how to save our model in kaggle\n\n1. Save the model by using model.save(\"model_name.h5\") or other similar command. (Make sure to use .h5 extension. That would create a single file for your saved model.) Using this command will save your model in your notebook's memory.\n\n2. Save your notebook by going to Advanced Settings and select Always save output. Hit Save and then select Quick Save if you want your notebook to get saved as it is or otherwise it will run all your notebook and then save it (which might take long depending on your model training phase etc.)\n\n3. Go to notebook viewer (the saved notebook). Go to Output of notebook and create a private (or even public) dataset for that model.\n\n4. Then load that dataset into your any notebook. You can load the model by using model = tf.keras.models.load_model(\"..input/dataset_name/model_name.h5\"). You can even download the model file from dataset for offline purposes.\n\nI did not try the method above. It is just for your reference. https://www.kaggle.com/questions-and-answers/92749","metadata":{}},{"cell_type":"code","source":"## save the models\n\nfor fold, model in enumerate(best_model_fold):\n    torch.save(model.state_dict(), 'swin_transformer_fold_{}.h5'.format(fold))","metadata":{"execution":{"iopub.status.busy":"2021-10-12T06:19:33.826681Z","iopub.status.idle":"2021-10-12T06:19:33.827093Z","shell.execute_reply":"2021-10-12T06:19:33.826896Z","shell.execute_reply.started":"2021-10-12T06:19:33.826872Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # my pretrained weighted: https://www.kaggle.com/zhicongliang/pawpularity-swintransformer\n# ## load the models\n# best_model_fold = []\n# for fold in range(5):\n#     model = Model(config.model_name).to(config.device)\n#     model.load_state_dict(torch.load('../input/pawpularity-swintransformer/swin_transformer_fold_{}.h5'.format(fold), map_location=torch.device(config.device)))\n#     best_model_fold.append(model)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:10:55.157746Z","iopub.execute_input":"2021-11-08T13:10:55.158046Z","iopub.status.idle":"2021-11-08T13:11:04.454241Z","shell.execute_reply.started":"2021-11-08T13:10:55.158015Z","shell.execute_reply":"2021-11-08T13:11:04.453257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## step 4. Visualize the training/validation curve","metadata":{}},{"cell_type":"code","source":"train_losses = np.array(train_losses_fold)\nval_losses = np.array(val_losses_fold)","metadata":{"execution":{"iopub.status.busy":"2021-10-12T06:19:33.830365Z","iopub.status.idle":"2021-10-12T06:19:33.830803Z","shell.execute_reply":"2021-10-12T06:19:33.830601Z","shell.execute_reply.started":"2021-10-12T06:19:33.830579Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index = range(0, config.epoch, config.logger_interval)\nfig = plt.figure(figsize=(16,6))\nplt.subplot(121)\nplt.plot(index, train_losses.mean(axis=0), label='Training Loss')\nplt.subplot(122)\nplt.plot(index, val_losses.mean(axis=0), label='Validation Loss')\nplt.legend(fontsize=15)\nplt.xlabel('Epoch', fontsize=15)","metadata":{"execution":{"iopub.status.busy":"2021-10-12T06:19:33.832154Z","iopub.status.idle":"2021-10-12T06:19:33.832597Z","shell.execute_reply":"2021-10-12T06:19:33.832371Z","shell.execute_reply.started":"2021-10-12T06:19:33.832349Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 5. Visualize the learning rate","metadata":{}},{"cell_type":"code","source":"learning_rate = np.array(learning_rate_fold)\nindex = range(0, config.epoch, config.logger_interval)\nfig = plt.figure(figsize=(8,6))\nplt.plot(index, learning_rate[0,:], label='learning rate')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 6. Grad-Cam","metadata":{}},{"cell_type":"code","source":"# gradcam reshape_transform for vit\ndef reshape_transform(tensor, height=7, width=7):\n    result = tensor.reshape(tensor.size(0),\n                            height, width, tensor.size(2))\n\n    # like in CNNs.\n    result = result.permute(0, 3, 1, 2)\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:11:08.197945Z","iopub.execute_input":"2021-11-08T13:11:08.198836Z","iopub.status.idle":"2021-11-08T13:11:08.205528Z","shell.execute_reply.started":"2021-11-08T13:11:08.198783Z","shell.execute_reply":"2021-11-08T13:11:08.204108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## prepare the images we would like to visualize\n\nfrom copy import deepcopy\n\nnorm_transform = T.Compose([\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n\norg_images = [] # store the original image inside the range [0,1]\nlabels = []     # store the label\nimages = []     # store the normalized images as the input for the model\nfor idx in range(16):\n    image, label = train_val_set.__getitem__(idx)\n    images.append(norm_transform(deepcopy(image)).unsqueeze(0).float().to(config.device))\n    org_images.append(image.unsqueeze(0))\n    labels.append(label)\n    \nimages = torch.cat(images)\norg_images = torch.cat(org_images)\nlabels = torch.tensor(labels)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:11:08.41384Z","iopub.execute_input":"2021-11-08T13:11:08.414427Z","iopub.status.idle":"2021-11-08T13:11:08.696492Z","shell.execute_reply.started":"2021-11-08T13:11:08.414391Z","shell.execute_reply":"2021-11-08T13:11:08.695525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the first well-trained model and do the prediction\nmodel = best_model_fold[0]\nmodel = model.eval().to(config.device)\nlogits = model(images)\npreds = logits.sigmoid().detach().cpu().squeeze(1).numpy() * 100\nlabels = labels.cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:11:10.088818Z","iopub.execute_input":"2021-11-08T13:11:10.089682Z","iopub.status.idle":"2021-11-08T13:11:16.039811Z","shell.execute_reply.started":"2021-11-08T13:11:10.089647Z","shell.execute_reply":"2021-11-08T13:11:16.038867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## use gradcam for visualization\ncam = GradCAMPlusPlus(\n            model=model,\n            target_layer=model.backbone.layers[-1].blocks[-1].norm1, \n            use_cuda=config.device,\n            reshape_transform=reshape_transform)\n\ngrayscale_cams = cam(input_tensor=images, target_category=None, eigen_smooth=True)\norg_images = org_images.numpy().transpose(0, 2, 3, 1)","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:11:32.776803Z","iopub.execute_input":"2021-11-08T13:11:32.777247Z","iopub.status.idle":"2021-11-08T13:11:33.800293Z","shell.execute_reply.started":"2021-11-08T13:11:32.777205Z","shell.execute_reply":"2021-11-08T13:11:33.799037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 12))\nfor it, (image, grayscale_cam, pred, label) in enumerate(zip(org_images, grayscale_cams, preds, labels)):\n    plt.subplot(4, 4, it + 1)\n    visualization = show_cam_on_image(image, grayscale_cam)\n    plt.imshow(visualization)\n    plt.title('pred: {:.1f} label: {}'.format(pred, label))\n    plt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2021-11-08T13:11:48.473577Z","iopub.execute_input":"2021-11-08T13:11:48.474416Z","iopub.status.idle":"2021-11-08T13:11:49.522804Z","shell.execute_reply.started":"2021-11-08T13:11:48.474358Z","shell.execute_reply":"2021-11-08T13:11:49.521774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## step 6. Make submission","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(os.path.join(config.root, 'test.csv'))\ntest_id = df_test.index\ndf_test['Id'] = df_test[\"Id\"].apply(lambda x: os.path.join(config.root, \"test\", x + \".jpg\")) # we transform the Id to its image path\n\ntest_transform = T.Compose([\n    T.Resize([224, 224]),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntest_set = PetfinderDataset(df_test, transform=test_transform)\ntest_loader = DataLoader(test_set, batch_size=config.val_batchsize, num_workers=4)\n\n# get the testing prediction\ntest_pred = np.zeros((df_test.shape[0],1))\n\nfor model in best_model_fold:\n    for batch_idx, data in enumerate(test_loader):\n        data = data.to(config.device)\n        output = model(data)\n        if batch_idx == 0:\n            preds = output.detach().sigmoid().to('cpu').numpy()* 100\n        else:\n            preds = np.vstack((preds, output.sigmoid().detach().to('cpu').numpy()* 100))\n\n    test_pred += preds\n\n# take the average over folds\ntest_pred = test_pred / len(best_model_fold)\n\nsubmission = pd.read_csv(os.path.join(config.root, 'test.csv'))[['Id']]\nsubmission['Pawpularity'] = test_pred\nsubmission","metadata":{"execution":{"iopub.status.busy":"2021-10-12T06:19:33.833797Z","iopub.status.idle":"2021-10-12T06:19:33.834565Z","shell.execute_reply":"2021-10-12T06:19:33.834317Z","shell.execute_reply.started":"2021-10-12T06:19:33.834293Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-12T06:19:33.835809Z","iopub.status.idle":"2021-10-12T06:19:33.836228Z","shell.execute_reply":"2021-10-12T06:19:33.836027Z","shell.execute_reply.started":"2021-10-12T06:19:33.836006Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion: \nThis notebook gives a score 18.27021, which ranks 544/1431 in the leaderboard. (Oct. 29, 2021) ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}