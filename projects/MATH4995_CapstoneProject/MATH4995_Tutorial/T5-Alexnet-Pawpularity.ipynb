{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 - AlexNet - Pawpularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an ongoing CV-focused kaggle contest (3 months to go from now, Oct, 2021). And you are getting the chance to win a cash prize!\n",
    "\n",
    "In this contest, you will help the website [PetFinder.my](https://petfinder.my/) to give \"Pawpularity\" scores to pet photos, which will help them find their homes faster.\n",
    "\n",
    "The \"Pawpularity\" scores in the trainning set is derived from each pet profile's page view statistics at the listing pages, using an algorithm that normalizes the traffic data across different pages, platforms and various metrics.\n",
    "\n",
    "`Metadata`\n",
    "* For each image, you are provided optional metadata, manually labeling each photo for key visual quality and composition parameters.\n",
    "\n",
    "* These labels are not used for deriving our Pawpularity score, but it may be beneficial for better understanding the content and co-relating them to a photo's attractiveness. Our end goal is to deploy AI solutions that can generate intelligent recommendations (i.e. show a closer frontal pet face, add accessories, increase subject focus, etc) and automatic enhancements (i.e. brightness, contrast) on the photos, so we are hoping to have predictions that are more easily interpretable.\n",
    "\n",
    "* You may use these labels as you see fit, and optionally build an intermediate / supplementary model to predict the labels from the photos. If your supplementary model is good, we may integrate it into our AI tools as well.\n",
    "\n",
    "* In our production system, new photos that are dynamically scored will not contain any photo labels. If the Pawpularity prediction model requires photo label scores, we will use an intermediary model to derive such parameters, before feeding them to the final model.\n",
    "\n",
    "`Evaluation Metrics`\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "[PetFinder.my - Pawpularity Contest](https://www.kaggle.com/c/petfinder-pawpularity-score/overview/description)\n",
    "\n",
    "[Reference](https://www.kaggle.com/phalanx/train-swin-t-pytorch-lightning/notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, I will build the pipeline and use AlexNet as a demo. When you submit the code to Kaggle, you may encounter error even though you can successfully run and save the notebook. Please refer to Discussion for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet (2012) \n",
    "https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### History\n",
    "\n",
    "AlexNet was primarily designed by Alex Krizhevsky. It was published with Ilya Sutskever and Krizhevsky’s doctoral advisor Geoffrey Hinton, and is a Convolutional Neural Network or CNN.\n",
    "\n",
    "After competing in ImageNet Large Scale Visual Recognition Challenge, AlexNet shot to fame. It achieved a top-5 error of 15.3%. This was 10.8% lower than that of runner up. \n",
    "\n",
    "The primary result of the original paper was that the depth of the model was absolutely required for its high performance. This was quite expensive computationally but was made feasible due to GPUs or Graphical Processing Units, during training.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture\n",
    "\n",
    "![architecture](https://miro.medium.com/max/1400/1*5bnqbGcBSLzaNMsz5dHkfg.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technique\n",
    "\n",
    "1. ReLU non-linearity:\n",
    "\n",
    "    Using ReLU non-linearity, AlexNet shows us that deep CNN’s can be trained much faster with the help of saturating activation functions such as Tanh or Sigmoid. The figure shown below shows us that with the help of ReLUs(solid curve), AlexNet can achieve a 25% training error rate. This is six times faster than an equivalent network that uses tanh(dotted curve). This was tested on the CIFAR-10 dataset.\n",
    "    \n",
    "    ![ReLU v.s. tanh](https://lh3.googleusercontent.com/yFU1a9lC3c9crWLNm_48DHbkUaWCP2ikC-GdndD58mseiZ4qQVLLnXTSoWJu8cEFTjv8xVMmjNOLz9h8y88-J1dog0vOdZdjNRsDjI1PGeXfx_-zmcnMf9XRGiMhqJWeDu80hDyy)\n",
    "    \n",
    "\n",
    "2. Overlapping Pooling:  \n",
    "\n",
    "    CNNs traditionally “pool” outputs of neighboring groups of neurons with no overlapping. However, when the authors introduced overlap, they saw a reduction in error by about 0.5% and found that models with overlapping pooling generally find it harder to overfit. (the figure below is an example of pooling. For Alexnet's overlap pooling, they use a kernel size=3 and stride=2)\n",
    "\n",
    "    ![max-pool](https://lh4.googleusercontent.com/zAsVIGQRrXN-RQxroXCDXrdhSAMim7MsAdUJja2JV3j5zZAFT7TobX_F85SF2m3y9ifLJaNv8x3LztDvRg4TW30HzX1kQ1PQoZNNEXSDS46jd6nnmNJyLEjxmxZDtI2_Lh4nV8g_)\n",
    "    \n",
    "\n",
    "3. Data Augmentation: \n",
    "\n",
    "    When you show a Neural Net different variation of the same image, it helps prevent overfitting. It also forces the Neural Net to memorize the key features and helps in generating additional data. \n",
    "    \n",
    "    Ramdom Flip: \n",
    "    ![Ramdom Flip](https://lh5.googleusercontent.com/FyGSIFFcz5Rn6KIpxBoIXeDd5zeSjDSbW5uijKPF26vlVHICeVUQ5FEHryWYTnzFdc4UjWrvtLRBAcqOhgpbQ60cinAZnCX8uTKEcvLESDo4fG9VSumOIlXZyiC9FY-JcwLNjYb7)\n",
    "    \n",
    "    Random Crop: \n",
    "    ![Random Crop](https://lh4.googleusercontent.com/LsJ4ckx-M2t-21f-d0gL0UxjvO7EWWuyrktRtwYhQd19naspFYHWF_uoYwYzWqfAkgM-isJpsYmyeVRiOGyyfKBq7X84_PL1qX5bc-dG6Tz4CbF-FIOFXa_562iunhnSWWNJkXGH)\n",
    "    \n",
    "    ColorJitter: \n",
    "    ![ColorJitter](https://pytorch.org/vision/stable/_images/sphx_glr_plot_transforms_006.png)\n",
    "    \n",
    "    RandomAffine:\n",
    "    ![RandomAffine](https://pytorch.org/vision/stable/_images/sphx_glr_plot_transforms_010.png)\n",
    "    \n",
    "4. Dropout: During dropout, a neuron is dropped from the Neural Network with a probability of `rate`. When a neuron is dropped, it does not contribute to forward propagation or backward propagation. Every input goes through a different Neural Network architecture, as shown in the image below. As a result, the learned weight parameters are more robust and do not get overfitted easily.\n",
    "\n",
    "    ![Dropout](https://lh4.googleusercontent.com/i4wnkHE5-KTEYAW4M8SbCMNtzYGirIpkG1XaY1t9tqBbrHTLzHeELOij2_ySJ0sfCMdPwGK2wZXr9_bsnBNhES7mvDmRB2q-8keTwyuC9pk1CFyLswH7ciajlgydPFNaoR4sQPIW)\n",
    "    \n",
    "    \n",
    "reference: https://www.mygreatlearning.com/blog/alexnet-the-first-cnn-to-win-image-net/\n",
    "\n",
    "reference: https://towardsdatascience.com/alexnet-the-architecture-that-challenged-cnns-e406d5297951"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:02:41.575852Z",
     "iopub.status.busy": "2021-10-12T02:02:41.575381Z",
     "iopub.status.idle": "2021-10-12T02:02:56.006323Z",
     "shell.execute_reply": "2021-10-12T02:02:56.005458Z",
     "shell.execute_reply.started": "2021-10-12T02:02:41.575771Z"
    }
   },
   "outputs": [],
   "source": [
    "# install package by adding dataset\n",
    "# you can find the torchsummary package in dataset: https://www.kaggle.com/truthr/torchsummary\n",
    "# and I also upload the python-box package in dataset: https://www.kaggle.com/zhicongliang/pythonbox\n",
    "# then you can add these datasets to your notebook\n",
    "!pip install ../input/torchsummary/torchsummary-1.5.1-py3-none-any.whl\n",
    "!pip install ../input/pythonbox/python_box-5.4.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:02:56.008269Z",
     "iopub.status.busy": "2021-10-12T02:02:56.008033Z",
     "iopub.status.idle": "2021-10-12T02:03:01.252602Z",
     "shell.execute_reply": "2021-10-12T02:03:01.251868Z",
     "shell.execute_reply.started": "2021-10-12T02:02:56.008242Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from box import Box\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "#from torchvision.io import read_image # this require the latest torchvision version\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T11:29:07.628386Z",
     "iopub.status.busy": "2021-10-08T11:29:07.627346Z",
     "iopub.status.idle": "2021-10-08T11:29:07.631719Z",
     "shell.execute_reply": "2021-10-08T11:29:07.630854Z",
     "shell.execute_reply.started": "2021-10-08T11:29:07.628334Z"
    }
   },
   "source": [
    "## Step 0. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a dictionary to store our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:03:01.254087Z",
     "iopub.status.busy": "2021-10-12T02:03:01.253815Z",
     "iopub.status.idle": "2021-10-12T02:03:01.26052Z",
     "shell.execute_reply": "2021-10-12T02:03:01.259588Z",
     "shell.execute_reply.started": "2021-10-12T02:03:01.254055Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'root': '../input/petfinder-pawpularity-score/',\n",
    "    'device': 'cuda', # 'cpu' for cpu, 'cuda' for gpu\n",
    "    'n_splits': 5,\n",
    "    'seed': 2021,\n",
    "    'train_batchsize': 128,\n",
    "    'val_batchsize': 128,\n",
    "    'epoch': 20,\n",
    "    'learning_rate': 0.01,\n",
    "    'logger_interval': 1,\n",
    "    'milestone': [10, 15],\n",
    "    'gamma': 0.1,\n",
    "}\n",
    "\n",
    "# transform key to attribute. it will be easier for us to refer to these parameters later\n",
    "config = Box(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "### Step 1. Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are using dataset like cifar10, mnist, svhn and etc., we can directly use torchvision.datasets. However, if you would like to use our own data, we need to constrcut a custom Dataset that will help us load the data and perform some basic transformations.\n",
    "\n",
    "The most important functions of a custom Dataset is `__len__` and `__getitem__`.\n",
    "\n",
    "The `__len__` function will return the number of elements in this dataset, while `__getitem__` will return an image-label pair that can be accepted by pytorch given an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:03:01.263334Z",
     "iopub.status.busy": "2021-10-12T02:03:01.262912Z",
     "iopub.status.idle": "2021-10-12T02:03:01.274314Z",
     "shell.execute_reply": "2021-10-12T02:03:01.273645Z",
     "shell.execute_reply.started": "2021-10-12T02:03:01.26328Z"
    }
   },
   "outputs": [],
   "source": [
    "# define Custom Dataset with pytorch\n",
    "class PetfinderDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, image_size=224, transform=None):\n",
    "        self._X = df[\"Id\"].values\n",
    "        self._y = None\n",
    "        if \"Pawpularity\" in df.keys():\n",
    "            self._y = df[\"Pawpularity\"].values\n",
    "        if not transform:\n",
    "            # we resize all the image to the same size\n",
    "            self._transform = T.Compose([\n",
    "                T.Resize([image_size, image_size]),\n",
    "                T.ToTensor(), # transform the PIL image type to torch.tensor\n",
    "            ])\n",
    "        else:\n",
    "            self._transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self._X[idx]\n",
    "        # given the index(path), read the raw image, and then transform it\n",
    "        # image = read_image(image_path)  # this require the latest torchvision version\n",
    "        image = Image.open(image_path)\n",
    "        image = self._transform(image)\n",
    "        # if we have label, then we return the image-label pair (for training)\n",
    "        # if not, we directly return the image (for testing)\n",
    "        if self._y is not None:\n",
    "            label = self._y[idx]\n",
    "            return image, label\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:03:01.275795Z",
     "iopub.status.busy": "2021-10-12T02:03:01.275472Z",
     "iopub.status.idle": "2021-10-12T02:03:01.363087Z",
     "shell.execute_reply": "2021-10-12T02:03:01.36241Z",
     "shell.execute_reply.started": "2021-10-12T02:03:01.275761Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(config.root, 'train.csv'))\n",
    "df['Id'] = df[\"Id\"].apply(lambda x: os.path.join(config.root, \"train\", x + \".jpg\")) # we transform the Id to its image path\n",
    "\n",
    "train_val_set = PetfinderDataset(df)\n",
    "\n",
    "print('# of data:', len(df))\n",
    "print('range of label [{}, {}]'.format(df['Pawpularity'].min(), df['Pawpularity'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:03:01.364589Z",
     "iopub.status.busy": "2021-10-12T02:03:01.364209Z",
     "iopub.status.idle": "2021-10-12T02:03:02.739165Z",
     "shell.execute_reply": "2021-10-12T02:03:02.738394Z",
     "shell.execute_reply.started": "2021-10-12T02:03:01.364555Z"
    }
   },
   "outputs": [],
   "source": [
    "# we show some images here\n",
    "plt.figure(figsize=(12, 12))\n",
    "for idx  in range(16):\n",
    "    image, label = train_val_set.__getitem__(idx)\n",
    "    plt.subplot(4, 4, idx+1)\n",
    "    plt.imshow(image.permute(1, 2, 0));\n",
    "    plt.axis('off')\n",
    "    plt.title('Pawpularity: {}'.format(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Define AlexNet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:03:02.740437Z",
     "iopub.status.busy": "2021-10-12T02:03:02.740185Z",
     "iopub.status.idle": "2021-10-12T02:03:02.771994Z",
     "shell.execute_reply": "2021-10-12T02:03:02.771365Z",
     "shell.execute_reply.started": "2021-10-12T02:03:02.740407Z"
    }
   },
   "outputs": [],
   "source": [
    "# AlexNet model architecture from the One weird trick...\n",
    "# <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, droprate=0.2):\n",
    "        super(AlexNet, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            ## conv2d output_shape = (image_shape-kernel_shape+2*padding)/stride + 1\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(droprate),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(droprate),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 1),\n",
    "        )\n",
    "\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def extract_features(self, inputs):\n",
    "        \"\"\" Returns output of the final convolution layer \"\"\"\n",
    "        x = self.features(inputs)\n",
    "        return x\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.features(inputs)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:03:02.774613Z",
     "iopub.status.busy": "2021-10-12T02:03:02.773966Z",
     "iopub.status.idle": "2021-10-12T02:03:04.132649Z",
     "shell.execute_reply": "2021-10-12T02:03:04.131964Z",
     "shell.execute_reply.started": "2021-10-12T02:03:02.774575Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchsummary\n",
    "# here we show the summary of the model\n",
    "model = AlexNet()\n",
    "torchsummary.summary(model, (3,224,224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Train Our Model with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:03:04.134227Z",
     "iopub.status.busy": "2021-10-12T02:03:04.13394Z",
     "iopub.status.idle": "2021-10-12T02:03:04.144049Z",
     "shell.execute_reply": "2021-10-12T02:03:04.143065Z",
     "shell.execute_reply.started": "2021-10-12T02:03:04.134182Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval() # turn model into evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(config.device), target.float().to(config.device)/100.\n",
    "            output = model(data)\n",
    "            test_loss += F.mse_loss(output.view(-1), target.view(-1), reduction='sum').item()  # sum up batch loss\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    return np.sqrt(test_loss) # RMSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:03:04.147461Z",
     "iopub.status.busy": "2021-10-12T02:03:04.14722Z",
     "iopub.status.idle": "2021-10-12T02:03:04.153493Z",
     "shell.execute_reply": "2021-10-12T02:03:04.152718Z",
     "shell.execute_reply.started": "2021-10-12T02:03:04.147431Z"
    }
   },
   "outputs": [],
   "source": [
    "# we split the dataset into for cross-validation\n",
    "# here we treat the label \"Pawpularity\" as categorical data, and use the StratifiedKfol Function\n",
    "# actually it is numerical data\n",
    "skf = StratifiedKFold(\n",
    "    n_splits=config.n_splits, shuffle=True, random_state=config.seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T02:03:04.155083Z",
     "iopub.status.busy": "2021-10-12T02:03:04.154768Z",
     "iopub.status.idle": "2021-10-12T02:03:28.754564Z",
     "shell.execute_reply": "2021-10-12T02:03:28.753224Z",
     "shell.execute_reply.started": "2021-10-12T02:03:04.155049Z"
    }
   },
   "outputs": [],
   "source": [
    "# we keep record of the training in each fold\n",
    "train_losses_fold = []\n",
    "val_losses_fold = []\n",
    "best_model_fold = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df[\"Id\"], df[\"Pawpularity\"])):\n",
    "    \n",
    "    print('================================ CV fold {} ================================'.format(fold))\n",
    "    \n",
    "    train_df = df.loc[train_idx].reset_index(drop=True)\n",
    "    val_df = df.loc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    # we would like to do some random transformation to our training data such that\n",
    "    # our model can be more rubost against different patterns in out-of-sample data\n",
    "    train_transform = T.Compose([\n",
    "        T.Resize([224, 224]), # crop the image size to 3*224*224\n",
    "        T.RandomHorizontalFlip(), # random flip the image horizontally\n",
    "        T.RandomVerticalFlip(), # random flip the image vertically\n",
    "        T.RandomAffine(15, translate=(0.1, 0.1), scale=(0.9, 1.1)), # Random affine transformation of the image keeping center invariant.\n",
    "        T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1), # randomly changes the brightness, saturation, and other properties of an image\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    # in validation set, we only convert our data to torch.float and do a normalization\n",
    "    val_transform = T.Compose([\n",
    "        T.Resize([224, 224]),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    train_set = PetfinderDataset(train_df, transform=train_transform)\n",
    "    val_set = PetfinderDataset(val_df, transform=val_transform)\n",
    "    \n",
    "    # then we define the dataloader for training and validation\n",
    "    # it tells the machine how to sample from our training/validation set\n",
    "    train_loader = DataLoader(train_set, batch_size=config.train_batchsize, num_workers=4)\n",
    "    val_loader = DataLoader(val_set, batch_size=config.val_batchsize, num_workers=4)\n",
    "    \n",
    "    model = AlexNet().to(config.device) # use GPU to accelerate the training. Kaggle gives us 30h every week.\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config.learning_rate)\n",
    "    # we decay the learning rate by factor gamma=0.1 when we reach each milestone epoch\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=config.milestone, gamma=config.gamma)\n",
    "    criterion = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(config.epoch):\n",
    "        print('\\t=================== Epoch {} ==================='.format(epoch))\n",
    "        \n",
    "        model.train() # turn model into training mode\n",
    "        batch_train_loss = []\n",
    "        \n",
    "        # iterate each bactch to update the model\n",
    "        for batch_idx, (data, target) in tqdm.tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            data, target = data.to(config.device), target.float().to(config.device) / 100. # we transform the label to [0,1]\n",
    "            \n",
    "            optimizer.zero_grad() # very important. without this step, grad will accumulate\n",
    "            output = model(data)\n",
    "            \n",
    "            loss = criterion(output.view(-1), target.view(-1)) \n",
    "            rmse = torch.sqrt(loss/len(target)) # rmse loss\n",
    "            rmse.backward()\n",
    "            optimizer.step() # update the model by the gradient\n",
    "            \n",
    "            batch_train_loss.append(loss.item())\n",
    "        \n",
    "        if epoch % config.logger_interval == 0:\n",
    "            train_loss = np.sqrt(np.sum(batch_train_loss)/len(train_loader.dataset)) * 100 # don't forget the final pawpularity range in [1,100]\n",
    "            val_loss = test(model, val_loader) * 100\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            print('\\t\\t train loss: {:.4f}'.format(train_loss))\n",
    "            print('\\t\\t val loss: {:.4f} -- best loss: {:.4f}'.format(val_loss, best_val_loss))\n",
    "            \n",
    "            # if we get a lower validation loss, then we record the model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = copy.deepcopy(model)\n",
    "                  \n",
    "        scheduler.step()\n",
    "    \n",
    "    train_losses_fold.append(train_losses)\n",
    "    val_losses_fold.append(val_losses)\n",
    "    best_model_fold.append(best_model)\n",
    "             \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### how to save our model in kaggle\n",
    "\n",
    "1. Save the model by using model.save(\"model_name.h5\") or other similar command. (Make sure to use .h5 extension. That would create a single file for your saved model.) Using this command will save your model in your notebook's memory.\n",
    "\n",
    "2. Save your notebook by going to Advanced Settings and select Always save output. Hit Save and then select Quick Save if you want your notebook to get saved as it is or otherwise it will run all your notebook and then save it (which might take long depending on your model training phase etc.)\n",
    "\n",
    "3. Go to notebook viewer (the saved notebook). Go to Output of notebook and create a private (or even public) dataset for that model.\n",
    "\n",
    "4. Then load that dataset into your any notebook. You can load the model by using model = tf.keras.models.load_model(\"..input/dataset_name/model_name.h5\"). You can even download the model file from dataset for offline purposes.\n",
    "\n",
    "I did not try the method above. It is just for your reference. https://www.kaggle.com/questions-and-answers/92749"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-12T02:03:28.756234Z",
     "iopub.status.idle": "2021-10-12T02:03:28.756696Z",
     "shell.execute_reply": "2021-10-12T02:03:28.756483Z",
     "shell.execute_reply.started": "2021-10-12T02:03:28.756457Z"
    }
   },
   "outputs": [],
   "source": [
    "## save the models\n",
    "\n",
    "for fold, model in enumerate(best_model_fold):\n",
    "    torch.save(model.state_dict(), 'alexnet_fold_{}.h5'.format(fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # my pretrained alexnet: https://www.kaggle.com/zhicongliang/pawpularityalenext\n",
    "# ## load the models\n",
    "# best_model_fold = []\n",
    "# for fold in range(5):\n",
    "#     model = AlexNet().to(config.device)\n",
    "#     model.load_state_dict(torch.load('../input/pawpularityalenext/alexnet_fold_{}.h5'.format(fold), map_location=torch.device(config.device)))\n",
    "#     best_model_fold.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 4. Visualize the training/validation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-12T02:03:28.757884Z",
     "iopub.status.idle": "2021-10-12T02:03:28.758454Z",
     "shell.execute_reply": "2021-10-12T02:03:28.758223Z",
     "shell.execute_reply.started": "2021-10-12T02:03:28.7582Z"
    }
   },
   "outputs": [],
   "source": [
    "train_losses = np.array(train_losses_fold)\n",
    "val_losses = np.array(val_losses_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-12T02:03:28.759824Z",
     "iopub.status.idle": "2021-10-12T02:03:28.760632Z",
     "shell.execute_reply": "2021-10-12T02:03:28.760408Z",
     "shell.execute_reply.started": "2021-10-12T02:03:28.760379Z"
    }
   },
   "outputs": [],
   "source": [
    "index = range(0, config.epoch, config.logger_interval)\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.plot(index, train_losses.mean(axis=0), label='Training Loss')\n",
    "plt.plot(index, val_losses.mean(axis=0), label='Validation Loss')\n",
    "plt.legend(fontsize=15)\n",
    "plt.xlabel('Epoch', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 5. Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-12T02:03:28.761862Z",
     "iopub.status.idle": "2021-10-12T02:03:28.762704Z",
     "shell.execute_reply": "2021-10-12T02:03:28.76248Z",
     "shell.execute_reply.started": "2021-10-12T02:03:28.762451Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(os.path.join(config.root, 'test.csv'))\n",
    "test_id = df_test.index\n",
    "df_test['Id'] = df_test[\"Id\"].apply(lambda x: os.path.join(config.root, \"test\", x + \".jpg\")) # we transform the Id to its image path\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.Resize([224, 224]),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_set = PetfinderDataset(df_test, transform=test_transform)\n",
    "test_loader = DataLoader(test_set, batch_size=config.val_batchsize, num_workers=4)\n",
    "\n",
    "# get the testing prediction\n",
    "test_pred = np.zeros((df_test.shape[0],1))\n",
    "\n",
    "for model in best_model_fold:\n",
    "    for batch_idx, data in enumerate(test_loader):\n",
    "        data = data.to(config.device)\n",
    "        output = model(data)\n",
    "        if batch_idx == 0:\n",
    "            preds = output.detach().to('cpu').numpy()* 100\n",
    "        else:\n",
    "            preds = np.vstack((preds, output.detach().to('cpu').numpy()* 100))\n",
    "\n",
    "    test_pred += preds\n",
    "\n",
    "# take the average over folds\n",
    "test_pred = test_pred / len(best_model_fold)\n",
    "\n",
    "submission = pd.read_csv(os.path.join(config.root, 'test.csv'))[['Id']]\n",
    "submission['Pawpularity'] = test_pred\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-12T02:03:28.763885Z",
     "iopub.status.idle": "2021-10-12T02:03:28.764746Z",
     "shell.execute_reply": "2021-10-12T02:03:28.764513Z",
     "shell.execute_reply.started": "2021-10-12T02:03:28.764487Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: \n",
    "This notebook gives a score 20.80063, which ranks 770/848 in the leaderboard. (Oct. 12, 2021) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
