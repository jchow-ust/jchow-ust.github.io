{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# T7 - Meta-Swin-Transformer - Pawpularity","metadata":{}},{"cell_type":"markdown","source":"This is an ongoing CV-focused kaggle contest (3 months to go from now, Oct, 2021). And you are getting the chance to win a cash prize!\n\nIn this contest, you will help the website [PetFinder.my](https://petfinder.my/) to give \"Pawpularity\" scores to pet photos, which will help them find their homes faster.\n\nThe \"Pawpularity\" scores in the trainning set is derived from each pet profile's page view statistics at the listing pages, using an algorithm that normalizes the traffic data across different pages, platforms and various metrics.\n\n`Metadata`\n* For each image, you are provided optional metadata, manually labeling each photo for key visual quality and composition parameters.\n\n* These labels are not used for deriving our Pawpularity score, but it may be beneficial for better understanding the content and co-relating them to a photo's attractiveness. Our end goal is to deploy AI solutions that can generate intelligent recommendations (i.e. show a closer frontal pet face, add accessories, increase subject focus, etc) and automatic enhancements (i.e. brightness, contrast) on the photos, so we are hoping to have predictions that are more easily interpretable.\n\n* You may use these labels as you see fit, and optionally build an intermediate / supplementary model to predict the labels from the photos. If your supplementary model is good, we may integrate it into our AI tools as well.\n\n* In our production system, new photos that are dynamically scored will not contain any photo labels. If the Pawpularity prediction model requires photo label scores, we will use an intermediary model to derive such parameters, before feeding them to the final model.\n\n`Evaluation Metrics`\n\n$$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}$$\n\n[PetFinder.my - Pawpularity Contest](https://www.kaggle.com/c/petfinder-pawpularity-score/overview/description)\n\n[Reference](https://www.kaggle.com/phalanx/train-swin-t-pytorch-lightning/notebook)\n\n[Reference](https://www.kaggle.com/abhishek/tez-pawpular-swin-ference)","metadata":{}},{"cell_type":"markdown","source":"### Update","metadata":{}},{"cell_type":"markdown","source":"In this verison, I update the model and dataset module such that it can cooperate the metadata.","metadata":{}},{"cell_type":"markdown","source":"For loading the pretrained model, please include the dataset: https://www.kaggle.com/liucong12601/timmswin","metadata":{}},{"cell_type":"code","source":"# install package by adding dataset\n# and I also upload the python-box package in dataset: https://www.kaggle.com/zhicongliang/pythonbox\n# and you can find timm in dataset: https://www.kaggle.com/kozodoi/timm-pytorch-image-models\n# then you can add these datasets to your notebook\n!pip install ../input/pythonbox/python_box-5.4.1-py3-none-any.whl\n!pip install ../input/timm-pytorch-image-models/pytorch-image-models-master","metadata":{"execution":{"iopub.status.busy":"2021-11-15T11:54:23.842478Z","iopub.execute_input":"2021-11-15T11:54:23.843267Z","iopub.status.idle":"2021-11-15T11:55:27.723031Z","shell.execute_reply.started":"2021-11-15T11:54:23.84317Z","shell.execute_reply":"2021-11-15T11:55:27.721889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tqdm\nfrom PIL import Image\nimport copy\n\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom box import Box\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as T\n# https://rwightman.github.io/pytorch-image-models/\nimport timm\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')","metadata":{"execution":{"iopub.status.busy":"2021-11-15T11:55:27.726739Z","iopub.execute_input":"2021-11-15T11:55:27.727196Z","iopub.status.idle":"2021-11-15T11:55:35.445365Z","shell.execute_reply.started":"2021-11-15T11:55:27.727137Z","shell.execute_reply":"2021-11-15T11:55:35.443854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 0. Configuration","metadata":{"execution":{"iopub.execute_input":"2021-10-08T11:29:07.628386Z","iopub.status.busy":"2021-10-08T11:29:07.627346Z","iopub.status.idle":"2021-10-08T11:29:07.631719Z","shell.execute_reply":"2021-10-08T11:29:07.630854Z","shell.execute_reply.started":"2021-10-08T11:29:07.628334Z"}}},{"cell_type":"markdown","source":"Here we define a dictionary to store our parameters.","metadata":{}},{"cell_type":"code","source":"config = {\n    'root': '../input/petfinder-pawpularity-score/',\n    'device': 'cuda', # 'cpu' for cpu, 'cuda' for gpu\n    'n_splits': 5,\n    'seed': 2021,\n    'train_batchsize': 64,\n    'val_batchsize': 64,\n    'epoch': 20,\n    'learning_rate': 1e-5,\n    'logger_interval': 1,\n    'model_name': 'swin_tiny_patch4_window7_224',\n    'pretrain_path': '../input/timmswin/swin_tiny_patch4_window7_224.pth',\n    'eta_min': 1e-4,\n    'T_0': 20\n}\n\n# transform key to attribute. it will be easier for us to refer to these parameters later\nconfig = Box(config)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T12:02:53.196861Z","iopub.execute_input":"2021-11-15T12:02:53.197173Z","iopub.status.idle":"2021-11-15T12:02:53.2055Z","shell.execute_reply.started":"2021-11-15T12:02:53.197142Z","shell.execute_reply":"2021-11-15T12:02:53.203956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dense_features = [\n    'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n    'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'\n]","metadata":{"execution":{"iopub.status.busy":"2021-11-15T12:02:53.24441Z","iopub.execute_input":"2021-11-15T12:02:53.244927Z","iopub.status.idle":"2021-11-15T12:02:53.249685Z","shell.execute_reply.started":"2021-11-15T12:02:53.244899Z","shell.execute_reply":"2021-11-15T12:02:53.248429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 1. Load the data","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"}},{"cell_type":"markdown","source":"If we are using dataset like cifar10, mnist, svhn and etc., we can directly use torchvision.datasets. However, if you would like to use our own data, we need to constrcut a custom Dataset that will help us load the data and perform some basic transformations.\n\nThe most important functions of a custom Dataset is `__len__` and `__getitem__`.\n\nThe `__len__` function will return the number of elements in this dataset, while `__getitem__` will return an image-label pair that can be accepted by pytorch given an index.","metadata":{}},{"cell_type":"code","source":"# define Custom Dataset with pytorch\nclass PetfinderDataset(Dataset):\n\n    def __init__(self, df, dense_features, image_size=224, transform=None):\n        self._X = df[\"Id\"].values\n        self._features = df[dense_features].values\n        self._y = None\n        if \"Pawpularity\" in df.keys():\n            self._y = df[\"Pawpularity\"].values\n        if not transform:\n            # we resize all the image to the same size\n            self._transform = T.Compose([\n                T.Resize([image_size, image_size]),\n                T.ToTensor(), # transform the PIL image type to torch.tensor\n            ])\n        else:\n            self._transform = transform\n\n    def __len__(self):\n        return len(self._X)\n\n    def __getitem__(self, idx):\n        image_path = self._X[idx]\n        # given the index(path), read the raw image, and then transform it\n        # image = read_image(image_path)  # this require the latest torchvision version\n        image = Image.open(image_path)\n        image = self._transform(image)\n        \n        features = self._features[idx, :]\n        \n        # if we have label, then we return the image-label pair (for training)\n        # if not, we directly return the image (for testing)\n        if self._y is not None:\n            label = self._y[idx]\n            return image, features, label\n        return image, features","metadata":{"execution":{"iopub.status.busy":"2021-11-15T12:02:53.656906Z","iopub.execute_input":"2021-11-15T12:02:53.65726Z","iopub.status.idle":"2021-11-15T12:02:53.666137Z","shell.execute_reply.started":"2021-11-15T12:02:53.657231Z","shell.execute_reply":"2021-11-15T12:02:53.665202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(os.path.join(config.root, 'train.csv'))\ndf['Id'] = df[\"Id\"].apply(lambda x: os.path.join(config.root, \"train\", x + \".jpg\")) # we transform the Id to its image path\n\ntrain_val_set = PetfinderDataset(df, dense_features=dense_features)\n\nprint('# of data:', len(df))\nprint('range of label [{}, {}]'.format(df['Pawpularity'].min(), df['Pawpularity'].max()))","metadata":{"execution":{"iopub.status.busy":"2021-11-15T12:02:53.815697Z","iopub.execute_input":"2021-11-15T12:02:53.816006Z","iopub.status.idle":"2021-11-15T12:02:53.890101Z","shell.execute_reply.started":"2021-11-15T12:02:53.815979Z","shell.execute_reply":"2021-11-15T12:02:53.889098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we show some images here\nplt.figure(figsize=(12, 12))\nfor idx  in range(16):\n    image, features, label = train_val_set.__getitem__(idx)\n    plt.subplot(4, 4, idx+1)\n    plt.imshow(image.permute(1, 2, 0));\n    plt.axis('off')\n    plt.title('Pawpularity: {}'.format(label))","metadata":{"execution":{"iopub.status.busy":"2021-11-15T12:02:53.909032Z","iopub.execute_input":"2021-11-15T12:02:53.909896Z","iopub.status.idle":"2021-11-15T12:02:55.34567Z","shell.execute_reply.started":"2021-11-15T12:02:53.909851Z","shell.execute_reply":"2021-11-15T12:02:55.335011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2. Define Swin-Transformer\n\n","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, name):\n        super(Model, self).__init__()\n        self.backbone = timm.create_model(name, \n                                          pretrained=False, # it would be very easy to set it to true\n                                                            # but in kaggle we could not use internet to download it\n                                          num_classes=0, \n                                          in_chans=3)\n        \n        state_dict = torch.load(config.pretrain_path, map_location=config.device)['model']\n        del state_dict['head.weight'] # in the model, we don't have these two parameters actually\n        del state_dict['head.bias']\n        \n        self.backbone.load_state_dict(state_dict)\n        self.backbone.head = nn.Linear(self.backbone.num_features, 128)\n        self.dropout = nn.Dropout(0.1)\n        self.dense1 = nn.Linear(140, 61)\n        self.dense2 = nn.Linear(61, 1)\n        \n    def forward(self, x, features):\n        x1 = self.backbone(x)\n        \n        x = self.dropout(x1)\n        x = torch.cat([x, features], dim=1)\n        x = self.dense1(x)\n        x = self.dense2(x)\n        \n        return x\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-15T12:02:55.355224Z","iopub.execute_input":"2021-11-15T12:02:55.356263Z","iopub.status.idle":"2021-11-15T12:02:55.384687Z","shell.execute_reply.started":"2021-11-15T12:02:55.356161Z","shell.execute_reply":"2021-11-15T12:02:55.383409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(config.model_name)\nmodel","metadata":{"execution":{"iopub.status.busy":"2021-11-15T12:02:55.390464Z","iopub.execute_input":"2021-11-15T12:02:55.392994Z","iopub.status.idle":"2021-11-15T12:02:56.31528Z","shell.execute_reply.started":"2021-11-15T12:02:55.392948Z","shell.execute_reply":"2021-11-15T12:02:56.314161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3. Train Our Model with Cross Validation","metadata":{}},{"cell_type":"code","source":"def test(model, test_loader):\n    model.eval() # turn model into evaluation mode\n    test_loss = 0\n    with torch.no_grad():\n        for data, features, target in test_loader:\n            data, features, target = data.to(config.device),features.float().to(config.device), target.float().to(config.device)/100.\n            output = model(data, features)\n            test_loss += F.mse_loss(output.sigmoid().view(-1), \n                                    target.view(-1), reduction='sum').item()  # sum up batch loss\n\n    test_loss /= len(test_loader.dataset)\n    return np.sqrt(test_loss) # RMSE ","metadata":{"execution":{"iopub.status.busy":"2021-11-15T12:02:56.317928Z","iopub.execute_input":"2021-11-15T12:02:56.318364Z","iopub.status.idle":"2021-11-15T12:02:56.326725Z","shell.execute_reply.started":"2021-11-15T12:02:56.318298Z","shell.execute_reply":"2021-11-15T12:02:56.325264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mixup(x, features, y, alpha=1):\n    assert alpha > 0, \"alpha should be larger than 0\"\n    assert x.size(0) > 1, \"Mixup cannot be applied to a single instance.\"\n    \n    lam = np.random.beta(alpha, alpha)\n#     for the shape of lam, run the following two lines\n#     import seaborn as sns\n#     sns.distplot(np.random.beta(0.5,0.5, 1000), bins=100)\n    rand_index = torch.randperm(x.size()[0]) # random permutation of images in the batch x\n    mixed_x = lam * x + (1-lam) * x[rand_index, :]\n    mixed_features = lam * features + (1-lam) * features[rand_index, :]\n    target_a, target_b = y, y[rand_index]\n    return mixed_x, mixed_features, target_a, target_b, lam\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-15T12:02:56.328733Z","iopub.execute_input":"2021-11-15T12:02:56.329057Z","iopub.status.idle":"2021-11-15T12:02:56.339346Z","shell.execute_reply.started":"2021-11-15T12:02:56.329006Z","shell.execute_reply":"2021-11-15T12:02:56.337999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we split the dataset into for cross-validation\n# here we treat the label \"Pawpularity\" as categorical data, and use the StratifiedKfol Function\n# actually it is numerical data\nskf = StratifiedKFold(\n    n_splits=config.n_splits, shuffle=True, random_state=config.seed\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T12:02:56.34115Z","iopub.execute_input":"2021-11-15T12:02:56.342236Z","iopub.status.idle":"2021-11-15T12:02:56.353822Z","shell.execute_reply.started":"2021-11-15T12:02:56.342117Z","shell.execute_reply":"2021-11-15T12:02:56.352575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we keep record of the training in each fold\ntrain_losses_fold = []\nval_losses_fold = []\nbest_model_fold = []\nlearning_rate_fold = []\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(df[\"Id\"], df[\"Pawpularity\"])):\n    \n    print('================================ CV fold {} ================================'.format(fold))\n    \n    train_df = df.loc[train_idx].reset_index(drop=True)\n    val_df = df.loc[val_idx].reset_index(drop=True)\n    \n    # we would like to do some random transformation to our training data such that\n    # our model can be more rubost against different patterns in out-of-sample data\n    train_transform = T.Compose([\n        T.Resize([224, 224]), # crop the image size to 3*224*224\n        T.RandomHorizontalFlip(), # random flip the image horizontally\n        T.RandomVerticalFlip(), # random flip the image vertically\n        T.RandomAffine(15, translate=(0.1, 0.1), scale=(0.9, 1.1)), # Random affine transformation of the image keeping center invariant.\n        T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1), # randomly changes the brightness, saturation, and other properties of an image\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    # in validation set, we only convert our data to torch.float and do a normalization\n    val_transform = T.Compose([\n        T.Resize([224, 224]),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    \n    train_set = PetfinderDataset(train_df, transform=train_transform, dense_features=dense_features)\n    val_set = PetfinderDataset(val_df, transform=val_transform, dense_features=dense_features)\n    \n    # then we define the dataloader for training and validation\n    # it tells the machine how to sample from our training/validation set\n    train_loader = DataLoader(train_set, batch_size=config.train_batchsize, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_set, batch_size=config.val_batchsize, num_workers=4)\n    \n    model = Model(config.model_name).to(config.device) # use GPU to accelerate the training. Kaggle gives us 30h every week.\n    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate)\n    # we decay the learning rate by factor gamma=0.1 when we reach each milestone epoch\n    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, eta_min=config.eta_min, T_0=config.T_0)\n    # https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n    criterion = nn.BCEWithLogitsLoss()\n    \n    train_losses = []\n    val_losses = []\n    learning_rates = []\n    \n    best_val_loss = np.inf\n    best_model = None\n    \n    for epoch in range(config.epoch):\n        print('\\t=================== Epoch {} ==================='.format(epoch))\n        \n        model.train() # turn model into training mode\n        batch_train_loss = []\n        \n        # iterate each bactch to update the model\n        for batch_idx, (data, features, target) in tqdm.tqdm(enumerate(train_loader), total=len(train_loader)):\n            data, features, target = data.to(config.device), features.float().to(config.device), target.float().to(config.device) / 100. # we transform the label to [0,1]\n            optimizer.zero_grad() # very important. without this step, grad will accumulate\n            \n            if torch.rand(1)[0] < 0.5:\n                mix_images, mixed_features, target_a, target_b, lam = mixup(data, features, target, alpha=0.5)\n                output = model(mix_images, mixed_features)\n                loss = lam * criterion(output.view(-1), target_a.view(-1)) + (1-lam) * criterion(output.view(-1), target_b)\n            else:\n                output = model(data, features)\n                loss = criterion(output.view(-1), target.view(-1))\n                \n            loss.backward()\n            optimizer.step() # update the model by the gradient\n            \n            batch_train_loss.append(loss.item())\n        \n        if epoch % config.logger_interval == 0:\n            train_loss = np.sum(batch_train_loss)/len(train_loader) # BCEWithLogitsLoss loss\n            val_loss = test(model, val_loader) * 100 # RMSE loss\n            \n            train_losses.append(train_loss)\n            val_losses.append(val_loss)\n            \n            print('\\t\\t train loss: {:.4f}'.format(train_loss))\n            print('\\t\\t val loss: {:.4f} -- best loss: {:.4f}'.format(val_loss, best_val_loss))\n            \n            # if we get a lower validation loss, then we record the model\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                best_model = copy.deepcopy(model)\n\n            learning_rates.append(optimizer.param_groups[0]['lr'])\n                  \n        scheduler.step()\n    \n    train_losses_fold.append(train_losses)\n    val_losses_fold.append(val_losses)\n    learning_rate_fold.append(learning_rates)\n    best_model_fold.append(best_model)\n             \n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-15T12:10:17.717093Z","iopub.execute_input":"2021-11-15T12:10:17.717451Z","iopub.status.idle":"2021-11-15T12:13:11.820762Z","shell.execute_reply.started":"2021-11-15T12:10:17.717377Z","shell.execute_reply":"2021-11-15T12:13:11.817427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### how to save our model in kaggle\n\n1. Save the model by using model.save(\"model_name.h5\") or other similar command. (Make sure to use .h5 extension. That would create a single file for your saved model.) Using this command will save your model in your notebook's memory.\n\n2. Save your notebook by going to Advanced Settings and select Always save output. Hit Save and then select Quick Save if you want your notebook to get saved as it is or otherwise it will run all your notebook and then save it (which might take long depending on your model training phase etc.)\n\n3. Go to notebook viewer (the saved notebook). Go to Output of notebook and create a private (or even public) dataset for that model.\n\n4. Then load that dataset into your any notebook. You can load the model by using model = tf.keras.models.load_model(\"..input/dataset_name/model_name.h5\"). You can even download the model file from dataset for offline purposes.\n\nI did not try the method above. It is just for your reference. https://www.kaggle.com/questions-and-answers/92749","metadata":{}},{"cell_type":"code","source":"## save the models\n\nfor fold, model in enumerate(best_model_fold):\n    torch.save(model.state_dict(), 'meta_swin_transformer_fold_{}.h5'.format(fold))","metadata":{"execution":{"iopub.status.busy":"2021-11-15T12:13:15.363597Z","iopub.execute_input":"2021-11-15T12:13:15.363864Z","iopub.status.idle":"2021-11-15T12:13:15.571424Z","shell.execute_reply.started":"2021-11-15T12:13:15.363836Z","shell.execute_reply":"2021-11-15T12:13:15.570258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # my pretrained weighted: https://www.kaggle.com/zhicongliang/pawpularitymetaswintransformer\n# ## load the models\n# best_model_fold = []\n# for fold in range(5):\n#     model = Model(config.model_name).to(config.device)\n#     model.load_state_dict(torch.load('../input/pawpularitymetaswintransformer/meta_swin_transformer_fold_{}.h5'.format(fold), map_location=torch.device(config.device)))\n#     best_model_fold.append(model)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T12:13:16.788664Z","iopub.execute_input":"2021-11-15T12:13:16.789278Z","iopub.status.idle":"2021-11-15T12:13:16.79352Z","shell.execute_reply.started":"2021-11-15T12:13:16.789247Z","shell.execute_reply":"2021-11-15T12:13:16.792205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## step 4. Visualize the training/validation curve","metadata":{}},{"cell_type":"code","source":"train_losses = np.array(train_losses_fold)\nval_losses = np.array(val_losses_fold)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T12:13:17.00479Z","iopub.execute_input":"2021-11-15T12:13:17.006557Z","iopub.status.idle":"2021-11-15T12:13:17.01143Z","shell.execute_reply.started":"2021-11-15T12:13:17.006513Z","shell.execute_reply":"2021-11-15T12:13:17.010502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index = range(0, config.epoch, config.logger_interval)\nfig = plt.figure(figsize=(16,6))\nplt.subplot(121)\nplt.plot(index, train_losses.mean(axis=0), label='Training Loss')\nplt.subplot(122)\nplt.plot(index, val_losses.mean(axis=0), label='Validation Loss')\nplt.legend(fontsize=15)\nplt.xlabel('Epoch', fontsize=15)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T12:13:17.909878Z","iopub.execute_input":"2021-11-15T12:13:17.910158Z","iopub.status.idle":"2021-11-15T12:13:18.354578Z","shell.execute_reply.started":"2021-11-15T12:13:17.910128Z","shell.execute_reply":"2021-11-15T12:13:18.353665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 5. Visualize the learning rate","metadata":{}},{"cell_type":"code","source":"learning_rate = np.array(learning_rate_fold)\nindex = range(0, config.epoch, config.logger_interval)\nfig = plt.figure(figsize=(8,6))\nplt.plot(index, learning_rate[0,:], label='learning rate')","metadata":{"execution":{"iopub.status.busy":"2021-11-15T12:13:19.629186Z","iopub.execute_input":"2021-11-15T12:13:19.629807Z","iopub.status.idle":"2021-11-15T12:13:19.86532Z","shell.execute_reply.started":"2021-11-15T12:13:19.629772Z","shell.execute_reply":"2021-11-15T12:13:19.864283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## step 6. Make submission","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(os.path.join(config.root, 'test.csv'))\ntest_id = df_test.index\ndf_test['Id'] = df_test[\"Id\"].apply(lambda x: os.path.join(config.root, \"test\", x + \".jpg\")) # we transform the Id to its image path\n\ntest_transform = T.Compose([\n    T.Resize([224, 224]),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntest_set = PetfinderDataset(df_test, dense_features=dense_features, transform=test_transform)\ntest_loader = DataLoader(test_set, batch_size=config.val_batchsize, num_workers=4)\n\n# get the testing prediction\ntest_pred = np.zeros((df_test.shape[0],1))\n\nfor model in best_model_fold:\n    for batch_idx, (data, features) in enumerate(test_loader):\n        data, features = data.float().to(config.device), features.float().to(config.device)\n        output = model(data, features)\n        if batch_idx == 0:\n            preds = output.detach().sigmoid().to('cpu').numpy()* 100\n        else:\n            preds = np.vstack((preds, output.sigmoid().detach().to('cpu').numpy()* 100))\n\n    test_pred += preds\n\n# take the average over folds\ntest_pred = test_pred / len(best_model_fold)\n\nsubmission = pd.read_csv(os.path.join(config.root, 'test.csv'))[['Id']]\nsubmission['Pawpularity'] = test_pred\nsubmission","metadata":{"execution":{"iopub.status.busy":"2021-11-15T12:13:20.838145Z","iopub.execute_input":"2021-11-15T12:13:20.838935Z","iopub.status.idle":"2021-11-15T12:13:21.299837Z","shell.execute_reply.started":"2021-11-15T12:13:20.838735Z","shell.execute_reply":"2021-11-15T12:13:21.298853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-15T12:13:21.985607Z","iopub.execute_input":"2021-11-15T12:13:21.986101Z","iopub.status.idle":"2021-11-15T12:13:21.996819Z","shell.execute_reply.started":"2021-11-15T12:13:21.986033Z","shell.execute_reply":"2021-11-15T12:13:21.995465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion: \n ","metadata":{}},{"cell_type":"markdown","source":"This notebook give a score of 18.31227, which is worse than the previous one of 18.27021.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}