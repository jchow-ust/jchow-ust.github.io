{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# T8 - SVR-Boost - Meta-Swin-Transformer - Pawpularity","metadata":{}},{"cell_type":"markdown","source":"This is an ongoing CV-focused kaggle contest (3 months to go from now, Oct, 2021). And you are getting the chance to win a cash prize!\n\nIn this contest, you will help the website [PetFinder.my](https://petfinder.my/) to give \"Pawpularity\" scores to pet photos, which will help them find their homes faster.\n\nThe \"Pawpularity\" scores in the trainning set is derived from each pet profile's page view statistics at the listing pages, using an algorithm that normalizes the traffic data across different pages, platforms and various metrics.\n\n`Metadata`\n* For each image, you are provided optional metadata, manually labeling each photo for key visual quality and composition parameters.\n\n* These labels are not used for deriving our Pawpularity score, but it may be beneficial for better understanding the content and co-relating them to a photo's attractiveness. Our end goal is to deploy AI solutions that can generate intelligent recommendations (i.e. show a closer frontal pet face, add accessories, increase subject focus, etc) and automatic enhancements (i.e. brightness, contrast) on the photos, so we are hoping to have predictions that are more easily interpretable.\n\n* You may use these labels as you see fit, and optionally build an intermediate / supplementary model to predict the labels from the photos. If your supplementary model is good, we may integrate it into our AI tools as well.\n\n* In our production system, new photos that are dynamically scored will not contain any photo labels. If the Pawpularity prediction model requires photo label scores, we will use an intermediary model to derive such parameters, before feeding them to the final model.\n\n`Evaluation Metrics`\n\n$$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}$$\n\n[PetFinder.my - Pawpularity Contest](https://www.kaggle.com/c/petfinder-pawpularity-score/overview/description)\n\n[Reference](https://www.kaggle.com/cdeotte/rapids-svr-boost-17-8)\n","metadata":{}},{"cell_type":"markdown","source":"### Update","metadata":{}},{"cell_type":"markdown","source":"In this verison, I combine the prediction of swin-transform and SVR (please see the reference for more details).","metadata":{}},{"cell_type":"markdown","source":"# How to Add RAPIDS SVR Head\nThere are 3 steps to building a double headed model. The first step is to train your Image NN backbone and head. The next step is to train our RAPIDS SVR head with extracted embeddings from frozen Image NN backbone. Lastly, we infer with both heads and average the predictions.\n\nThe SVR here is imported from a package called [cuML](https://docs.rapids.ai/api/cuml/stable/), which is developed by the [RAPIDS project](https://rapids.ai/). Comparing with sklearn, cuML is a suite of libraries that implement machine learning algorithms that can be run on GPU.\n\n\n![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Oct-2021/st1.png)\n![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Oct-2021/st2.png)\n![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Oct-2021/st3.png)\n\n[1]: https://www.kaggle.com/abhishek/tez-pawpular-swin-ference\n[2]: https://www.kaggle.com/cdeotte/rapids-svr-boost-17-8?scriptVersionId=76282086","metadata":{"execution":{"iopub.status.busy":"2021-11-23T09:20:56.599687Z","iopub.execute_input":"2021-11-23T09:20:56.600084Z","iopub.status.idle":"2021-11-23T09:20:56.62437Z","shell.execute_reply.started":"2021-11-23T09:20:56.600039Z","shell.execute_reply":"2021-11-23T09:20:56.622645Z"}}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# install package by adding dataset\n# and I also upload the python-box package in dataset: https://www.kaggle.com/zhicongliang/pythonbox\n# and you can find timm in dataset: https://www.kaggle.com/kozodoi/timm-pytorch-image-models\n# then you can add these datasets to your notebook\n!pip install ../input/pythonbox/python_box-5.4.1-py3-none-any.whl\n!pip install ../input/timm-pytorch-image-models/pytorch-image-models-master","metadata":{"execution":{"iopub.status.busy":"2021-11-23T10:07:48.346039Z","iopub.execute_input":"2021-11-23T10:07:48.34643Z","iopub.status.idle":"2021-11-23T10:08:48.055893Z","shell.execute_reply.started":"2021-11-23T10:07:48.346348Z","shell.execute_reply":"2021-11-23T10:08:48.055064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tqdm\nfrom PIL import Image\nimport copy\n\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom box import Box\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as T\n# https://rwightman.github.io/pytorch-image-models/\nimport timm\n\n# SVR from RAPIDS\nfrom cuml.svm import SVR\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')","metadata":{"execution":{"iopub.status.busy":"2021-11-23T10:08:48.058564Z","iopub.execute_input":"2021-11-23T10:08:48.059142Z","iopub.status.idle":"2021-11-23T10:08:57.87036Z","shell.execute_reply.started":"2021-11-23T10:08:48.059103Z","shell.execute_reply":"2021-11-23T10:08:57.869549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 0. Configuration","metadata":{"execution":{"iopub.execute_input":"2021-10-08T11:29:07.628386Z","iopub.status.busy":"2021-10-08T11:29:07.627346Z","iopub.status.idle":"2021-10-08T11:29:07.631719Z","shell.execute_reply":"2021-10-08T11:29:07.630854Z","shell.execute_reply.started":"2021-10-08T11:29:07.628334Z"}}},{"cell_type":"markdown","source":"Here we define a dictionary to store our parameters.","metadata":{}},{"cell_type":"code","source":"config = {\n    'root': '../input/petfinder-pawpularity-score/',\n    'device': 'cuda', # 'cpu' for cpu, 'cuda' for gpu\n    'n_splits': 5,\n    'seed': 2021,\n    'train_batchsize': 64,\n    'val_batchsize': 64,\n    'epoch': 20,\n    'learning_rate': 1e-5,\n    'logger_interval': 1,\n    'model_name': 'swin_tiny_patch4_window7_224',\n    'pretrain_path': '../input/timmswin/swin_tiny_patch4_window7_224.pth',\n    'eta_min': 1e-4,\n    'T_0': 20\n}\n\n# transform key to attribute. it will be easier for us to refer to these parameters later\nconfig = Box(config)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T10:08:57.871621Z","iopub.execute_input":"2021-11-23T10:08:57.871915Z","iopub.status.idle":"2021-11-23T10:08:57.88022Z","shell.execute_reply.started":"2021-11-23T10:08:57.871877Z","shell.execute_reply":"2021-11-23T10:08:57.879472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dense_features = [\n    'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n    'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'\n]","metadata":{"execution":{"iopub.status.busy":"2021-11-23T10:08:57.882557Z","iopub.execute_input":"2021-11-23T10:08:57.882847Z","iopub.status.idle":"2021-11-23T10:08:57.891839Z","shell.execute_reply.started":"2021-11-23T10:08:57.882813Z","shell.execute_reply":"2021-11-23T10:08:57.891136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 1. Load the data","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"}},{"cell_type":"markdown","source":"If we are using dataset like cifar10, mnist, svhn and etc., we can directly use torchvision.datasets. However, if you would like to use our own data, we need to constrcut a custom Dataset that will help us load the data and perform some basic transformations.\n\nThe most important functions of a custom Dataset is `__len__` and `__getitem__`.\n\nThe `__len__` function will return the number of elements in this dataset, while `__getitem__` will return an image-label pair that can be accepted by pytorch given an index.","metadata":{}},{"cell_type":"code","source":"# define Custom Dataset with pytorch\nclass PetfinderDataset(Dataset):\n\n    def __init__(self, df, dense_features, image_size=224, transform=None):\n        self._X = df[\"Id\"].values\n        self._features = df[dense_features].values\n        self._y = None\n        if \"Pawpularity\" in df.keys():\n            self._y = df[\"Pawpularity\"].values\n        if not transform:\n            # we resize all the image to the same size\n            self._transform = T.Compose([\n                T.Resize([image_size, image_size]),\n                T.ToTensor(), # transform the PIL image type to torch.tensor\n            ])\n        else:\n            self._transform = transform\n\n    def __len__(self):\n        return len(self._X)\n\n    def __getitem__(self, idx):\n        image_path = self._X[idx]\n        # given the index(path), read the raw image, and then transform it\n        # image = read_image(image_path)  # this require the latest torchvision version\n        image = Image.open(image_path)\n        image = self._transform(image)\n        \n        features = self._features[idx, :]\n        \n        # if we have label, then we return the image-label pair (for training)\n        # if not, we directly return the image (for testing)\n        if self._y is not None:\n            label = self._y[idx]\n            return image, features, label\n        return image, features","metadata":{"execution":{"iopub.status.busy":"2021-11-23T10:08:57.893143Z","iopub.execute_input":"2021-11-23T10:08:57.893433Z","iopub.status.idle":"2021-11-23T10:08:57.904146Z","shell.execute_reply.started":"2021-11-23T10:08:57.893396Z","shell.execute_reply":"2021-11-23T10:08:57.903108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(os.path.join(config.root, 'train.csv'))\ndf['Id'] = df[\"Id\"].apply(lambda x: os.path.join(config.root, \"train\", x + \".jpg\")) # we transform the Id to its image path\n\ntrain_val_set = PetfinderDataset(df, dense_features=dense_features)\n\nprint('# of data:', len(df))\nprint('range of label [{}, {}]'.format(df['Pawpularity'].min(), df['Pawpularity'].max()))","metadata":{"execution":{"iopub.status.busy":"2021-11-23T10:08:57.90561Z","iopub.execute_input":"2021-11-23T10:08:57.906031Z","iopub.status.idle":"2021-11-23T10:08:57.991619Z","shell.execute_reply.started":"2021-11-23T10:08:57.905994Z","shell.execute_reply":"2021-11-23T10:08:57.990897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we show some images here\nplt.figure(figsize=(12, 12))\nfor idx  in range(16):\n    image, features, label = train_val_set.__getitem__(idx)\n    plt.subplot(4, 4, idx+1)\n    plt.imshow(image.permute(1, 2, 0));\n    plt.axis('off')\n    plt.title('Pawpularity: {}'.format(label))","metadata":{"execution":{"iopub.status.busy":"2021-11-23T10:08:57.993148Z","iopub.execute_input":"2021-11-23T10:08:57.993664Z","iopub.status.idle":"2021-11-23T10:08:59.199991Z","shell.execute_reply.started":"2021-11-23T10:08:57.993628Z","shell.execute_reply":"2021-11-23T10:08:59.198774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2. Define Swin-Transformer\n\n","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, name):\n        super(Model, self).__init__()\n        self.backbone = timm.create_model(name, \n                                          pretrained=False, # it would be very easy to set it to true\n                                                            # but in kaggle we could not use internet to download it\n                                          num_classes=0, \n                                          in_chans=3)\n        \n        state_dict = torch.load(config.pretrain_path, map_location=config.device)['model']\n        del state_dict['head.weight'] # in the model, we don't have these two parameters actually\n        del state_dict['head.bias']\n        \n        self.backbone.load_state_dict(state_dict)\n        self.backbone.head = nn.Linear(self.backbone.num_features, 128)\n        self.dropout = nn.Dropout(0.1)\n        self.dense1 = nn.Linear(140, 61)\n        self.dense2 = nn.Linear(61, 1)\n        \n    def forward(self, x, features):\n        x1 = self.backbone(x) # image embedding\n        \n        x = self.dropout(x1)\n        x = torch.cat([x, features], dim=1)\n        x = self.dense1(x)\n        x = self.dense2(x)\n        \n        # return the intermediate result\n        x = torch.cat([x, x1, features], dim=1)\n        return x\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-23T10:08:59.200944Z","iopub.execute_input":"2021-11-23T10:08:59.201162Z","iopub.status.idle":"2021-11-23T10:08:59.214764Z","shell.execute_reply.started":"2021-11-23T10:08:59.201133Z","shell.execute_reply":"2021-11-23T10:08:59.213832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(config.model_name)\nmodel","metadata":{"execution":{"iopub.status.busy":"2021-11-23T10:08:59.216472Z","iopub.execute_input":"2021-11-23T10:08:59.217186Z","iopub.status.idle":"2021-11-23T10:09:08.847456Z","shell.execute_reply.started":"2021-11-23T10:08:59.21715Z","shell.execute_reply":"2021-11-23T10:09:08.846671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3. Train Our Model with Cross Validation","metadata":{"execution":{"iopub.status.busy":"2021-11-23T07:21:53.395203Z","iopub.execute_input":"2021-11-23T07:21:53.395581Z","iopub.status.idle":"2021-11-23T07:21:53.399711Z","shell.execute_reply.started":"2021-11-23T07:21:53.395535Z","shell.execute_reply":"2021-11-23T07:21:53.398825Z"}}},{"cell_type":"code","source":"def test(model, test_loader):\n    model.eval() # turn model into evaluation mode\n    test_loss = 0\n    with torch.no_grad():\n        for data, features, target in test_loader:\n            data, features, target = data.to(config.device),features.float().to(config.device), target.float().to(config.device)/100.\n            output = model(data, features)\n            test_loss += F.mse_loss(output.sigmoid().view(-1), \n                                    target.view(-1), reduction='sum').item()  # sum up batch loss\n\n    test_loss /= len(test_loader.dataset)\n    return np.sqrt(test_loss) # RMSE ","metadata":{"execution":{"iopub.status.busy":"2021-11-23T10:09:08.849909Z","iopub.execute_input":"2021-11-23T10:09:08.850244Z","iopub.status.idle":"2021-11-23T10:09:08.857146Z","shell.execute_reply.started":"2021-11-23T10:09:08.850204Z","shell.execute_reply":"2021-11-23T10:09:08.856139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mixup(x, features, y, alpha=1):\n    assert alpha > 0, \"alpha should be larger than 0\"\n    assert x.size(0) > 1, \"Mixup cannot be applied to a single instance.\"\n    \n    lam = np.random.beta(alpha, alpha)\n#     for the shape of lam, run the following two lines\n#     import seaborn as sns\n#     sns.distplot(np.random.beta(0.5,0.5, 1000), bins=100)\n    rand_index = torch.randperm(x.size()[0]) # random permutation of images in the batch x\n    mixed_x = lam * x + (1-lam) * x[rand_index, :]\n    mixed_features = lam * features + (1-lam) * features[rand_index, :]\n    target_a, target_b = y, y[rand_index]\n    return mixed_x, mixed_features, target_a, target_b, lam\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-23T10:09:08.858421Z","iopub.execute_input":"2021-11-23T10:09:08.859203Z","iopub.status.idle":"2021-11-23T10:09:08.867909Z","shell.execute_reply.started":"2021-11-23T10:09:08.859164Z","shell.execute_reply":"2021-11-23T10:09:08.86721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we split the dataset into for cross-validation\n# here we treat the label \"Pawpularity\" as categorical data, and use the StratifiedKfol Function\n# actually it is numerical data\nskf = StratifiedKFold(\n    n_splits=config.n_splits, shuffle=True, random_state=config.seed\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T10:09:08.869545Z","iopub.execute_input":"2021-11-23T10:09:08.869987Z","iopub.status.idle":"2021-11-23T10:09:08.88054Z","shell.execute_reply.started":"2021-11-23T10:09:08.869948Z","shell.execute_reply":"2021-11-23T10:09:08.879735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# my pretrained weighted: https://www.kaggle.com/zhicongliang/pawpularitymetaswintransformer\n## load the models\nbest_model_fold = []\nfor fold in range(5):\n    model = Model(config.model_name).to(config.device)\n    model.load_state_dict(torch.load('../input/pawpularitymetaswintransformer/meta_swin_transformer_fold_{}.h5'.format(fold), map_location=torch.device(config.device)))\n    best_model_fold.append(model)","metadata":{"execution":{"iopub.status.busy":"2021-11-23T10:09:08.88206Z","iopub.execute_input":"2021-11-23T10:09:08.882346Z","iopub.status.idle":"2021-11-23T10:09:16.110675Z","shell.execute_reply.started":"2021-11-23T10:09:08.882308Z","shell.execute_reply":"2021-11-23T10:09:16.109858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we keep record of the training in each fold\nsuper_final_prediction_nn = []\nsuper_final_prediction_svr = []\nsuper_final_oof_prediction_nn = []\nsuper_final_oof_prediction_svr = []\nsuper_final_oof_true = []\n\nsvrs = []\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(df[\"Id\"], df[\"Pawpularity\"])):\n    \n    print('================================ CV fold {} ================================'.format(fold))\n    \n    train_df = df.loc[train_idx].reset_index(drop=True)\n    val_df = df.loc[val_idx].reset_index(drop=True)\n    \n    # we would like to do some random transformation to our training data such that\n    # our model can be more rubost against different patterns in out-of-sample data\n    train_transform = T.Compose([\n        T.Resize([224, 224]), # crop the image size to 3*224*224\n        T.RandomHorizontalFlip(), # random flip the image horizontally\n        T.RandomVerticalFlip(), # random flip the image vertically\n        T.RandomAffine(15, translate=(0.1, 0.1), scale=(0.9, 1.1)), # Random affine transformation of the image keeping center invariant.\n        T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1), # randomly changes the brightness, saturation, and other properties of an image\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    # in validation set, we only convert our data to torch.float and do a normalization\n    val_transform = T.Compose([\n        T.Resize([224, 224]),\n        T.ToTensor(),\n        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    \n    train_set = PetfinderDataset(train_df, transform=train_transform, dense_features=dense_features)\n    val_set = PetfinderDataset(val_df, transform=val_transform, dense_features=dense_features)\n    \n    # then we define the dataloader for training and validation\n    # it tells the machine how to sample from our training/validation set\n    train_loader = DataLoader(train_set, batch_size=config.train_batchsize, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_set, batch_size=config.val_batchsize, num_workers=4)\n    \n    # get pretrained best model\n    model = best_model_fold[fold]\n    \n    ### Training SVR over training set\n    # get model heads\n    for batch_idx, (data, features, target) in enumerate(train_loader):\n        data, features = data.float().to(config.device), features.float().to(config.device)\n        output = model(data, features)\n        output = output.detach()\n        if batch_idx == 0:\n            train_pred = output\n        else:\n            train_pred = torch.cat([train_pred, output], dim=0)\n    \n    # get the embedding feature of the neural network\n    train_embed = train_pred[:,1:]\n    \n    ## fit RAPIDS SVR\n    clf = SVR(C=5.)\n    clf.fit(train_embed, train_df.Pawpularity.values.astype('int32'))\n    svrs.append(clf)\n    \n    ### validate the model over validation set\n    # get model heads\n    for batch_idx, (data, features, target) in enumerate(val_loader):\n        data, features = data.float().to(config.device), features.float().to(config.device)\n        output = model(data, features)\n        output = output.detach()\n        if batch_idx == 0:\n            val_pred = output\n        else:\n            val_pred = torch.cat([val_pred, output], dim=0)\n    \n    val_embed = val_pred[:,1:]\n    \n    final_oof_prediction_nn = val_pred[:,0].sigmoid().to('cpu').numpy() * 100\n    final_oof_prediction_svr = clf.predict(val_embed).get().clip(0,100)\n    \n    super_final_oof_prediction_nn.append(final_oof_prediction_nn)\n    super_final_oof_prediction_svr.append(final_oof_prediction_svr)\n    \n    final_oof_true = val_df['Pawpularity'].values\n    super_final_oof_true.append(final_oof_true)\n    \n    # COMPUTE RSME\n    rsme = np.sqrt( np.mean( (super_final_oof_true[-1] - super_final_oof_prediction_nn[-1])**2.0 ) )\n    print('NN RSME =',rsme,'\\n')\n    rsme = np.sqrt( np.mean( (super_final_oof_true[-1] - super_final_oof_prediction_svr[-1])**2.0 ) )\n    print('SVR RSME =',rsme,'\\n')\n    \n    w = 0.1\n    oof2 = (1-w) * super_final_oof_prediction_nn[-1] + w * super_final_oof_prediction_svr[-1]\n    rsme = np.sqrt( np.mean( (super_final_oof_true[-1] - oof2)**2.0 ) )\n    print('Ensemble RSME =',rsme,'\\n')\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-23T10:22:24.744449Z","iopub.execute_input":"2021-11-23T10:22:24.744711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 4. Make Submission","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv(os.path.join(config.root, 'test.csv'))\ntest_id = df_test.index\ndf_test['Id'] = df_test[\"Id\"].apply(lambda x: os.path.join(config.root, \"test\", x + \".jpg\")) # we transform the Id to its image path\n\ntest_transform = T.Compose([\n    T.Resize([224, 224]),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntest_set = PetfinderDataset(df_test, dense_features=dense_features, transform=test_transform)\ntest_loader = DataLoader(test_set, batch_size=config.val_batchsize, num_workers=4)\n\n# get the testing prediction\ntest_pred_nn = np.zeros((df_test.shape[0]))\ntest_pred_svr = np.zeros((df_test.shape[0]))\n\nfor (model, clf) in zip(best_model_fold, svrs):\n    for batch_idx, (data, features) in enumerate(test_loader):\n        data, features = data.float().to(config.device), features.float().to(config.device)\n        \n        # NN prediction\n        output = model(data, features).detach()\n        if batch_idx == 0:\n            pred_nn = output[:,0].sigmoid().to('cpu').numpy()* 100\n        else:\n            pred_nn = np.vstack((pred_nn, output[:,0].sigmoid().to('cpu').numpy()* 100))\n\n        # SVR prediction\n        pred = clf.predict(output[:,1:]).get().clip(0,100)\n        if batch_idx == 0:\n            pred_svr = pred\n        else:\n            pred_svr = np.vstack((pred_svr, pred))\n        \n    test_pred_nn += pred_nn\n    test_pred_svr += pred_svr\n\n# take the average over folds\nw = 0.1\ntest_pred = (1-w) * test_pred_nn / len(best_model_fold) + w * test_pred_svr / len(svrs)\n\n\n## SVR prediction\nsubmission = pd.read_csv(os.path.join(config.root, 'test.csv'))[['Id']]\nsubmission['Pawpularity'] = test_pred\nsubmission","metadata":{"execution":{"iopub.status.busy":"2021-11-23T09:16:40.749794Z","iopub.execute_input":"2021-11-23T09:16:40.750084Z","iopub.status.idle":"2021-11-23T09:16:43.084993Z","shell.execute_reply.started":"2021-11-23T09:16:40.750053Z","shell.execute_reply":"2021-11-23T09:16:43.083982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}