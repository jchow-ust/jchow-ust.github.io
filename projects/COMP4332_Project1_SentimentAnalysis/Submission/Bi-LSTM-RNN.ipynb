{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12304,"status":"ok","timestamp":1649173062343,"user":{"displayName":"Jasper Chow","userId":"06566271520968338357"},"user_tz":-480},"id":"0Xmk3b-8k35D","outputId":"a219fcc0-bfa0-4cc5-8e15-1ed5c201225a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: keras-self-attention in /usr/local/lib/python3.7/dist-packages (0.51.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (1.21.5)\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/4-2/COMP4332/Project1\n","/content/drive/My Drive/4-2/COMP4332/Project1\n"]}],"source":["import os\n","import math\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","from itertools import chain\n","\n","import keras\n","import tensorflow as tf\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, Activation, Embedding, Dropout, BatchNormalization, Input, \\\n","      Add, Concatenate, Bidirectional, SimpleRNN, LSTM, GRU, \\\n","      LayerNormalization, Conv1D, MaxPooling1D, Lambda, GlobalAveragePooling1D\n","\n","!pip install keras-self-attention\n","from keras_self_attention import SeqSelfAttention, SeqWeightedAttention\n","from tensorflow.keras.utils import to_categorical\n","\n","from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, classification_report\n","\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from string import punctuation\n","\n","# if running from google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# cd to correct directory (the one containing this notebook)\n","%cd /content/drive/My\\ Drive/4-2/COMP4332/Project1\n","!pwd # check working directory is correct\n","\n","\n","stop_words = set(stopwords.words(\"english\"))\n","stop_words.remove(\"not\") # added by HCJ\n","ps = PorterStemmer()"]},{"cell_type":"markdown","metadata":{"id":"8vKEwfP2k35T"},"source":["## Data Loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pb68IdIok35W"},"outputs":[],"source":["def load_data(file_name, no_stars=False):\n","    df = pd.read_csv(file_name)\n","    if no_stars:\n","        dummy = None\n","    else:\n","        dummy = df[\"stars\"]\n","    return df[\"review_id\"], df[\"text\"], dummy\n","\n","def write_predictions(file_name, pred, text):\n","    d = {'review_id': text, 'stars': pred}\n","    df = pd.DataFrame(d)\n","    df.to_csv(file_name, index=False)"]},{"cell_type":"markdown","metadata":{"id":"qM9A7r7Fk35X"},"source":["## Feature Extractor"]},{"cell_type":"markdown","metadata":{"id":"zFY2iem8k35Z"},"source":["- Token, Stemming, Stopwords, Punctuation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fUU7uiDCk35a"},"outputs":[],"source":["def tokenize(text):\n","    return nltk.word_tokenize(text)\n","\n","def stem(tokens):\n","    return [ps.stem(token).lower() for token in tokens]\n","\n","def filter_stopwords(tokens):\n","    return [token for token in tokens if token not in stop_words and not token.isnumeric()]\n","\n","def remove_punc(tokens):\n","    return ''.join([token.lower() for token in tokens if token not in punctuation])\n","\n","def get_feats_dict(feats, min_freq=-1, max_freq=-1, max_size=-1):\n","    \"\"\"\n","    :param data: a list of features, type: list(list)\n","    :param min_freq: the lowest fequency that the fequency of a feature smaller than it will be filtered out, type: int\n","    :param max_freq: the highest fequency that the fequency of a feature larger than it will be filtered out, type: int\n","    :param max_size: the max size of feature dict, type: int\n","    return a feature dict that maps features to indices, sorted by frequencies\n","    \"\"\"\n","    # count all features\n","    feat_cnt = Counter(feats) # [\"text\", \"text\", \"mine\"] --> {\"text\": 2, \"mine\": 1}\n","    if max_size > 0 and min_freq == -1 and max_freq == -1:\n","        valid_feats = [\"<pad>\", \"<unk>\"] + [f for f, cnt in feat_cnt.most_common(max_size-2)]\n","    else:\n","        valid_feats = [\"<pad>\", \"<unk>\"]\n","        for f, cnt in feat_cnt.most_common():\n","            if (min_freq == -1 or cnt >= min_freq) and \\\n","                (max_freq == -1 or cnt <= max_freq):\n","                valid_feats.append(f)\n","    if max_size > 0 and len(valid_feats) > max_size:\n","        valid_feats = valid_feats[:max_size]\n","    print(\"Size of features:\", len(valid_feats))\n","    \n","    # build a mapping from features to indices\n","    feats_dict = dict(zip(valid_feats, range(len(valid_feats))))\n","    return feats_dict\n","\n","def get_index_vector(feats, feats_dict, max_len):\n","    global unk_count\n","    \"\"\"\n","    :param feats: a list of features within a given data point/review, type: list\n","    :param feats_dict: a dict from features (tokens) to indices, type: dict\n","    :param feats: a list of features, type: list\n","    return a feature vector\n","    \"\"\"\n","    # initialize the vector as all zeros\n","    vector = np.zeros(max_len, dtype=np.int64)\n","    for i, f in enumerate(feats):\n","        if i == max_len:\n","            break\n","        # get the feature index, return 1 (<unk>) if the feature does not exist\n","        f_idx = feats_dict.get(f, 1)\n","        vector[i] = f_idx\n","    return vector"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":816,"status":"ok","timestamp":1649173063149,"user":{"displayName":"Jasper Chow","userId":"06566271520968338357"},"user_tz":-480},"id":"6pHLfe8Uk35h","outputId":"8c1dc8fa-38fa-42f6-a827-7d75aabde42e"},"outputs":[{"output_type":"stream","name":"stdout","text":["0       We came in today during closing & they still a...\n","1       Tiny, but casual location for breakfast/brunch...\n","2       We keep going to the same plaza to eat pizza b...\n","3       Tim Hortons is the epitome of Canadian mediocr...\n","4       Workers here are very friendly and they know a...\n","                              ...                        \n","1995    I really don't want to give this place 3 stars...\n","1996    I don't think I'll ever be as satisfied and ha...\n","1997    The food is delicious and inexpensive. The mus...\n","1998    Amazing pizza, large variety of food, and a mo...\n","1999    I honestly had NO IDEA pastries could taste so...\n","Name: text, Length: 2000, dtype: object\n"]}],"source":["train_file = \"data/train.csv\"\n","valid_file = \"data/valid.csv\"\n","actual_test_file = \"data/test.csv\"\n","\n","# load data\n","train_ids, train_texts, train_labels = load_data(train_file)\n","valid_ids, valid_texts, valid_labels = load_data(valid_file)\n","test_ids_ac, test_texts_ac, _ = load_data(actual_test_file, no_stars=True)\n","print(valid_texts)"]},{"cell_type":"markdown","metadata":{"id":"41h5S-AUk35d"},"source":["## Classifier: RNN"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1649173063150,"user":{"displayName":"Jasper Chow","userId":"06566271520968338357"},"user_tz":-480},"id":"kOuvlIk1om5g","colab":{"base_uri":"https://localhost:8080/"},"outputId":"06272751-2757-4823-dae4-ae52e8eaba6e"},"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([7.1896725 6.1782417 6.678189  7.9382534 9.431003 ], shape=(5,), dtype=float32)\n","tf.Tensor(-1.0, shape=(), dtype=float32)\n","tf.Tensor(1.3755213, shape=(), dtype=float32)\n"]}],"source":["\n","# returns sorted indices of unique values, then the occurrence of each of those values\n","tr_unique_vals, tr_unique_counts = np.unique(train_labels, return_counts=True)\n","LDAM_n_j = tf.math.sqrt(tf.math.sqrt(tf.convert_to_tensor(tr_unique_counts, np.float32))) # derived, HCJ\n","LDAM_C = tf.constant(-1, dtype=\"float32\") # hyperparamter, can tune, HCJ\n","print(LDAM_n_j)\n","print(LDAM_C)\n","\n","#print(tr_unique_counts)\n","#print(LDAM_n_j)\n","\n","def LDAM(targets, preds):\n","  global LDAM_C\n","  global LDAM_n_j\n","\n","  # Let k be the number of unique classes.\n","  # 'preds' is the set of outputs generated by the model (size B*k) tf Tensor\n","  # 'targets' is the set of corresponding GT one-hot labels (size B*k) tf Tensor\n","  # 'LDAM_C' is a hyperparameter to be tuned during validation, 'LDAM_n_j' is a k*1 tf tensor whose ith entry stores (the number of elements who has label i)^(1/4)\n","  \n","  deltas = LDAM_C/LDAM_n_j # actually = C/((n_j)^0.25). Is tf tensor of size [k]\n","  #print(deltas)\n","\n","  # for a given input x with corresponding label y (y in {0,...,4}) the model predicts a k-dimensional vector (p_0,...,p_{k-1}) where p_i denotes the probability the model believes x belongs to class i\n","  # we need to extract the necessary values into a tf tensor of size [B] called z_y. Example:\n","  # (0.40, 0.32, 0.24, 0.02, 0.02) | 0\n","  # (0.80, 0.04, 0.05, 0.01, 0.10) | 2\n","  # (0.20, 0.60, 0.10, 0.09, 0.01) | 1\n","  # ...\n","  # then we need to extract index 0 from 1st vector, index 2 from 2nd element, index 1 from 3rd vector, etc...: this gives (0.32, 0.05, 0.6, ...)\n","  # source (tested): https://stackoverflow.com/questions/61096522/pytorch-tensor-advanced-indexing\n","  # this essentially refers to the probability that the model classified the input x correctly, for all x in the batch\n","  actual_label = tf.math.argmax(targets, axis=1)\n","  #print(actual_label)\n","\n","  z_y = tf.gather(params=preds, indices=actual_label, batch_dims=1)\n","  #print(z_y)\n","\n","  deltas_indexed = tf.gather(params=deltas, indices=actual_label, batch_dims=0)\n","  e_zyminusdeltay = tf.math.exp(z_y - deltas_indexed) # torch tensor of size [B]\n","  #print(e_zyminusdeltay)\n","\n","  # since each value of label is in {0,...,4}, doing (actual_label+1) mod k allows us to extract every probability not equal to the probability x was predicted to be y (its corresponding label)\n","  e_zj_sum = tf.zeros(shape=(targets.shape[0],))\n","  for i in range(1,5):\n","      z_j_i = tf.gather(params=preds, indices=(actual_label+i)%5, batch_dims=1)\n","      #print(tf.math.exp(z_j_i))\n","      e_zj_sum += tf.math.exp(z_j_i)\n","  \n","  #print(e_zj_sum)\n","  # compute L_LDAM((x,y);f) for all x in the batch. Use nat log\n","  result = -tf.math.log(e_zyminusdeltay/(e_zyminusdeltay+e_zj_sum))\n","  #print(result)\n","\n","  # take average of the entries to get a real-valued result\n","  return tf.math.reduce_mean(result)\n","\n","TEST1 = tf.convert_to_tensor(np.array([[0.40, 0.32, 0.24, 0.02, 0.02],\n","                 [0.80, 0.04, 0.05, 0.01, 0.10],\n","                 [0.20, 0.60, 0.10, 0.09, 0.01]]), np.float32)\n","TEST2 = tf.convert_to_tensor(np.array([[1, 0, 0, 0, 0],\n","                 [0, 0, 1, 0, 0],\n","                 [0, 1, 0, 0, 0]]), np.float32)\n","ldam_test = LDAM(TEST2, TEST1)\n","print(ldam_test)\n","\n","\n","def macro_soft_f1(y, y_hat):\n","    \"\"\"If we want to optimize macro-F1 score, we can use it as a loss function.\n","    However since it not differentiable, we use soft F1 as loss instead.\n","    Average (1 - soft-F1) across all labels.\n","    Use probability values instead of binary predictions.\n","    \n","    Args:\n","        y (int32 Tensor): targets array of shape (B, k) where B = batch size, k = number of classes\n","        y_hat (float32 Tensor): probability matrix of shape (B, k)\n","        \n","    Returns:\n","        cost (scalar Tensor): value of the loss function for the batch\n","    \"\"\"\n","    \n","    y = tf.cast(y, tf.float32)\n","    y_hat = tf.cast(y_hat, tf.float32)\n","    tp = tf.reduce_sum(y_hat * y, axis=0)\n","    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n","    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n","    soft_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n","    cost = 1 - soft_f1 # reduce 1 - soft-f1 in order to increase soft-f1\n","    macro_cost = tf.reduce_mean(cost) # average on all labels\n","    \n","    return macro_cost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-TdVJSI8qDYS"},"outputs":[],"source":["def attention_RNN(input_length, vocab_size, embedding_size,\n","              hidden_size, output_size, num_mlp_layers,\n","              rnn_type=\"lstm\",\n","              bidirectional=False,\n","              embedding_matrix=None,\n","              activation=\"tanh\",\n","              dropout_rate=0.0,\n","              batch_norm=False,                        \n","              l2_reg=0.0,\n","              loss=\"categorical_crossentropy\",\n","              optimizer=\"Adam\",\n","              learning_rate=0.001,\n","              trainable=False,\n","              metric='accuracy'):\n","    x = Input(shape=(input_length,))\n","    \n","    ################################\n","    ###### Word Representation #####\n","    ################################\n","    # word representation layer\n","\n","    # The model will take as input an integer matrix of size (batch, input_length),\n","    # and the largest integer (i.e. word index) in the input should be no larger than vocab_size-1\n","    # Now output shape is (batch, input_length, output_dim)\n","    if embedding_matrix is not None:\n","        emb = Embedding(input_dim=vocab_size,\n","                        output_dim=embedding_size,\n","                        input_length=input_length,\n","                        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n","                        trainable=trainable, mask_zero=True)(x)\n","    else:\n","        emb = Embedding(input_dim=vocab_size,\n","                        output_dim=embedding_size,\n","                        input_length=input_length,\n","                        embeddings_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=0))(x)\n","\n","    ################################\n","    ####### Recurrent Layers #######\n","    ################################\n","    # recurrent layers\n","    if rnn_type == \"rnn\":\n","        fn = SimpleRNN\n","    elif rnn_type == \"lstm\":\n","        fn = LSTM\n","    elif rnn_type == \"gru\":\n","        fn = GRU\n","    else:\n","        raise NotImplementedError\n","    h = emb\n","    #h = SeqSelfAttention(attention_activation='sigmoid')(h)\n","    if bidirectional:\n","        h = Bidirectional(fn(hidden_size,\n","                                    kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n","                                    recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n","                                    return_sequences=True,recurrent_dropout=dropout_rate))(h)\n","    else:\n","        h = fn(hidden_size,\n","                                    kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n","                                    recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n","                                    return_sequences=True,recurrent_dropout=dropout_rate)(h)\n","    #h = Dropout(dropout_rate, seed=0)(h)\n","    h = SeqWeightedAttention()(h)\n","    #h = SeqSelfAttention(attention_activation='sigmoid')(h)\n","    #h = GlobalAveragePooling1D()(h)\n","    \n","    ################################\n","    #### Fully Connected Layers ####\n","    ################################\n","    # multi-layer perceptron\n","    for i in range(num_mlp_layers-1):\n","        new_h = Dense(hidden_size,\n","                      kernel_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),    ## He initializer\n","                      bias_initializer=\"zeros\",\n","                      kernel_regularizer=keras.regularizers.l2(l2_reg))(h)\n","        new_h = Activation(activation)(new_h) # add activation\n","        # add batch normalization layer\n","        if batch_norm:\n","            new_h = BatchNormalization()(new_h)\n","        # add residual connection\n","        if i == 0:\n","            h = new_h\n","        else: \n","            h = Add()([h, new_h])\n","    \n","    y = Dense(output_size,\n","              activation=\"softmax\",\n","              kernel_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),      ## He initializer\n","              bias_initializer=\"zeros\")(h)\n","    \n","    # set the loss, the optimizer, and the metric\n","    if optimizer == \"SGD\":\n","        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n","    elif optimizer == \"RMSprop\":\n","        optmizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n","    elif optimizer == \"Adam\":\n","        optmizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    else:\n","        raise NotImplementedError\n","    model = Model(x, y)\n","\n","    if loss==\"LDAM\": # added by HCJ\n","        model.compile(loss=LDAM, optimizer=optimizer, metrics=[metric])\n","    elif loss==\"macro_soft_f1\": # added by HCJ\n","        model.compile(loss=macro_soft_f1, optimizer=optimizer, metrics=[metric])\n","    else:\n","        model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n","    \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F4_375VBk35e"},"outputs":[],"source":["def build_RNN(input_length, vocab_size, embedding_size,\n","              hidden_size, output_size,\n","              num_rnn_layers, num_mlp_layers,\n","              rnn_type=\"lstm\",\n","              bidirectional=False,\n","              embedding_matrix=None,\n","              activation=\"tanh\",\n","              dropout_rate=0.0,\n","              batch_norm=False,                        \n","              l2_reg=0.0,\n","              loss=\"categorical_crossentropy\",\n","              optimizer=\"Adam\",\n","              learning_rate=0.001,\n","              trainable=False,\n","              metric='accuracy'):\n","    \"\"\"\n","    :param input_length: the maximum length of sentences, type: int\n","    :param vocab_size: the vacabulary size, type: int\n","    :param embedding_size: the dimension of word representations, type: int\n","    :param hidden_size: the dimension of the hidden states, type: int\n","    :param output_size: the dimension of the prediction, type: int\n","    :param num_rnn_layers: the number of layers of the RNN, type: int\n","    :param num_mlp_layers: the number of layers of the MLP, type: int\n","    :param rnn_type: the type of RNN, type: str\n","    :param bidirectional: whether to use bidirectional rnn, type: bool\n","    :param activation: the activation type, type: str\n","    :param dropout_rate: the probability of dropout, type: float\n","    :param batch_norm: whether to enable batch normalization, type: bool\n","    :param l2_reg: the weight for the L2 regularizer, type: str\n","    :param loss: the training loss, type: str\n","    :param optimizer: the optimizer, type: str\n","    :param learning_rate: the learning rate for the optimizer, type: float\n","    :param metric: the metric, type: list of str\n","    return a RNN for text classification,\n","    # activation document: https://keras.io/activations/\n","    # dropout document: https://keras.io/layers/core/#dropout\n","    # embedding document: https://keras.io/layers/embeddings/#embedding\n","    # recurrent layers document: https://keras.io/layers/recurrent\n","    # batch normalization document: https://keras.io/layers/normalization/\n","    # losses document: https://keras.io/losses/\n","    # optimizers document: https://keras.io/optimizers/\n","    # metrics document: https://keras.io/metrics/\n","    \"\"\"\n","    x = Input(shape=(input_length,))\n","    \n","    ################################\n","    ###### Word Representation #####\n","    ################################\n","    # word representation layer\n","\n","    # The model will take as input an integer matrix of size (batch, input_length),\n","    # and the largest integer (i.e. word index) in the input should be no larger than vocab_size-1\n","    # Now output shape is (batch, input_length, output_dim)\n","    if embedding_matrix is not None:\n","        emb = Embedding(input_dim=vocab_size,\n","                        output_dim=embedding_size,\n","                        input_length=input_length,\n","                        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n","                        trainable=trainable)(x)\n","    else:\n","        emb = Embedding(input_dim=vocab_size,\n","                        output_dim=embedding_size,\n","                        input_length=input_length,\n","                        embeddings_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=0))(x)\n","    \n","    # BatchNorm and Conv layers added by HCJ\n","    #emb = Conv1D(filters=200, kernel_size=3)(emb)\n","    #emb = BatchNormalization()(emb)\n","    #emb = Activation(activation)(emb)\n","    #emb = MaxPooling1D(pool_size=2)(emb)\n","\n","    ################################\n","    ####### Recurrent Layers #######\n","    ################################\n","    # recurrent layers\n","    if rnn_type == \"rnn\":\n","        fn = SimpleRNN\n","    elif rnn_type == \"lstm\":\n","        fn = LSTM\n","    elif rnn_type == \"gru\":\n","        fn = GRU\n","    else:\n","        raise NotImplementedError\n","    h = emb\n","    unused = \"\"\"for i in range(num_rnn_layers):\n","        is_last = (i == num_rnn_layers-1)\n","        if bidirectional:\n","            h = Bidirectional(fn(hidden_size,\n","                                 kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n","                                 recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n","                                 return_sequences=not is_last))(h)\n","        else:\n","            h = fn(hidden_size,\n","                   kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n","                   recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n","                   return_sequences=not is_last)(h)\n","        h = Dropout(dropout_rate, seed=0)(h)\"\"\"\n","\n","    for i in range(num_rnn_layers):\n","        should_return_seq = (i < num_rnn_layers - 1) # last layer shouldn't return sequence\n","        if bidirectional:\n","            # hidden_size = number of LSTM units in a layer\n","            # if input to an LSTM layer is (B, T, F) where B=batch size, T=timesteps, F=features, output of bidirectional LSTM is:\n","            # (B, hidden_size) if not returning sequence\n","            # (B, T, hidden_size) if returning sequence\n","            rnn_h = Bidirectional(fn(hidden_size,\n","                                 kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n","                                 recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n","                                 return_sequences=should_return_seq))(h)\n","        else:\n","            rnn_h = fn(hidden_size,\n","                   kernel_initializer=keras.initializers.glorot_uniform(seed=0),\n","                   recurrent_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),\n","                   return_sequences=should_return_seq)(h)\n","        rnn_h = Dropout(dropout_rate, seed=0)(rnn_h)\n","        if should_return_seq:\n","            # set residual connections for stacked LSTM as per:\n","            # https://stackoverflow.com/questions/60772323/how-to-implement-a-skip-connection-structure-between-lstm-layers\n","            if i > 0: #or h.shape[-1] == hidden_size:\n","                h = Add()([h, rnn_h])\n","            else: \n","                # only occurs if:\n","                # i==0. This basically means we always want to apply LSTM at least once\n","                h = rnn_h\n","        else:\n","            if num_rnn_layers==1:\n","                h = rnn_h\n","            else:\n","                # only occurs at the last layer (since that is the ONLY layer we are not returning a sequence), so\n","                # select only the last element of the previous output.\n","                def slice_last(x):\n","                    return x[..., -1, :]\n","                h = Add()([Lambda(slice_last)(h), rnn_h])\n","    # the output of recurrent layer should be h, not rnn_h\n","    \n","    ################################\n","    #### Fully Connected Layers ####\n","    ################################\n","    # multi-layer perceptron\n","    for i in range(num_mlp_layers-1):\n","        new_h = Dense(hidden_size,\n","                      kernel_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),    ## He initializer\n","                      bias_initializer=\"zeros\",\n","                      kernel_regularizer=keras.regularizers.l2(l2_reg))(h)\n","        # add batch normalization layer\n","        if batch_norm:\n","            new_h = BatchNormalization()(new_h)\n","        # add residual connection\n","        if i == 0:\n","            h = new_h\n","        else: \n","            h = Add()([h, new_h])\n","        # add activation\n","        h = Activation(activation)(h)\n","    \n","    y = Dense(output_size,\n","              activation=\"softmax\",\n","              kernel_initializer=keras.initializers.Orthogonal(gain=1.0, seed=0),      ## He initializer\n","              bias_initializer=\"zeros\")(h)\n","    \n","    # set the loss, the optimizer, and the metric\n","    if optimizer == \"SGD\":\n","        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n","    elif optimizer == \"RMSprop\":\n","        optmizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n","    elif optimizer == \"Adam\":\n","        optmizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    else:\n","        raise NotImplementedError\n","    model = Model(x, y)\n","\n","    if loss==\"LDAM\": # added by HCJ\n","        model.compile(loss=LDAM, optimizer=optimizer, metrics=[metric])\n","    elif loss==\"macro_soft_f1\": # added by HCJ\n","        model.compile(loss=macro_soft_f1, optimizer=optimizer, metrics=[metric])\n","    else:\n","        model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n","    \n","    return model"]},{"cell_type":"markdown","metadata":{"id":"Bf0ODBI8k35g"},"source":["## Google pretrained embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":77218,"status":"ok","timestamp":1649173140991,"user":{"displayName":"Jasper Chow","userId":"06566271520968338357"},"user_tz":-480},"id":"gvOsbV9MU5Xa","outputId":"1ffaf90a-1d8d-492f-c6c2-e9afba5e2670"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/4-2/COMP5214/A2\n","/content/drive/My Drive/4-2/COMP4332/Project1\n"]}],"source":["from gensim import models\n","%cd /content/drive/My\\ Drive/4-2/COMP5214/A2\n","path = \"./GoogleNews-vectors-negative300.bin\"\n","limit=3000000\n","gensim_model = models.KeyedVectors.load_word2vec_format(path, binary=True, limit=limit) \n","%cd /content/drive/My\\ Drive/4-2/COMP4332/Project1"]},{"cell_type":"code","source":["# TODO use this if augmentation successful\n","train_texts_aug = train_texts\n","train_labels_aug = train_labels"],"metadata":{"id":"z6kyUnueajZZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H8ARZpDxk35j"},"outputs":[],"source":["# extract features\n","train_no_punc = [remove_punc(text) for text in train_texts_aug] # use standard train_texts here if this doesn't work\n","valid_no_punc = [remove_punc(text) for text in valid_texts]\n","\n","train_tokens = [tokenize(text) for text in train_no_punc]\n","valid_tokens = [tokenize(text) for text in valid_no_punc]\n","\n","train_stop = [filter_stopwords(tokens) for tokens in train_tokens]\n","valid_stop = [filter_stopwords(tokens) for tokens in valid_tokens]\n","\n","# removed stemming for Google word embeddings since many of them 'miss'\n","#train_stemmed = [stem(tokens) for tokens in train_stop]\n","#valid_stemmed = [stem(tokens) for tokens in valid_stop]\n","\n","train_features = train_stop\n","valid_features = valid_stop\n","\n","test_no_punc_ac = [remove_punc(text) for text in test_texts_ac]\n","test_tokens_ac = [tokenize(text) for text in test_no_punc_ac]\n","test_stop_ac = [filter_stopwords(tokens) for tokens in test_tokens_ac]\n","# removed stemming\n","#test_stemmed_ac = [stem(tokens) for tokens in test_stop_ac]\n","test_features_ac = test_stop_ac"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1649173155633,"user":{"displayName":"Jasper Chow","userId":"06566271520968338357"},"user_tz":-480},"id":"pVjcmahSk35l","outputId":"d329084b-96df-4f2e-cd83-6a7d83092481"},"outputs":[{"output_type":"stream","name":"stdout","text":["Size of features: 4894\n"]}],"source":["min_freq = 20\n","max_freq = -1\n","# build a mapping from features to indices\n","feats_dict = get_feats_dict(\n","    chain.from_iterable(train_features),\n","    min_freq=min_freq,\n","    max_freq=max_freq)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1649173155634,"user":{"displayName":"Jasper Chow","userId":"06566271520968338357"},"user_tz":-480},"id":"ZTAJNBjpqme6","outputId":"7779fdbe-cbbf-415e-b314-277ff764e8bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["['roy', 'papa', 'randomly', 'sassy', 'wiped', 'trustworthy', 'kalbi', 'washer', 'rail', 'ben', 'niece', 'icecream', 'dancers', 'pregnancy', 'marriott', 'lineup', 'extraordinary', 'changs', 'warmed', 'haircuts', 'instant', 'worthwhile', 'lab', 'category', 'consultation', 'walnuts', 'tricky', 'shark', 'justin', 'hooters', 'mental', 'sorbet', 'buttermilk', 'cactus', 'bloor', 'hunger', 'carb', 'encounter', 'filed', 'rob', 'hoagie', 'calzone', 'cox', 'drugs', 'gates', 'reaction', 'afterward', 'hat', 'brief', 'gut', 'excitement', 'chemicals', 'gems', 'banh', 'skimp', 'jacket', 'brewed', 'kinks', 'spam', 'tours', 'carbs', 'granite', 'rubbery', 'swing', 'crackers', 'johns', 'negatives', 'speaker', 'agency', 'alcoholic', 'yall', 'feeding', 'moneys', 'breeze', 'wherever', 'struggle', 'mama', 'lemongrass', 'jus', 'lite', 'filter', 'presence', 'clock', 'mojito', 'player', 'jalapenos', 'clueless', 'international', 'gal', 'norm', 'repeatedly', 'asleep', 'gallery', 'bubbly', 'pint', 'artificial', 'rocked', 'flooring', 'hyped', 'hangover', 'tattoos', 'thirty', 'abundance', 'forks', 'boots', 'cereal', 'removing', 'survive', 'iceberg', 'cuticles', 'wake', 'quiche', 'embarrassing', 'baseball', 'realizing', 'quietly', 'primary', 'di', 'buddies', 'partial', 'xrays', 'concrete', 'friendliness', 'unsatisfied', 'delighted', 'med', 'stickers', 'flip', 'traveled', 'occurred', 'huh', 'browse', 'christina', 'poker', 'bears', 'aussi', 'peu', 'collect', 'lawn', 'shells', 'winning', 'peeling', 'bug', 'avoided', 'familys', 'touches', 'dye', 'hygienist', 'blanket', 'hilton', 'slots', 'individuals', 'handy', 'yo', 'wontons', 'ils', 'smallest', 'michelle', 'paul', 'drawn', 'palazzo', 'hamachi', 'courtyard', 'smelling', 'remains', 'coach', 'potentially', 'alignment', 'devoured', 'lying', 'horribly', 'separated', 'rides', 'marquee', 'trout', 'sincerely', 'virtually', 'montréal', 'shelf', 'blended', 'skillet', 'outback', 'medicine', 'pomegranate', 'position', 'kung', 'returns', 'processed', 'uneven', 'rainy', 'agreement', 'verizon', 'inventory', 'wave', 'ignoring', 'flawless', 'coverage', 'greg', 'drinker', 'tiger', 'warn', 'addressed', 'jeans', 'excuses', 'saturdays', 'specialist', 'buyer', 'dislike', 'michelin', 'lux', 'questionable', 'patties', 'wrapping', 'independent', 'bud', 'daniel', 'apologies', 'laundry', 'expired', 'arrives', 'smiled', 'showers', 'magnificent', 'facials', 'dept', 'sizzling', 'pairs', 'holds', 'idk', 'fathers', 'feelings', 'eatery', 'judging', 'g', 'oatmeal', 'garnish', 'requires', 'remodeled', 'celebrating', 'cigar', 'microwaved', 'button', 'bride', 'attendant', 'revisit', 'skins', 'approved', 'recognize', 'lid', 'expressed', 'specialties', 'pilot', 'forgetting', 'carried', 'copy', 'spotless', 'fireplace', 'personnel', 'basketball', 'spilled', 'protection', 'motor', 'pathetic', 'commercial', 'buildings', 'justice', 'jesus', 'infused', 'complementary', 'dmv', 'discover', 'displayed', 'jokes', 'transferred', 'certificate', 'relationship', 'herb', 'upsell', 'steaming', 'crossed', 'innout', 'muffins', 'matters', 'trails', 'equipped', 'cry', 'dripping', 'marshmallow', 'pops', 'matching', 'eric', 'discussed', 'grounds', 'delayed', 'bottomless', 'situated', 'palms', 'k', 'entrée', 'trays', 'relief', 'karaoke', 'urgent', 'agents', 'savoury', 'thinks', 'plantains', 'warmer', 'killed', 'chuck', 'socks', 'equal', 'breaking', 'charcoal', 'francisco', 'toddler', 'mobile', 'channel', 'sauna', 'patiently', 'terrace', 'dente', 'jumped', 'bridal', 'overrated', 'persons', 'dreams', 'scrub', 'sweat', 'meanwhile', 'sautéed', 'shrimps', 'vdara', 'mani', '5th', '3pm', 'quicker', 'beets', 'rounds', 'speakers', 'stolen', 'nose', 'creativity', 'expand', 'shank', 'surrounded', 'dominos', 'eg', 'verify', 'sriracha', 'shiny', 'hood', 'salesperson', 'apply', 'royal', 'mandatory', 'drag', 'gin', 'drizzled', 'enjoys', 'explains', 'er', 'applebees', 'supreme', 'brake', 'shooting', 'brush', 'increased', 'manhattan', 'uni', 'expanded', 'blueberries', 'kevin', 'ink', 'sits', 'hangout', 'fruity', 'shorter', 'kindness', 'lap', 'directed', 'squeezed', 'meets', 'bummed', 'santa', 'bat', 'nissan', 'thousands', 'um', 'november', 'hefty', 'scotch', 'fillings', 'baklava', 'tissue', 'plays', 'breaks', 'acrylic', 'gesture', 'opinions', 'macaroons', 'spoiled', 'luxurious', 'motel', 'nailed', 'stunning', 'discounted', 'clearance', 'reputation', 'ha', 'grace', 'yea', 'continuously', 'chunk', 'smog', 'msg', 'frustration', 'reviewing', 'transaction', 'ihop', 'nacho', 'wellington', 'rear', 'perform', 'designer', 'robert', 'yuck', 'cravings', 'aquarium', 'cracker', 'cones', 'bull', 'edit', 'journey', 'miller', 'overcharged', 'claims', 'shy', 'determined', 'arrowhead', 'creating', 'delivering', 'deserved', 'disease', 'sarah', 'cabinet', 'overdone', 'dangerous', 'exhibits', 'glazed', 'sing', 'washrooms', 'huevos', 'yolk', 'earn', 'carlos', 'fluid', 'steer', 'ranging', 'removal', 'hosts', 'horrendous', 'tricks', 'leads', 'paste', 'press', 'supermarket', 'carries', 'activities', 'homeless', 'admission', 'brussels', 'character', 'sweetest', 'tropical', 'pasty', 'trivia', 'stressful', 'tow', 'knocked', 'comedy', 'remaining', 'effective', 'tradition', 'mechanics', 'luxor', 'brakes', 'summerlicious', 'foam', 'generic', 'spaces', 'operation', 'lanes', 'rosemary', 'arrangements', 'mold', '11pm', 'trader', 'bao', 'oriented', 'halibut', 'curly', 'james', 'sisters', 'leaking', 'gyms', 'blvd', 'superior', 'expertise', 'grabbing', 'po', 'handing', 'polenta', 'effect', 'whip', 'notified', 'coloring', 'stole', 'dang', 'bob', 'inhouse', 'tails', 'yonge', 'leftover', 'worthy', 'skirt', 'offended', 'boarding', 'timing', 'concoction', 'rolling', 'silverware', 'nathan', 'encourage', 'dedicated', 'vermicelli', 'expectation', 'shoulders', 'seasonings', 'france', 'skipped', 'candied', 'responsibility', 'crushed', 'statement', 'lotus', 'hesitation', 'creepy', 'reported', 'feedback', 'nerve', 'toffee', 'rubbed', 'chatted', 'tikka', 'native', 'flan', 'drenched', 'scary', 'cranberry', 'torta', 'zombie', 'panera', 'elderly', 'catching', 'intense', 'hunt', 'churros', 'tout', 'si', 'aisle', 'snob', 'stared', 'beet', 'croutons', 'surface', 'plumber', 'washing', 'vacuum', 'pest', 'suddenly', 'george', 'healthier', 'voicemail', 'walkin', 'thousand', 'grey', 'aloha', 'painless', 'cobb', 'baristas', '11am', 'oasis', 'intended', 'magical', 'oxtail', 'mandarin', 'blackened', 'settle', 'attempts', 'worries', 'ad', 'newly', 'rule', 'chances', 'disgusted', 'housemade', 'bachelorette', 'ugly', 'rum', 'offices', 'curtains', 'raving', 'privacy', 'spray', 'fishy', 'okra', 'competent', 'tamales', 'cigarette', 'desire', 'nc', 'ur', 'frames', 'monte', 'delay', 'deliciousness', 'rescue', 'dying', 'fudge', 'adventurous', 'senior', 'steel', 'upfront', 'closes', 'passionate', 'trainer', 'humor', 'necessarily', 'pico', 'mornings', 'paneer', 'uncle', 'leaf', 'reserve', 'remarkable', 'anthony', 'associate', 'linda', 'cans', 'lousy', 'require', 'flew', 'margherita', 'mmm', 'dumb', 'butcher', 'eve', 'sundae', 'term', 'unexpected', 'hitting', 'ultimate', 'était', 'se', 'chipped', 'tab', 'citrus', 'profile', 'projects', 'wicked', 'gooey', 'charred', 'grooming', 'varied', 'caramelized', 'hiking', 'mongolian', 'underwhelmed', 'hookah', 'lash', 'flies', 'dairy', 'allergic', '10pm', 'outcome', 'ship', 'chilled', 'palate', 'customize', 'celebration', 'plants', 'jim', 'accidentally', 'lay', '90s', 'commented', 'arcade', 'pea', 'therapist', 'greater', 'sucker', 'remain', 'sin', 'raise', 'pollo', 'fits', 'stools', 'skilled', 'mochi', 'september', 'dozens', 'attending', 'bold', 'grape', 'stressed', 'dime', 'acts', 'lee', 'f', 'prep', 'incident', 'slide', 'brain', 'tie', 'themed', 'exotic', 'amanda', 'accommodated', 'confit', 'wipe', 'bubbles', 'clerk', 'orleans', 'smothered', 'soaked', 'filing', 'guard', 'shipping', 'bench', 'poisoning', 'referring', 'university', 'habit', 'lean', 'unbelievably', 'flakes', 'bmw', 'proximity', 'heavily', 'liquid', 'cater', 'trick', 'maria', 'tools', 'staring', 'screwed', 'yrs', 'union', 'quit', 'yep', 'sensitive', 'ruin', 'tipped', 'cheesesteak', 'wandering', 'radio', 'force', 'ideas', 'imo', 'skinny', 'conclusion', 'watering', 'toyota', 'pump', 'flexible', 'parked', 'veg', 'laser', 'honesty', 'ms', 'meaty', 'transfer', 'penn', 'ability', 'records', '9am', 'dunkin', 'players', 'conditioning', 'burning', 'physically', 'shack', 'parent', 'slides', 'drizzle', 'colour', 'mole', 'premade', 'opt', 'dipped', 'signing', 'verde', 'bison', 'boston', 'packaged', 'terribly', 'allergies', 'fucking', 'truffles', 'bummer', 'updates', 'metro', 'panini', 'sauteed', 'refrigerator', 'doughnuts', 'fuss', 'anticipated', 'creek', 'textures', 'array', 'thoughts', 'pic', 'inn', 'chai', 'heater', 'associates', 'apples', 'cow', '5star', 'campus', 'mondays', 'friendliest', 'tavern', 'justify', 'torn', 'crusty', 'hollandaise', 'occasional', 'embarrassed', 'navigate', 'substitute', 'peanuts', 'integrity', 'grub', 'proud', 'herbs', 'approximately', 'oreo', 'babies', 'boutique', 'february', 'combos', 'provider', 'upbeat', 'fond', 'medication', 'ins', 'backed', 'dragon', 'skill', 'interaction', 'shampoo', 'sophisticated', 'hence', 'neon', 'naturally', 'brian', 'professionals', 'lunches', 'idiot', 'taylor', 'searched', 'streets', 'reminiscent', 'bird', 'melissa', 'sand', 'chilly', 'bulk', 'trucks', 'fois', 'therapy', 'stephanie', 'canned', 'hook', 'ou', 'subtle', 'procedures', 'reuben', 'invited', 'trusted', 'scoops', 'ryan', 'lattes', 'resident', 'fashioned', 'successful', 'amounts', 'purse', 'born', 'stamp', 'assigned', 'impress', 'preparation', 'atm', 'significantly', 'apartments', 'priority', 'carolina', 'vegetarians', 'climbing', 'gay', 'gracious', 'joined', 'reverse', 'confidence', 'purpose', 'riding', 'cable', 'countless', 'overlooking', 'insanely', 'cleanest', 'couches', 'incompetent', 'succulent', 'tao', 'epic', 'enormous', 'fitting', 'bouncer', 'boneless', 'song', 'lift', 'p', 'false', 'prescriptions', 'injury', 'dentists', 'constant', 'convinced', 'supporting', '80s', 'driven', 'instructions', 'shaped', 'irritated', 'confirmation', 'attractive', 'practices', 'farmers', 'comforting', 'signage', 'fifteen', 'disorganized', 'excessive', 'stadium', 'concierge', 'automatically', 'branch', 'slot', 'appeal', 'marinara', 'someones', 'decently', 'valentines', 'beside', 'error', 'painting', 'quotes', 'panda', 'stained', 'renovated', 'deluxe', 'feast', 'colorful', 'marrow', 'defiantly', 'visitors', 'county', 'death', 'grain', 'arts', 'fountains', 'route', 'carpaccio', 'series', 'bargain', 'depot', 'jennifer', 'cider', 'pricier', 'staffed', 'units', 'jobs', 'stories', 'sahara', 'gear']\n"]}],"source":["print(sorted(feats_dict, key=feats_dict.get, reverse=True)[:1000])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LxdXTkPbk35r"},"outputs":[],"source":["max_len = 256 # each review will become an nparray of this length, where each element of said array is a word's integer representation\n","\n","# build the feature matrix by converting each data point (review) to a index vector, and then stack vectors to form matrix\n","train_feats_matrix = np.vstack(\n","    [get_index_vector(f, feats_dict, max_len) for f in train_features])\n","valid_feats_matrix = np.vstack(\n","    [get_index_vector(f, feats_dict, max_len) for f in valid_features])\n","test_feats_matrix_ac = np.vstack(\n","    [get_index_vector(f, feats_dict, max_len) for f in test_features_ac])\n","\n","num_classes = max(train_labels)\n","\n","# convert each label to a one-hot vector, and then stack vectors as a matrix\n","train_label_matrix = to_categorical(train_labels_aug-1, num_classes=num_classes)\n","valid_label_matrix = to_categorical(valid_labels-1, num_classes=num_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1649173156244,"user":{"displayName":"Jasper Chow","userId":"06566271520968338357"},"user_tz":-480},"id":"H4nj1NkCU-a6","outputId":"62a116bf-4e7d-4e9b-c26d-e53dcd0dbd46"},"outputs":[{"output_type":"stream","name":"stdout","text":["<pad>\n","<unk>\n","didnt\n","wasnt\n","doesnt\n","isnt\n","flavour\n","favourite\n","shouldnt\n","flavours\n","ayce\n","bellagio\n","hasnt\n","neighbourhood\n","theatre\n","cancelled\n","mandalay\n","blt\n","phx\n","glutenfree\n","cest\n","summerlin\n","dennys\n","brussel\n","soso\n","centre\n","instagram\n","markham\n","aint\n","doughnut\n","10am\n","80s\n","doughnuts\n","colour\n","90s\n","10pm\n","11am\n","grey\n","panera\n","yonge\n","11pm\n","summerlicious\n","luxor\n","ihop\n","applebees\n","vdara\n","dente\n","savoury\n","innout\n","michelin\n","montréal\n","bloor\n","changs\n","marriott\n","Converted 4840 words (54 misses)\n","(4894, 300)\n"]}],"source":["EMBEDDING_SIZE = 300\n","embedding_matrix = np.zeros((len(feats_dict), EMBEDDING_SIZE), dtype=np.float32)\n","hits = 0\n","misses = 0\n","\n","for word in feats_dict:\n","    word_idx = feats_dict.get(word, -1)\n","    if word in gensim_model:\n","        embedding_matrix[word_idx] = np.array(gensim_model[word], dtype=np.float32)\n","        hits+=1\n","    else:\n","        print(word)\n","        embedding_matrix[word_idx] = np.array(np.zeros(EMBEDDING_SIZE), dtype=np.float32)\n","        misses+=1\n","\n","print(\"Converted %d words (%d misses)\" % (hits, misses))\n","print(embedding_matrix.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ozvolcKQ9Xmk"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\") # ignore UndefinedMetricWarning that occurs when training (since support of F-score and precision for some classes may be 0)\n","\n","class Metrics(keras.callbacks.Callback):\n","    def on_train_begin(self, logs={}):\n","        pass\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        # convert from one-hot back to class labels\n","        train_predict = np.argmax((np.asarray(self.model.predict(train_feats_matrix))).round(), axis=1)\n","        train_targ = np.argmax(train_label_matrix, axis=1)\n","\n","        val_predict = np.argmax((np.asarray(self.model.predict(valid_feats_matrix))).round(), axis=1)\n","        val_targ = np.argmax(valid_label_matrix, axis=1)\n","\n","        print(\"\\n\")\n","        print(classification_report(train_targ, train_predict, labels=[0, 1, 2, 3, 4]))\n","        print(classification_report(val_targ, val_predict, labels=[0, 1, 2, 3, 4]))\n","        print(\"\\n\")\n","        return\n"," \n","metrics = Metrics()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1053,"status":"ok","timestamp":1649177356377,"user":{"displayName":"Jasper Chow","userId":"06566271520968338357"},"user_tz":-480},"id":"Gnec5TTYk35x","outputId":"b601f2a0-483f-49a6-a012-7c2107c8836f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 256)]             0         \n","                                                                 \n"," embedding_1 (Embedding)     (None, 256, 300)          1468200   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 256, 64)           93440     \n","                                                                 \n"," seq_weighted_attention_1 (S  (None, 64)               65        \n"," eqWeightedAttention)                                            \n","                                                                 \n"," dense_2 (Dense)             (None, 64)                4160      \n","                                                                 \n"," activation_1 (Activation)   (None, 64)                0         \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 64)               256       \n"," hNormalization)                                                 \n","                                                                 \n"," dense_3 (Dense)             (None, 5)                 325       \n","                                                                 \n","=================================================================\n","Total params: 1,566,446\n","Trainable params: 1,566,318\n","Non-trainable params: 128\n","_________________________________________________________________\n","None\n"]}],"source":["os.makedirs(\"models\", exist_ok=True)\n","# create bidirectional LSTM model with stacked LSTM layers to learn higher-level features\n","# we have BatchNorm, dropout and L2 regularization already within our model so don't need to make dropout parameter p very large else we risk underfitting\n","unused = \"\"\"model = build_RNN(input_length=max_len, vocab_size=len(feats_dict),\n","                  embedding_size=EMBEDDING_SIZE, hidden_size=64, output_size=num_classes,\n","                  rnn_type=\"lstm\", num_rnn_layers=1, bidirectional=True, num_mlp_layers=1,\n","                  embedding_matrix=embedding_matrix, trainable=True,\n","                  activation=\"relu\",\n","                  loss=\"macro_soft_f1\", # added by HCJ\n","                  batch_norm=True, learning_rate=2e-5,\n","                  l2_reg=0.001, dropout_rate=0.2)\"\"\"\n","\n","model = attention_RNN(input_length=max_len, vocab_size=len(feats_dict),\n","                  embedding_size=EMBEDDING_SIZE, hidden_size=64, output_size=num_classes,\n","                  rnn_type=\"lstm\", bidirectional=False, num_mlp_layers=2,\n","                  embedding_matrix=embedding_matrix, trainable=True,\n","                  activation=\"relu\",\n","                  loss=\"LDAM\", # added by HCJ\n","                  batch_norm=True, learning_rate=2e-5,\n","                  l2_reg=0.0001, dropout_rate=0.2)\n","\n","best_model_name = \"bilstm_weights_attention_final.hdf5\" \n","checkpointer = keras.callbacks.ModelCheckpoint(\n","    filepath=os.path.join(\"models\", best_model_name),\n","    monitor=\"val_loss\", # changed from val_accuracy to val_loss\n","    verbose=0,\n","    save_best_only=False)\n","earlystopping = keras.callbacks.EarlyStopping(\n","    monitor='val_loss',\n","    patience=10,\n","    verbose=1)\n","print(model.summary())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ZgLNVpJ_5kq","executionInfo":{"status":"ok","timestamp":1649179377190,"user_tz":-480,"elapsed":2006906,"user":{"displayName":"Jasper Chow","userId":"06566271520968338357"}},"outputId":"e0ad646c-de48-4fba-8e55-37e95001f1b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","180/180 [==============================] - ETA: 0s - loss: 1.2346 - accuracy: 0.5894\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.28      0.98      0.43      2672\n","           1       0.00      0.00      0.00      1457\n","           2       0.00      0.00      0.00      1989\n","           3       0.64      0.20      0.30      3971\n","           4       0.84      0.78      0.81      7911\n","\n","    accuracy                           0.53     18000\n","   macro avg       0.35      0.39      0.31     18000\n","weighted avg       0.55      0.53      0.49     18000\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.26      0.97      0.41       282\n","           1       0.00      0.00      0.00       136\n","           2       0.00      0.00      0.00       212\n","           3       0.57      0.18      0.27       466\n","           4       0.81      0.72      0.76       904\n","\n","    accuracy                           0.50      2000\n","   macro avg       0.33      0.37      0.29      2000\n","weighted avg       0.54      0.50      0.46      2000\n","\n","\n","\n","180/180 [==============================] - 231s 1s/step - loss: 1.2346 - accuracy: 0.5894 - val_loss: 1.2956 - val_accuracy: 0.6215\n","Epoch 2/10\n","180/180 [==============================] - ETA: 0s - loss: 1.1481 - accuracy: 0.6770\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.44      0.97      0.61      2672\n","           1       0.73      0.14      0.23      1457\n","           2       0.75      0.16      0.27      1989\n","           3       0.73      0.40      0.52      3971\n","           4       0.79      0.92      0.85      7911\n","\n","    accuracy                           0.67     18000\n","   macro avg       0.69      0.52      0.50     18000\n","weighted avg       0.71      0.67      0.63     18000\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.40      0.95      0.56       282\n","           1       0.41      0.07      0.11       136\n","           2       0.61      0.10      0.18       212\n","           3       0.57      0.28      0.38       466\n","           4       0.74      0.85      0.79       904\n","\n","    accuracy                           0.60      2000\n","   macro avg       0.54      0.45      0.40      2000\n","weighted avg       0.61      0.60      0.55      2000\n","\n","\n","\n","180/180 [==============================] - 190s 1s/step - loss: 1.1481 - accuracy: 0.6770 - val_loss: 1.1998 - val_accuracy: 0.6310\n","Epoch 3/10\n","180/180 [==============================] - ETA: 0s - loss: 1.1093 - accuracy: 0.7224\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.64      0.94      0.76      2672\n","           1       0.88      0.18      0.30      1457\n","           2       0.62      0.55      0.58      1989\n","           3       0.74      0.59      0.66      3971\n","           4       0.83      0.92      0.87      7911\n","\n","    accuracy                           0.75     18000\n","   macro avg       0.74      0.64      0.63     18000\n","weighted avg       0.76      0.75      0.73     18000\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.55      0.88      0.67       282\n","           1       0.27      0.02      0.04       136\n","           2       0.42      0.42      0.42       212\n","           3       0.50      0.36      0.42       466\n","           4       0.75      0.81      0.78       904\n","\n","    accuracy                           0.62      2000\n","   macro avg       0.50      0.50      0.47      2000\n","weighted avg       0.59      0.62      0.59      2000\n","\n","\n","\n","180/180 [==============================] - 191s 1s/step - loss: 1.1093 - accuracy: 0.7224 - val_loss: 1.1859 - val_accuracy: 0.6270\n","Epoch 4/10\n","180/180 [==============================] - ETA: 0s - loss: 1.0774 - accuracy: 0.7558\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.77      0.92      0.84      2672\n","           1       0.73      0.58      0.65      1457\n","           2       0.73      0.60      0.66      1989\n","           3       0.76      0.66      0.71      3971\n","           4       0.85      0.92      0.88      7911\n","\n","    accuracy                           0.80     18000\n","   macro avg       0.77      0.74      0.75     18000\n","weighted avg       0.79      0.80      0.79     18000\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.62      0.74      0.67       282\n","           1       0.31      0.32      0.31       136\n","           2       0.40      0.37      0.39       212\n","           3       0.46      0.38      0.42       466\n","           4       0.76      0.79      0.78       904\n","\n","    accuracy                           0.61      2000\n","   macro avg       0.51      0.52      0.51      2000\n","weighted avg       0.60      0.61      0.61      2000\n","\n","\n","\n","180/180 [==============================] - 190s 1s/step - loss: 1.0774 - accuracy: 0.7558 - val_loss: 1.1969 - val_accuracy: 0.6160\n","Epoch 5/10\n","180/180 [==============================] - ETA: 0s - loss: 1.0529 - accuracy: 0.7806\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.84      0.89      0.86      2672\n","           1       0.73      0.66      0.69      1457\n","           2       0.74      0.67      0.70      1989\n","           3       0.81      0.66      0.73      3971\n","           4       0.84      0.93      0.88      7911\n","\n","    accuracy                           0.82     18000\n","   macro avg       0.79      0.76      0.77     18000\n","weighted avg       0.81      0.82      0.81     18000\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.66      0.63      0.64       282\n","           1       0.30      0.36      0.33       136\n","           2       0.40      0.41      0.40       212\n","           3       0.49      0.36      0.42       466\n","           4       0.74      0.82      0.78       904\n","\n","    accuracy                           0.61      2000\n","   macro avg       0.52      0.52      0.51      2000\n","weighted avg       0.60      0.61      0.60      2000\n","\n","\n","\n","180/180 [==============================] - 191s 1s/step - loss: 1.0529 - accuracy: 0.7806 - val_loss: 1.2008 - val_accuracy: 0.6180\n","Epoch 6/10\n","180/180 [==============================] - ETA: 0s - loss: 1.0293 - accuracy: 0.8044\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.80      0.94      0.87      2672\n","           1       0.73      0.70      0.71      1457\n","           2       0.84      0.62      0.72      1989\n","           3       0.80      0.71      0.75      3971\n","           4       0.87      0.92      0.89      7911\n","\n","    accuracy                           0.83     18000\n","   macro avg       0.81      0.78      0.79     18000\n","weighted avg       0.83      0.83      0.82     18000\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.60      0.77      0.67       282\n","           1       0.30      0.38      0.33       136\n","           2       0.40      0.27      0.33       212\n","           3       0.47      0.40      0.43       466\n","           4       0.77      0.78      0.77       904\n","\n","    accuracy                           0.61      2000\n","   macro avg       0.51      0.52      0.51      2000\n","weighted avg       0.60      0.61      0.60      2000\n","\n","\n","\n","180/180 [==============================] - 229s 1s/step - loss: 1.0293 - accuracy: 0.8044 - val_loss: 1.2042 - val_accuracy: 0.6125\n","Epoch 7/10\n","180/180 [==============================] - ETA: 0s - loss: 1.0132 - accuracy: 0.8208\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.85      0.93      0.89      2672\n","           1       0.83      0.70      0.76      1457\n","           2       0.85      0.69      0.76      1989\n","           3       0.74      0.76      0.75      3971\n","           4       0.88      0.90      0.89      7911\n","\n","    accuracy                           0.83     18000\n","   macro avg       0.83      0.80      0.81     18000\n","weighted avg       0.83      0.83      0.83     18000\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.63      0.71      0.67       282\n","           1       0.27      0.24      0.25       136\n","           2       0.42      0.35      0.38       212\n","           3       0.44      0.52      0.48       466\n","           4       0.78      0.73      0.76       904\n","\n","    accuracy                           0.60      2000\n","   macro avg       0.51      0.51      0.51      2000\n","weighted avg       0.61      0.60      0.61      2000\n","\n","\n","\n","180/180 [==============================] - 192s 1s/step - loss: 1.0132 - accuracy: 0.8208 - val_loss: 1.2109 - val_accuracy: 0.6045\n","Epoch 8/10\n","180/180 [==============================] - ETA: 0s - loss: 1.0023 - accuracy: 0.8320\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.85      0.93      0.89      2672\n","           1       0.77      0.74      0.76      1457\n","           2       0.81      0.72      0.76      1989\n","           3       0.83      0.72      0.77      3971\n","           4       0.87      0.94      0.90      7911\n","\n","    accuracy                           0.85     18000\n","   macro avg       0.83      0.81      0.82     18000\n","weighted avg       0.85      0.85      0.85     18000\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.62      0.66      0.64       282\n","           1       0.26      0.36      0.30       136\n","           2       0.35      0.34      0.35       212\n","           3       0.45      0.34      0.39       466\n","           4       0.76      0.79      0.77       904\n","\n","    accuracy                           0.59      2000\n","   macro avg       0.49      0.50      0.49      2000\n","weighted avg       0.59      0.59      0.59      2000\n","\n","\n","\n","180/180 [==============================] - 191s 1s/step - loss: 1.0023 - accuracy: 0.8320 - val_loss: 1.2169 - val_accuracy: 0.5945\n","Epoch 9/10\n","180/180 [==============================] - ETA: 0s - loss: 0.9941 - accuracy: 0.8398\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.80      0.95      0.87      2672\n","           1       0.90      0.71      0.80      1457\n","           2       0.87      0.70      0.78      1989\n","           3       0.83      0.74      0.78      3971\n","           4       0.87      0.94      0.90      7911\n","\n","    accuracy                           0.85     18000\n","   macro avg       0.85      0.81      0.83     18000\n","weighted avg       0.85      0.85      0.85     18000\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.60      0.80      0.69       282\n","           1       0.29      0.24      0.26       136\n","           2       0.39      0.30      0.34       212\n","           3       0.45      0.37      0.41       466\n","           4       0.75      0.80      0.77       904\n","\n","    accuracy                           0.61      2000\n","   macro avg       0.50      0.50      0.49      2000\n","weighted avg       0.59      0.61      0.59      2000\n","\n","\n","\n","180/180 [==============================] - 193s 1s/step - loss: 0.9941 - accuracy: 0.8398 - val_loss: 1.2073 - val_accuracy: 0.6090\n","Epoch 10/10\n","180/180 [==============================] - ETA: 0s - loss: 0.9833 - accuracy: 0.8506\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.87      0.95      0.91      2672\n","           1       0.91      0.74      0.82      1457\n","           2       0.86      0.74      0.79      1989\n","           3       0.83      0.76      0.80      3971\n","           4       0.87      0.94      0.91      7911\n","\n","    accuracy                           0.86     18000\n","   macro avg       0.87      0.83      0.84     18000\n","weighted avg       0.86      0.86      0.86     18000\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.64      0.74      0.69       282\n","           1       0.26      0.18      0.21       136\n","           2       0.40      0.37      0.38       212\n","           3       0.46      0.43      0.44       466\n","           4       0.75      0.79      0.77       904\n","\n","    accuracy                           0.61      2000\n","   macro avg       0.50      0.50      0.50      2000\n","weighted avg       0.60      0.61      0.60      2000\n","\n","\n","\n","180/180 [==============================] - 191s 1s/step - loss: 0.9833 - accuracy: 0.8506 - val_loss: 1.2039 - val_accuracy: 0.6135\n"]}],"source":["np.random.seed(0)\n","tf.random.set_seed(0)\n","cnn_bilstm_history = model.fit(train_feats_matrix, train_label_matrix,\n","                    validation_data=(valid_feats_matrix, valid_label_matrix), # added by HCJ\n","                    epochs=10, batch_size=100, verbose=1, shuffle=True, # added verbose=1, changed epochs\n","                    callbacks=[checkpointer, earlystopping, metrics]) # added metrics to compute F1 score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":280},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1649177264240,"user":{"displayName":"Jasper Chow","userId":"06566271520968338357"},"user_tz":-480},"id":"0yGHJmVXk351","outputId":"ac414eae-701d-4142-e078-f7273a08fefe"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 720x288 with 2 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAmcAAAEHCAYAAADrruHWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeVyVZfr48c8NIrjhmqTiOpkrnAMSookpWjppqKWD4NhMVtZkVuOMZaVp9bNRx0lTp2lxFNvUZtyKr7mkkprmiHE0RUwtUtTccUdZrt8f9+GIrAcEDsv9fr3Oy/Ns93OdAz1d3KsSEQzDMAzDMIzywc3VARiGYRiGYRg3meTMMAzDMAyjHDHJmWEYhmEYRjlikjPDMAzDMIxyxCRnhmEYhmEY5YhJzgzDMAzDMMqRaq4OoCQ1atRIWrVq5eowDMMoI7t27TojIneURtlKqf7AO4A7MF9EpuU43gJYBNSznzNBRFYrpVoB+4ED9lO/E5GnC7ufeX4ZRtWT3zOsUiVnrVq1Ii4uztVhGIZRRpRSv5RSue7AP4H7gWRgp1LqCxFJyHbaROBzEfmXUqojsBpoZT92WESsRbmneX4ZRtWT3zPMNGsahmHkFgwcEpGfROQGsAQYlOMcAbzt7+sCx8swPsMwKjGTnBmGYeTWDDiabTvZvi+7KcDvlVLJ6FqzsdmOtVZKxSulvlFKhZZqpIZhVDomOTMMwyieSCBaRHyBB4GPlVJuwAmghYgEAOOAz5RS3nkVoJQarZSKU0rFnT59uswCNwyjfKtUfc4M56SlpZGcnExqaqqrQzEMp3h5eeHr64uHh0dZ3fIY0Dzbtq99X3aPA/0BRGS7UsoLaCQip4Dr9v27lFKHgbuBXB3KROQD4AOAoKAgs9CxYRiASc6qpOTkZOrUqUOrVq1QSrk6HMMokIhw9uxZkpOTad26dVnddifQVinVGp2UDQeicpxzBOgDRCulOgBewGml1B3AORHJUEq1AdoCP5VV4IZhVHymWbMKSk1NpWHDhiYxMyoEpRQNGzYs05peEUkHngXWoqfF+FxE9iml3lBKhdtP+wvwpFJqN7AY+KOICNAT2KOUsgH/BZ4WkXNlFrxhGBWeqTmrokxiZlQkrvh9FZHV6I7+2fe9lu19AnBvHtctA5aVeoCGYVRaVbLm7Isv4J13XB2FYRiGYRiVxrHVhZ/jpCqZnMXEwMSJcPWqqyOputzd3bFarVgsFgIDA9m2bRsAx48fZ+jQoQDExsYycODAXNfGxMQQEBCAxWKhY8eOvP/++0ydOhWr1YrVanWUbbVamTNnDlOmTEEpxaFDhxxlzJ49G6VUkSb9TEpK4rPPPnNs22w2Vq++vf8Y33rrrVu2u3fvflvlZckZa1E4E8MTTzxBQkJCoecZhmFUSjcuQPIqsL0MYh/Lc2QpXDlSMuWLSKV5denSRZyxaZMIiCxd6tTplU5CQoKrQ5BatWo53q9Zs0Z69uyZ65xNmzbJgAEDbtl348YNadKkiRw9elRERFJTUyUxMTHfskVEJk+eLH5+fvLmm2869nXv3l06deokO3fudDrmnPEsXLhQxowZ4/T1eckZa0nJ67vLkpaWVir3LG15/d4CcVIOnj0l8XL2+WUYhotcOCCye7LI2u4in7mLfIrIkpoiV/T/j+TGJZHMzCIVmd8zrErWnIWGQrNmUMyKBaOEXbx4kfr16wO6xqdz5875nnvp0iXS09Np2LAhAJ6enrRr167QewwePJhVq1YBcPjwYerWrUujRo3yPDcpKYnQ0FACAwNvqdWbMGECW7ZswWq1Mn36dF577TWWLl2K1Wpl6dKlXLlyhVGjRhEcHExAQIDjftHR0Tz88MP079+ftm3b8uKLLzrKu3btGlarlREjRgBQu3ZtQP/RNH78eDp37oyfnx9Lly4FdG1ir169GDp0KO3bt2fEiBGI5J6BIXuss2bNIjo6mvDwcMLCwujTpw+XL1+mT58+BAYG4ufn54g1ewwF3atXr16OWsfatWvz6quvYrFYCAkJ4eTJk47vOSQkBD8/PyZOnOgo1zAMo9wTgYs/woF5+l+AlB9g7xuQmQYdX4I+m2DoOajpq4971IYS6h9bJQcEuLvD8OEwZw6cPw/2vKDq+rpX7n0tfgd3PwPpVyH2wdzH2/xRv1LPwNahtx7rG1voLbOSktTUVE6cOMHGjRudCrVBgwaEh4fTsmVL+vTpw8CBA4mMjMTNreC/M7y9vWnevDl79+5l1apVREREsHDhwjzPbdy4MevXr8fLy4uDBw8SGRlJXFwc06ZNY+bMmcTExADg4+NDXFwc8+bNA+CVV14hLCyMBQsWkJKSQnBwMH379gV0E2h8fLwjmRw7dizTpk1j3rx52Gy2XDEsX74cm83G7t27OXPmDPfccw89e/YEID4+nn379tG0aVPuvfdevv32W3r06HHL9TljjY6O5vvvv2fPnj00aNCA9PR0VqxYgbe3N2fOnCEkJITw8PBcHe+dudeVK1cICQlh6tSpvPjii3z44YdMnDiR559/nueff57IyEjee++9An8+hmEYLpeRCslfwK/r4MR6uGpvouwyF7zvhqYPwiNnwLNBqYdSJWvOAKKiIC0NlpkxVS5Ro0YNbDYbiYmJrFmzhkcffTTPGqC8zJ8/nw0bNhAcHMzMmTMZNWqUU9cNHz6cJUuWsHLlSoYMGZLveWlpaTz55JP4+fkxbNgwp/tWrVu3jmnTpmG1WunVqxepqakcOaL/4+7Tpw9169bFy8uLjh078ssvBa/XvXXrViIjI3F3d8fHx4f77ruPnTt3AhAcHIyvry9ubm5YrVaSkpKciu/++++nQQP9UBERXnnlFfz9/enbty/Hjh1z1Hhl58y9qlev7ugb2KVLF8c527dvZ9iwYQBEReWcIswwDMPFMlLh1w3w69d6WzJg+0g4sgwaBsE9/4KHDkG7Z/XxajXKJDGDKlpzBhAQAO3a6abNJ55wdTQuVlBNV7WaBR/3auRUTVlBunXrxpkzZyjK8jV+fn74+fkxcuRIWrduTXR0dKHXDBw4kPHjxxMUFIS3983VdFasWMHrr78O6MQvJiYGHx8fdu/eTWZmJl5eXk7FJCIsW7YsVzPrjh078PT0dGy7u7uTnp7uVJl5yausHTt28NRTTwHwxhtv3PL5stSqVcvx/tNPP+X06dPs2rULDw8PWrVqlec8Ys7E7eHh4ahxu93PZhiGUapS9sGJNfDreji1GTKuwR2hcGdfqFYLfhsPde4GN9emR1W25kwpiIyE2Fg4lnNRFqNMJSYmkpGR4ehHVpDLly8TGxvr2LbZbLRs2dKp+9SsWZPp06fz6quv3rJ/yJAh2Gw2bDYbQUFBXLhwgSZNmuDm5sbHH39MRkYGAHXq1OHSpUuO63Ju9+vXj7lz5zpqAOPj4wuNycPDg7S0tFz7Q0NDWbp0KRkZGZw+fZrNmzcTHBycbzldu3Z1fIbw8PBcseV04cIFGjdujIeHB5s2bSq0Jq84QkJCWGavml6yZEmJl28YhlGg9CtwehscztaFJX48xP9Vj6r8zZNw35fQ6/9uHq/b0eWJGVTh5Ax0ciYC9r7WRhnK6nNmtVqJiIhg0aJFuLu75zpvw4YN+Pr6Ol7x8fHMmDGDdu3aYbVamTx5slO1ZlmGDx9OYGBggec888wzLFq0CIvFQmJioqPGyd/fH3d3dywWC7NmzaJ3794kJCQ4BgRMmjSJtLQ0/P396dSpE5MmTSo0ntGjR+Pv7+8YEJBlyJAh+Pv7Y7FYCAsLY8aMGdx5551Of86cseY0YsQI4uLi8PPz46OPPqJ9+/ZOl+2s2bNn8/bbb+Pv78+hQ4eoW7duid/DMAzjFr9+DdtGQkxH+I83rL8XdoyCGyn6eOBMGHwUBiZA0DvQbCB41HFtzHlQzvbzKVbhSi0ABgKnRCTXEDyl1CDgTSATSAdeEJGt9mN/ACbaT/1/IrKosPsFBQVJUeatArjnHp2gFfGyCm3//v106NDB1WEYldzVq1epUaMGSimWLFnC4sWLbxkVWlR5/d4qpXaJSNDtxloeFOf5ZRhV0o0UOPc9nP8ezu3S7+/7ArzbwcH39YjKBl2gfiA0CNTvazQtsZGUJSm/Z1hp191FA/OAj/I5vgH4QkREKeUPfA60V0o1ACYDQYAAu5RSX4jI+ZIOMCoKxo2DAwd0HzTDMErGrl27ePbZZxER6tWrx4IFC1wdkmEYFU3qGZ2EebeDWi3h+FqI7X/zeM0WOvnKtPd1vetJaPuUa2ItQaWanInIZqVUqwKOX862WQudiAH0A9aLfbFgpdR6oD96ceESFREBf/kLLF4MU6aUdOmGUXWFhoaye/duV4dhGEZFcuMC/Dj3Zo1Y1nQWgW9D+z9DfStY/mavGQvQg9KyU5Wjt5bLe70ppYYAfwMaAwPsu5sBR7OdlmzfV+KaNoXevfWozcmTy2Wtp2EYhmFUPld+gaTFcGYb3HGvntjVrTr88DrUbgN3dIcGY+3Nk130NTV8oNME18ZdBlyenInICmCFUqonuv9Z36Jcr5QaDYwGaNGiRbFiiIrS02ns2gVBlaL3imEYhmGUUz9/DIf/Dae+0dveHXRyBnousWEpelqLKqzc1P+JyGagjVKqEXAMaJ7tsK99X17XfSAiQSISdMcddxTr3g8/DNWrm+WcDMMwDKPEZabBqS03t4+ugNRfwTIVBiXpkZMdX7p5vIonZuDi5EwpdZeyz16plAoEPIGzwFrgAaVUfaVUfeAB+75SUb8+PPggLFkC9imtDMMwDMMoLhHdb2zXC7CiGXzdEy4d1se6LYIB+6HTK7qTv5FLqSZnSqnFwHagnVIqWSn1uFLqaaXU0/ZTHgH2KqVswD+BCPtC7efQTZw77a83sgYHlJaoKDhxAr75pjTvYmRxd3fHarVisVhuWVz8+PHjDB2q1+qMjY11LAuUXUxMDAEBAVgsFjp27Mj777/P1KlTHfOmZZVttVqZM2cOU6ZMQSnFoUOHHGXMnj0bpRRFmbogKSmJz7JVr9psNlavXl3crwCAt95665bt7t2731Z5xRUdHc2zz+olSt577z0++ij3AOvCFqXPOif7dxQXF8dzzz1XssEahlG+nYuH1Z1hTRAc/Bc07gk9V0FNe4OYRx3TwbswIlJpXl26dJHiunpVpHZtkccfL3YRFUZCQoKrQ5BatWo53q9Zs0Z69uyZ65xNmzbJgAEDbtl348YNadKkiRw9elRERFJTUyUxMTHfskVEJk+eLH5+fvLmm2869nXv3l06deokO3fudDrmnPEsXLhQxowZ4/T1eckZq6s481l+/vln6dSpU4Hn5PUzKyl5/d4CcVIOnj0l8bqd55dhuNSNiyKHo0WSY/R26hmR9feJHHxf5Po5l4ZW3uX3DCs3fc5crUYN3ffsv/+F69ddHU3VcvHiRerXrw8UXjtz6dIl0tPTHUs9eXp65lrLMi+DBw92TIB6+PBh6tatS6NGjfI8NykpidDQUAIDA2+p1ZswYQJbtmzBarUyffp0XnvtNZYuXepYIeDKlSuMGjWK4OBgAgICHPeLjo7m4Ycfpn///rRt25YXX3zRUV7WSglZKwTUrl0b0H80jR8/ns6dO+Pn58dS+zIWsbGx9OrVi6FDh9K+fXtGjBiRa8H4zMxMWrVqRUpKimNf27ZtOXnyJF9++SVdu3YlICCAvn375rnY+ZQpU5g5cyag5yqzWCxYLBb++c9/Fuk7mjVr1i21n+fOnWPw4MH4+/sTEhLCnj17HPcbNWoUvXr1ok2bNsyZM6fQn6dhGC6WmaHnHNv2e1juA9/9UXf0B/BsqNdcvms0VK/vyigrLJeP1ixPoqLgo4/gq69g8GBXR1M2XngBbLaSLdNqhdmzCz4nKylJTU3lxIkTbNy40amyGzRoQHh4OC1btqRPnz4MHDiQyMhI3NwK/jvD29ub5s2bs3fvXlatWkVERAQLFy7M89zGjRuzfv16vLy8OHjwIJGRkcTFxTFt2jRmzpxJTEwMAD4+PsTFxTFv3jwAXnnlFcLCwliwYAEpKSkEBwfTt68efGyz2YiPj3ckk2PHjmXatGnMmzcPWx4/gOXLl2Oz2di9ezdnzpzhnnvuoWfPnoBes3Pfvn00bdqUe++9l2+//ZYePXo4rnVzc2PQoEGsWLGCxx57jB07dtCyZUt8fHzo0aMH3333HUop5s+fz4wZM/jHP/6R7/f22GOPMW/ePHr27Mn48eOL/B1lXwd18uTJBAQEsHLlSjZu3Mijjz7q+OyJiYls2rSJS5cu0a5dO/70pz/h4eFR4M/UMAwX2jwYjsfo5Kv1H6D1SGjUzdVRVRqm5iybPn3gjjvMqM2yUKNGDWw2G4mJiaxZs4ZHH300Vw1QfubPn8+GDRsIDg5m5syZjBo1yqnrhg8fzpIlS1i5ciVDhgzJ97y0tDSefPJJ/Pz8GDZsGAkJCU6Vv27dOqZNm4bVaqVXr16kpqZy5IieQLFPnz7UrVsXLy8vOnbsWOhC41u3biUyMhJ3d3d8fHy477772LlzJwDBwcH4+vri5uaG1WolKSkp1/URERGO2rYlS5YQEREBQHJyMv369cPPz4+///3v7Nu3L98YUlJSSElJcSSFI0eOvK3vaOvWrY4ywsLCOHv2LBcvXgRgwIABeHp60qhRIxo3bpxnjZ5hGC5y9TjsnwlrguGGfaGeu5+B0GUw5AQE/0vPSWb6kZUYU3OWTbVqesWA+fPh4kXw9nZ1RKWvsBqustCtWzfOnDnD6dOnnb7Gz88PPz8/Ro4cSevWrZ1a/HzgwIGMHz+eoKAgvLP9cFesWMHrr78O6MQvJiYGHx8fdu/eTWZmJl5eXk7FJCIsW7YsVzPrjh078PT0dGy7u7uTnp7uVJl5yausHTt28NRTesmSN954g4ceeohDhw5x+vRpVq5cycSJepnasWPHMm7cOMLDw4mNjWVKMZfFmDVrVrG+o6J8JsMwXCj9GhxdDj9/BCe/BsmEhl3h2gldW9b0t66OsFIzNWc5REVBaiqsXOnqSKqOxMREMjIyHP3ICnL58uVbmspsNhstWzo3FLtmzZpMnz6dV1999Zb9Q4YMwWazYbPZCAoK4sKFCzRp0gQ3Nzc+/vhjMuzzq9SpU4dLly45rsu53a9fP+bOneuoAYyPjy80Jg8PD9LS0nLtDw0NZenSpWRkZHD69Gk2b95McHBwvuV07drV8RnCw8NRSjFkyBDGjRtHhw4dHN/thQsXaNZML7axaNGiAmOrV68e9erVY+vWrQB8+umnjmPOfkc5P1NWGbGxsTRq1OiWJNkwjHLk0kHY/nu49CN0fAUGHoB+30Hdjq6OrEowyVkOISHQqpVp2ixtWX3OrFYrERERLFq0CHd391znbdiwAV9fX8crPj6eGTNm0K5dO6xWK5MnT3aq1izL8OHDCQwMLPCcZ555hkWLFmGxWEhMTKRWLT0hor+/P+7u7lgsFmbNmkXv3r1JSEhwDAiYNGkSaWlp+Pv706lTJyZNmlRoPKNHj8bf398xICDLkCFD8Pf3x2KxEBYWxowZM7jzzjud/pygmzY/+eQTR5Mm6M73w4YNo0uXLvkOiMhu4cKFjBkzBqvVekuzs7PfUXZTpkxh165d+Pv7M2HChEKTQ8MwylBmBvy0COLtk8HW94ff7obww2B5E7zvdm18VYxytp9PRRAUFCRFmbcqP6++CtOnw7Fj4ONTAoGVM/v376dDhw6uDsMwiiSv31ul1C4RqRSLrpXU88swikQydfPlntfg4n5oEAT3bwV3z8KvNW5bfs8wU3OWh6govVLAf/7j6kgMwzAMo5Sk/ABr7oGtwwCBHv+BfjtMYlYOmOQsD506gb8/LF7s6kgMwzAMo4SlXdb/ejaCzOsQEg0P7oUWQ0GZtKA8MD+FfERFwbZt8PPPro6kdFSm5myj8jO/r4ZRAs7GwcZ+EPtbvfZljSbw4A/Q5g/glrvPr+E6JjnLx/Dh+t8lS1wbR2nw8vLi7Nmz5n94RoUgIpw9e/a2p+swjCorZR9sfhjW3gPn4qBZuO5rBmZusnLKzHOWj5Yt4d579ajNl192dTQly9fXl+Tk5CLNK2YYruTl5YWvr6+rwzCMiudYDHwTDtVqQ+fJ0GEceJgpbMo7k5wVICoKxoyBH34APz9XR1NyPDw8aN26tavDMIxyTynVH3gHcAfmi8i0HMdbAIuAevZzJojIavuxl4HHgQzgORFZW5axG1XY1WNw9Sg0CgGfMOg8Ce4eC16FT59jlA+mWbMAw4aBu7uZ88wwqiKllDvwT+C3QEcgUimVcwbOicDnIhIADAfetV/b0b7dCegPvGsvzzBKT+oZ+P6v8OVd8N1jul9ZtZrg/7pJzCoYk5wV4I474IEH9KjNzExXR2MYRhkLBg6JyE8icgNYAgzKcY4AWW1EdYHj9veDgCUicl1EfgYO2cszjJJ344Kep+yL1nBgFrSIgF6rTX+yCswkZ4WIioJffoHt210diWEYZawZcDTbdrJ9X3ZTgN8rpZKB1cDYIlxrGCXjxFrY+6Ze7/LBvdAtGmqbrisVmUnOCjFoENSoYZo2DcPIUyQQLSK+wIPAx0o5P1GUUmq0UipOKRVnBugYTsu4DgfmwY/v6u0WQ+HBPdDjc6hrVn+pDExyVog6dSA8HD7/HPJYn9owjMrrGNA827avfV92jwOfA4jIdsALaOTktYjIByISJCJBd9xxRwmGblRap7ZCTHvYNRZ+Xaf3KTeoV4lGrRkmOXNGVBScOQNff+3qSAzDKEM7gbZKqdZKqeroDv5f5DjnCNAHQCnVAZ2cnbafN1wp5amUag20Bf5XZpEblY8I7J8JG3qBqga910LoCldHZZQSk5w5oX9/qF/fNG0aRlUiIunAs8BaYD96VOY+pdQbSqlw+2l/AZ5USu0GFgN/FG0fukYtAVgDjBGRjLL/FEalcd4G8S+C72DoHwdNHjAd/isxM8+ZE6pXh6FDdXJ29SrUrOnqiAzDKAv2OctW59j3Wrb3CcC9+Vw7FZhaqgEalV/qKfBqDA0C4IHvoOE9JimrAkqt5kwptUApdUoptTef4yOUUnuUUj8opbYppSzZjiXZ99uUUnGlFWNRREXBlSvw5ZeujsQwDMOo9ETg0AewqhX8ukHvaxRsErMqojSbNaPRky/m52fgPhHxA94EPshxvLeIWEUkqJTiK5LQUGjWzDRtGoZhGKUs/Qps/wP87ym4IxTqWQq/xqhUSi05E5HNwLkCjm8TkfP2ze/Qo5nKLXd3vRj6V1/BuXw/lWEYhmHchosHYG1XSPoE/KboyWTN7P5VTnkZEPA48FW2bQHWKaV2KaVGuyimXKKi9HQay5a5OhLDMAyjUjqxFlJP6tGYfpPBzaz6VRW5PDlTSvVGJ2cvZdvdQ0QC0WvajVFK9Szg+jKbxDEgANq1M02bhmEYRgnKuA7n4vX7u8fCgARocr9rYzJcyqXJmVLKH5gPDBKRs1n7ReSY/d9TwAoKWJOuLCdxVErXnn3zDSQnl+qtDMMwjKrgyi+wPhQ2hMH1c/p/NF5mQuKqzmXJmVKqBbAcGCkiP2bbX0spVSfrPfAAkOeIT1eIjNSDaJYudXUkhmEYRoV27P/gqwC4dABC/g2eDVwdkVFOlOZUGouB7UA7pVSyUupxpdTTSqmn7ae8BjQE3s0xZYYPsNU+qeP/gP8TkTWlFWdRtW0L99xjmjYNwzCMYpJM2P0qfDMQarWE/rug+cOujsooR0ptEloRiSzk+BPAE3ns/wko1+OGo6Lgz3+GxERo397V0RiGYRgVi4Jrx+E3T0CXOVCthqsDMsoZlw8IqIgiInS3gMWLXR2JYRiGUWGc/AZS9un/gQTPh64fmsTMyJNJzoqhSRPo3Vs3bYq4OhrDMAyjXJNMSJgOG8Ng9yt6n5kiwyiASc6KKSoKDh2CuHKxuJRhGIZRLt04D98MAtsEaD4Uun/i6oiMCsAkZ8X08MN6QXQzMMAwDMPI0+Wf4KtA+HUtdJkL9y4BjzqujsqoAExyVkz168ODD+opNTIyXB2NYRiGUe7U8IUGQdB3C7R71ixabjjNJGe3ISoKTpzQk9IahmEYBmmXYdc4PaGse3UI/Q806urqqIwKpmomZyk/QOxA3RfgNgwcCLVrm6ZNwzAMA7iwH9YGw4/vwMkNro7GqMCqZnImmXB8Nez9f7dVTI0auu/Zf/8L16+XUGyGYRhGxZP0GawJghtnofd6aDHM1REZFVjVTM7qW+A3o+DHuXDp0G0VFRUFFy7AV1+VUGyGYRhGxXJgHmwbAQ0CoX883Bnm6oiMCq5qJmcA/m+CW3WwvXRbxfTpA3fcYZo2K5J9++CTTyAtzdWRGIZRKbQYBn5ToM9GqNnU1dEYlUDVTc5qNIGOE+Docji1tdjFVKumVwz48ku4eLEE4zNKnAj861/QpQuMHAkWC6xd6+qoDMOokI6thi1DITMdaviA32Rw83B1VEYlUXWTM4D24yDwbWjQ5baKiYqC1FRYubKE4jJKXEoKDBsGzzwDvXrBp5/CjRvQv78e2HHggKsjNAyjQsjMgN0T4ZsBcPkw3Djn6oiMSqhqJ2fVakL7P9/22mYhIdCqlWnaLK+++w6sVli1CmbMgNWrdUK9b5/e3rwZOnfWi9mfv70BvIZhVGbXTsKmB2DfVPjNk3D/NvBq7OqojEqoaidnWX79GtaHQvrVYl2ulP6f/ddfw8mTJRybUWyZmTr5Cg3VP6MtW2D8eHCz/9Z7eurtgwfhscfgnXegbVvd9Jme7trYDcMoZ0Rgy8NwZjuEREPXD8yi5UapMckZgJsnnN4K+/9R7CKiovRKAf/5TwnGZRTbqVN6BYeXXoLBgyE+Xtdw5sXHBz74AL7/XtegPfMMBAToZNswjCpOBDLT9F94QfOg3w5o8wdXR2VUciY5A2gcCs0fgYRpcPV4sYro1An8/U3TZnmwYYPu7P/NN/Dee/D551CvXuHXWa2waRMsWwZXrsD998OgQbpmzTCMKuhGCmwZAt//RW83CIB6fq6NyagSTHKWxTodJA32TCp2EVFRsH07/PRTCcZVTvz4I+zf7+ooCoB8VQkAACAASURBVJaeDhMn6qSqXj343//gqaeKtpydUnpi4YQE+NvfYONGnXiPH6/nszMMo4o49z2s6QLH/g9q/8bV0RhVjEnOstT5Ddz9HPy0UC/BUQzDh+t/lywpwbhc7MoV+MtfoEMH6NhRj27csEHX9JcnR47oUZhTp+r+Y3Fx4Hcbf+B6ecGECTopHTkS/vEP3R/tgw/MQveGUamJwKEPYV13yLwBfTdD++ddHZVRxZjkLLvOE+HexeDdvliXt2wJPXroaRrKW/JSHGvX6j5Yb78No0frxMdmg759IShIJ6HloeP8qlW6SXL3bv3d//vfUKtWyZTdpIkub+dOaNdO18R16QKxsSVTvmEY5cy1Y7DrBWh8n57t/45uro7IqIJMcpZd9XrQMkK3bRUzu4qK0k1iP/xQwrGVoTNn4NFHdS2Zp6eeauJf/4JXXoGkJF17dPkyREbq2qS5c3UNW1m7fh2ee053+G/dWnfoj4oqnXt16aK/h88/13Om9e6tmz/LQxN2ZqarIzCMSuDaCf3cr+kLD2yDXqvBq5GrozKqKJOc5eWnaFgXokfoFNGwYXrVgIo4MEBE1zx16ACLF8OkSbqmLDT05jleXvDkk7r/2cqV0LSpTpBatNDnnzpVNrH++CN066YTwxdegG3bdKJYmpTSP9/9++H//T9Yt05/VxMmlM3qEBcu6D6N//63bmru319/79WqQfPmEBama/ZmztS1iQkJenJkwzAKceQ/8GU7+HmR3q5vATd318ZkVGlKSqn9TSm1ABgInBKRznkcHwG8BCjgEvAnEdltP9YfeAdwB+aLyDRn7hkUFCRxcXG3H/yxGPjmIegyF9o9W+TLBwyAvXvh559vzqlV3v3yCzz9NKxZA127wocfOt9na9s2+PvfdULg6Ql/+INOHkorWfrkE/jTn6B6dYiOhoceKp37FOb4cXj5ZfjoIz0dx9Sp8Mc/gvttPtPPn9eJVdZr3z7977FjN8+pUeNmP8AWLeDoUT2q9OBBOHv25nlK6cStbdvcrzZt9HdYkSmldolIkKvjKAkl9vwynJdxA+LHw49zoFE36PG5rjkzjDKS3zOsNJOznsBl4KN8krPuwH4ROa+U+i0wRUS6KqXcgR+B+4FkYCcQKSIJhd2zxB5uIrDxfkixwUMHoXr9Il3+ySe6E/mWLboPWnmWkQHz5sGrr+rtt96CMWOKl2AcOKA7zn/0kV4aacgQPcoxv/nFiuryZXj2WVi0SNfmffYZ+JaD5+jOnTdr7wIC9GS22Wsb83P2bO4ELCEBTpy4eU7NmjoB69hRjxrNet+qVf6J//nzNxO1Q4duvj948NYVENzcdD/JrGTtrrtuvm/dGjwqwDKBpZmcFfZHolJqFtDbvlkTaCwi9ezHMoCszg1HRCS8sPuZ5KyMXTkCWyPg7HfQ7s8QMN2sjWmUuTJPzuw3bQXE5JWc5TivPrBXRJoppbqhE7V+9mMvA4jI3wq7X4k+3M7vhq8C7OtvzizSpZcu6ZqUP/4R3n23ZMIpDT/8AE88oaecePBBHWvLlrdf7q+/6ubGd9/V/bN69IAXX9Q1isWtSdy9Wy8w/+OPerqM117TzXnlhYgeIPHSS7oWa9gwvTpBq1Zw+nTeSVj21SRq176ZeGVPxFq0KNna17Nnb03Wsr+yN826u+vYs9e0NWqkk+7r1/W/Wa/s2wUdc/bcmTP1RMDOKK3krKh/JCqlxgIBIjLKvn1ZRGoX5Z4mOStjyV/A9pHQdQG0eMTV0RhVVH7PsPLyv7fHga/s75sBR7MdSwa6lnlE9S3wm1Hw41zoMB5q+Dh9aZ06EB6uO4+/8075q4FITdV9pqZPh/r1dQ3U8OFFmw+sIHfeqZv4Xn5Z9496+239fbRvr2vSRozQzZ/OENGDEcaNgwYN9DQevXsXfl1ZU0oPkBg0SCcX06bBF1+At7dOzrLUqaMTrwEDbk3Cmjcvue+/IA0b6lfO2kwRHWdeNW5bt+pay4K4u+ufafXq+pXfey8v/Z0UdI7FUnqfvwiCgUMi8hOAUmoJMAjIrwY/EphcRrEZxZWZAWf/p0dg+oZD+E/g2dDVURlGbiJSai+gFbpGrKBzegP7gYb27aHoJoSs4yOBeQVcPxqIA+JatGghJerqcZFT24p16apVIiAyebJIcnLJhnU7YmNF7r5bx/aHP4icOVP697xxQ+TTT0WsVn3fJk1E/vY3kfPnC77u3DmRRx7R1/TvL3LyZOnHWlKOHhV55hmRJ54QefttkbVr9b7MTFdHVnSZmSInTogkJIgcPqx/n0+dEklJEbl2TSQ93XWxAXFSOs8up59DQEvgBOCebV+6/bn0HTC4gPuU3vPLuNW1UyIb7hf5rJrIhQOujsYwRCT/Z5hLkzPAHzgM3J1tXzdgbbbtl4GXnblfly5dSuGrs8u4UaTTr18XCQ7W3zCIBASITJok8r//iWRklFKMBTh/XmT0aB1L69Yi69aVfQyZmfq+99+v46hdW2TcOJEjR3Kfu22bSMuWItWqifz97675zozyr5wkZy8Bc3Psa2b/tw2QBPymsHuW6vOrqjv7vcjyZiKLPUUOflgx/0oyKqX8nmEuG0uolGoBLAdGisiP2Q7tBNoqpVorpaoDw4EvXBGjw57JsD4UxPkJpapXh+++06M2p03Tk6JOnQrBwXr6iccf11NRFNZcVBJWrNBNZ/Pn61GUP/yglzgqa0rp+65bpxciHzRIN/u2aaPnVduzR8/ZNX267lCvlG5S++tfK86oV6PSOAY0z7bta9+Xl+HA4uw7ROSY/d+fgFggoORDNJySshc29gXlBg9sh7ueKJs+BIZxG0rtf3lKqcXAdqCdUipZKfW4UupppdTT9lNeAxoC7yqlbEqpOAARSQeeBdaimzs/F5F9pRWnU+q0hbM7IKlok5cppfsUvfSSHrl58iR8/LFeZmjZMj2asWFDPV/VP/+pp7MoScePwyOP6MlSfXx0x/+ZM0tu9vzbYbXqUa2HD+sRmMuX675Gd92l5w17+GGdwHUt+96GhgFO/pGolGoP1Ec/67L21VdKedrfNwLuJf++akZpO/YluHtBn0164XLDqABKdbRmWSu10U6SCWu7QuqvMPAAVKt520WmpelaoZgY+PJL3fEa9NxiAwfqV9euxZvSIjNT15K9+KIeATdliu5QX94GJmR3/rzu+L9qFYwapZeLMn/cGoUp5ak0HgRmo6fSWCAiU5VSb6CbIb6wnzMF8BKRCdmu6w68D2Si/wCeLSL/Lux+ZrRmCRO5udrL9bNmtn+jXHLJVBplrVQfbqe2wNc9wf9NvQZnCfvxx5uJ2pYtev6xRo30FBcDB0K/fnqUW2EOHNCJzebNelTjBx/o2ijDqIzMJLRGnq4cha1DIfhDqO/v6mgMI1/5PcNMTx5nNQ6F5o/AgdmQfq3Ei7/7bl27tWmTXttyyRKdkH35Jfzud7r5s29fmD1bNwXmdOOG7tNmsei+W//+t552wiRmRlWnlHpIKWWedVXFtROwIQwuHgAp+hJ8hlEemJqzorhyFCQDarcqvXvkkJ6uBxZ8+aWuWUuw91xp317XqD30kG6ufOop3dH/d7/TnezvvLPMQjQMl3Gm5kwp9Ql6FPgydPNkYpkEV0Sm5qwEpJ6GDb3gyi/Qe72ez8wwyjHTrFmSRCD9CngUaQLwEvHTTzpJi4mB2Fjddw2gWTM9I394oYvEGEbl4WyzplLKGz1R7GOAAAuBxSJyqZRDdJpJzm7TjfM3a8x6fQU+97k6IsMolGnWLEnbfg+bB+kkrYy1aQPPPaenozh7Fv77Xz0Df0KCScwMIz8ichH4L7AEaAIMAb63L7tkVAZunlCzBfRcaRIzo8IrL8s3VSyNusGusXAsBnwfclkYderoqTIMw8ifUiocXWN2F/ARECwip5RSNdFTXMx1ZXzGbUq/ApnpUL0u3LfK1dEYRokwyVlxtH0KDs4D23ho2h/cyvEcFYZhPALMEpHN2XeKyFWl1OMuism4DWlpaSQnJ5Oaeg1ST+lWDC8fM/+OUW55eXnh6+uLh5NzWpnkrDjcPMD6d9gcDgffh3bPujoiwzDyNwW99iUASqkagI+IJInIBpdFZRRbcnIyderUplWDNFR6bajd2ixgbpRbIsLZs2dJTk6mdevWTl1j+pwVV7OB4BMGh/5VpGWdDMMoc/9BTwibJcO+z6igUlOv0bB6Cir9ItRqaRIzo1xTStGwYUNSU1OdvsbUnBWXUhCyEKrX12u2GYZRXlUTkRtZGyJyw74kk1FR3TiPSvOEWs3B6w5XR2MYhVJFbHI3WcXtqNUCPOrozqjXz7k6GsMw8nbaPigAAKXUIOCMC+Mxblc1b/389fJxWQju7u5YrVYsFguBgYFs27YNgOPHjzN06FAAYmNjGThwYK5rY2JiCAgIwGKx0LFjR95//32mTp2K1WrFarU6yrZarcyZM4cpU6aglOLQoUOOMmbPno1SiqJMv5KUlMRnn91cI9pms7F69erifgUAvPXWW7dsd+/e/bbKy5Iz1qJwJoYnnniChIRyvOStiFSaV5cuXaTMZWaIfHWPyOZHyv7ehlHFode5LPC5APwG+A44AhwFtgF3FXZdWb9c8vyqSDIzRZKWiGRmSEJCgqujkVq1ajner1mzRnr27JnrnE2bNsmAAQNu2Xfjxg1p0qSJHD16VEREUlNTJTExMd+yRUQmT54sfn5+8uabbzr2de/eXTp16iQ7d+50Ouac8SxcuFDGjBnj9PV5yRlrScnru8uSlpZWKvcsbXn93ub3DDM1Z7dLuUGzh+DoMr3+pmEY5YqIHBaREKAj0EFEuovIocKuM8oREYj/K3w7HI4ud3U0uVy8eJH69esDusanc+fO+Z576dIl0tPTadhQ95Pz9PSkXbt2hd5j8ODBrFqlpwo5fPgwdevWpVGjvBdzT0pKIjQ0lMDAwFtq9SZMmMCWLVuwWq1Mnz6d1157jaVLl2K1Wlm6dClXrlxh1KhRBAcHExAQ4LhfdHQ0Dz/8MP3796dt27a8+OKLjvKuXbuG1WplxIgRANSurSdnFxHGjx9P586d8fPzY+nSpYCuTezVqxdDhw6lffv2jBgxIuuPqFtkj3XWrFlER0cTHh5OWFgYffr04fLly/Tp04fAwED8/PwcsWaPoaB79erVy1HrWLt2bV599VUsFgshISGcPHnS8T2HhITg5+fHxIkTHeWWBaf6nCmlagHXRCRTKXU30B74SsQsXAZAh7/Aoffh+3HQb4fpg2YY5YxSagDQCfDK6vshIm+4NCjDeXsmQeLbcPdzeo3jxBwrcH3dK/c1LX4Hdz8D6Vch9sHcx9v8Ub9Sz+hF0rPrG1toSFlJSWpqKidOnGDjxo1OfZQGDRoQHh5Oy5Yt6dOnDwMHDiQyMhI3t4L/v+Ht7U3z5s3Zu3cvq1atIiIigoULF+Z5buPGjVm/fj1eXl4cPHiQyMhI4uLimDZtGjNnziQmJgYAHx8f4uLimDdvHgCvvPIKYWFhLFiwgJSUFIKDg+nbty+gm0Dj4+MdyeTYsWOZNm0a8+bNw2az5Yph+fLl2Gw2du/ezZkzZ7jnnnvo2bMnAPHx8ezbt4+mTZty77338u2339KjR49brs8Za3R0NN9//z179uyhQYMGpKens2LFCry9vTlz5gwhISGEh4fn6tvlzL2uXLlCSEgIU6dO5cUXX+TDDz9k4sSJPP/88zz//PNERkby3nvvFfjzKWnOZhGb0Q+1ZsA6YCQQXVpBVTjVaoL1b3AuDpKK10ZuGEbpUEq9B0QAYwEFDANaujQow3l7p8K+qfCbJ6HL7HIzl1mNGjWw2WwkJiayZs0aHn300TxrgPIyf/58NmzYQHBwMDNnzmTUqFFOXTd8+HCWLFnCypUrGTJkSL7npaWl8eSTT+Ln58ewYcOc7lu1bt06pk2bhtVqpVevXqSmpnLkyBEA+vTpQ926dfHy8qJjx4788ssvBZa1detWIiMjcXd3x8fHh/vuu4+dO3cCEBwcjK+vL25ublitVpKSkpyK7/7776dBgwaArpl75ZVX8Pf3p2/fvhw7dsxR45WdM/eqXr26o29gly5dHOds376dYcOGARAVFeVUjCXF2dGaSm5O2PiuiMxQSuVOlauyViPgwDs6OWv9e1dHYxjGTd1FxF8ptUdEXldK/QP4ytVBGU64nAR734RWIyH4vfwTs4JquqrVLPi4VyOnasoK0q1bN86cOcPp06edvsbPzw8/Pz9GjhxJ69atiY6OLvSagQMHMn78eIKCgvD29nbsX7FiBa+//jqgE7+YmBh8fHzYvXs3mZmZeHl5ORWTiLBs2bJczaw7duzA09PTse3u7k56erpTZeYlr7J27NjBU089BcAbb7xxy+fLUqtWLcf7Tz/9lNOnT7Nr1y48PDxo1apVnlNVOBO3h4eHo8btdj9bSXG25kwppboBI4D/s+9zL52QKijlBj2/gPu+dHUkhmHcKuuJfVUp1RRIQ6+vaZR3tVvBA9shZEG57i6SmJhIRkaGox9ZQS5fvkxsbKxj22az0bKlcxW5NWvWZPr06bz66qu37B8yZAg2mw2bzUZQUBAXLlygSZMmuLm58fHHH5ORkQFAnTp1uHTpkuO6nNv9+vVj7ty5jhrA+Pj4QmPy8PAgLS13D6fQ0FCWLl1KRkYGp0+fZvPmzQQHB+dbTteuXR2fITw8PFdsOV24cIHGjRvj4eHBpk2bCq3JK46QkBCWLVsGwJIlS0q8/II4+9v+AvAysEJE9iml2gCbSi+sCqpmU3BzhxspcP2sq6MxDEP7UilVD/g78D2QBJj+B+XZ4YXwU7R+3yAA3MrflJxZfc6sVisREREsWrQId/fcdRYbNmzA19fX8YqPj2fGjBm0a9cOq9XK5MmTnao1yzJ8+HACAwMLPOeZZ55h0aJFWCwWEhMTHTVO/v7+uLu7Y7FYmDVrFr179yYhIcExIGDSpEmkpaXh7+9Pp06dmDRpUqHxjB49Gn9/f8eAgCxDhgzB398fi8VCWFgYM2bM4M4773T6c+aMNacRI0YQFxeHn58fH330Ee3bt3e6bGfNnj2bt99+G39/fw4dOkTdunVL/B75Uc62kTsuUMoNqC0iF0snpOILCgqSosz5UirSr8GXd0HT30LX+a6NxTAqOaXULhEJKuC4GxAiItvs256Al4hcKKsYnVUunl/lQdJi2DYCmvSDXqvzbMrcv38/HTp0cEFwRlVy9epVatSogVKKJUuWsHjx4ltGhRZVXr+3+T3DnKo5U0p9ppTyto/a3AskKKXGFzvCyqxaDWg5HA4vgPO7XR2NYVRpIpIJ/DPb9vXymJgZdkeXw/aR0Pg+CF1Wbjr/G1XTrl27sFqt+Pv78+677/KPf/yjzO7tbLNmR3tN2WB0R9rW6BGbRl46T9TLOn3/Fz0/j2EYrrRBKfWIKur6KUbZOrZaz2PWMBju+0J35DcMFwoNDWX37t3s2bOHzZs3c9ddd5XZvZ1NzjyUUh7o5OwL+/xmJuvIT/X64DcFTm4wU2sYhus9hV7o/LpS6qJS6pJSqtx1y6jyUnZDPX/dlOlRx9XRGIZLOZucvY/uRFsL2KyUagkU+HBTSi1QSp1SSu3N53h7pdR2pdR1pdRfcxxLUkr9oJSyKaUqZieMtk9Do+5w2qwaYBiuJCJ1RMRNRKqLiLd9O/c4fcM1Muxr0nd6Ge7fCtXruTYewygHnBoCIyJzgDnZdv2ilOpdyGXRwDzgo3yOnwOeQ9fG5aW3iFTcxYndPCBsPbhV19upp8GzkelDYRhlTCnVM6/9IrK5rGMxcrh4EDY9oPuXNQgEd+fm4zKMys7Z5ZvqApOBrIfcN8AbQL4da0Vks1KqVQHHTwGn7MuqVE5ZfSaun4O1wXBnX7jnX+VyWLhhVGLZBy95AcHALiDMNeEYAGRch28jIO2i/sPVMAwHZ5s1FwCXgN/ZXxeBvBf1KhkCrFNK7VJKjS7oRKXUaKVUnFIqriizM5ep6vX1CgKH58OWR/Rab4ZhlAkReSjb636gM3De1XFVefHj4Xw8hERDrRaujqbI3N3dsVqtWCyWWxYXP378OEOH6rU6Y2NjHcsCZRcTE0NAQAAWi4WOHTvy/vvvM3XqVMe8aVllW61W5syZw5QpU1BKcejQIUcZs2fPRilFUaZfSUpK4rPPbvaDttlsrF69urhfAQBvvfXWLdvdu3e/rfKKKzo6mmeffRaA9957j48+yt1oV9ii9FnnZP+O4uLieO6550o2WGeISKEvwObMvjzOaQXsLeScKcBfc+xrZv+3MbAb6OlMnF26dJFy7cA8kU+VyNruIqlnXR2NYVR4QJw48WyQW58vCkgo6nWl/Sr3z6+SdGS5yKeIxL1QrMsTEhJKOKCiq1WrluP9mjVrpGfPnrnO2bRpkwwYMOCWfTdu3JAmTZrI0aNHRUQkNTVVEhMT8y1bRGTy5Mni5+cnb775pmNf9+7dpVOnTrJz506nY84Zz8KFC2XMmDFOX5+XnLG6ijOf5eeff5ZOnToVeE5eP7OSktfvbX7PMGdrzq4ppRzLuCul7gWu3WZemC8ROWb/9xSwAt0MUfHdPQZ6fK4XSI//i6ujMYwqQSk1Vyk1x/6aB2xBrxRguMqxL6BBEFinuzqSEnHx4kXq168PFF47c+nSJdLT0x1LPXl6euZayzIvgwcPdkyAevjwYerWrUujRnk3ByclJREaGkpgYOAttXoTJkxgy5YtWK1Wpk+fzmuvvcbSpUsdKwRcuXKFUaNGERwcTEBAgON+0dHRPPzww/Tv35+2bdvy4osvOsrLWikha4WA2rVrA7riZ/z48XTu3Bk/Pz+WLl0K6NrEXr16MXToUNq3b8+IESNyLRifmZlJq1atSElJcexr27YtJ0+e5Msvv6Rr164EBATQt2/fPBc7nzJlCjNnzgT0XGUWiwWLxcI//+mY8tCp72jWrFm31H6eO3eOwYMH4+/vT0hICHv27HHcb9SoUfTq1Ys2bdowZ84cbpeznZ+eBj6y9z0D3STwh9u+ex7sE926icgl+/sH0P3bKocWQ6FGU/Au+aUmDMPIU/Z2n3RgsYh866pgDKDrAkhLAffqt13UCy+AzVYCMWVjtcLs2QWfk5WUpKamcuLECTZu3OhU2Q0aNCA8PJyWLVvSp08fBg4cSGRkJG5uBdeVeHt707x5c/bu3cuqVauIiIhg4cK8exc1btyY9evX4+XlxcGDB4mMjCQuLo5p06Yxc+ZMYmJiAPDx8SEuLo558+YB8MorrxAWFsaCBQtISUkhODiYvn37AroJND4+3pFMjh07lmnTpjFv3jxsefwAli9fjs1mY/fu3Zw5c4Z77rmHnj11t/X4+Hj27dtH06ZNuffee/n222/p0cNR/4ObmxuDBg1ixYoVPPbYY+zYsYOWLVvi4+NDjx49+O6771BKMX/+fGbMmFHg5LCPPfYY8+bNo2fPnowff7P7qbPfUfZ1UCdPnkxAQAArV65k48aNPProo47PnpiYyKZNm7h06RLt2rXjT3/6Ex4eHgX+TAvi7GjN3YBFKeVt376olHoB2JPfNUqpxUAvoJFSKhk9oMDDfv17Sqk70Q9NbyDTXl5HoBGwwj5fZDXgMxFZU7yPV07dYW+Tz7gO30ZC+xegcZ4DygzDuH3/BVJFJANAKeWulKopIqbzZ1k7+B40eQBqt9F9cSuwGjVqOP7HvH37dh599FH27s1z5qhc5s+fzw8//MDXX3/NzJkzWb9+vVPraw4fPpwlS5awdu1aNmzYkG9ylpaWxrPPPovNZsPd3Z0ff/zRqbjWrVvHF1984ah1Sk1N5ciRIwD06dPHsbZkx44d+eWXX2jevHm+ZW3dupXIyEjc3d3x8fHhvvvuY+fOnXh7exMcHIyvry8AVquVpKSkW5IzgIiICN544w0ee+wxlixZQkREBADJyclERERw4sQJbty4QevWrfONISUlhZSUFEdSOHLkSL766qtif0dbt251LIQeFhbG2bNnuXhRzyo2YMAAPD098fT0pHHjxpw8edLxGYujSMMG5db1NMcB+f5tISKRhZT1K5BX5BcBS1HiqrBunIOL+2HjA9D9U2jxiKsjMozKaAPQF7hs364BrANc03O5qjq+Bnb+Cdo9D10KqZYqgsJquMpCt27dOHPmDEUZlObn54efnx8jR46kdevWTiVnAwcOZPz48QQFBeHtfXOqvhUrVvD6668DOvGLiYnBx8eH3bt3k5mZiZeXc1OUiAjLli3L1cy6Y8cOPD09Hdvu7u6kp6c7VWZe8iprx44dPPXUUwC88cYbPPTQQxw6dIjTp0+zcuVKJk6cCMDYsWMZN24c4eHhxMbGMmXKlGLFMGvWrGJ9R0X5TLfD2T5neTETdt2uGk30pIsNAmHrMDj4L1dHZBiVkZeIZCVm2N8XujaQUqq/UuqAUuqQUmpCHsdn2SfKtimlflRKpWQ79gel1EH7q1S6gFQoV4/rNTPr+YHlb66OpsQlJiaSkZHh6EdWkMuXL9/SVGaz2WjZsqVT96lZsybTp0/n1VdfvWX/kCFDsNls2Gw2goKCuHDhAk2aNMHNzY2PP/6YjIwMAOrUqcOlS5cc1+Xc7tevH3PnznX0AYuPjy80Jg8PD9LS0nLtDw0NZenSpWRkZHD69Gk2b95McHD+3ce7du3q+Azh4eEopRgyZAjjxo2jQ4cOju/2woULNGvWDIBFixYVGFu9evWoV68eW7duBeDTTz91HHP2O8r5mbLKiI2NpVGjRrckySXpdpIzs3xTSfBsCGFfQ7OBsPMZ2F92C6saRhVxRSkVmLWhlOpCIQOalFLu6AXTf4vubhGplOqY/RwR+bOIWEXECswFltuvbYDuxtEVPZhpslKqYrfh3Y7MDNg2Qk8hdO9SqFbD1RGViKw+Z1arlYiICBYtWoS7nN2IkwAAIABJREFUu3uu8zZs2ICvr6/jFR8fz4wZM2jXrh1Wq5XJkyc7VWuWZfjw4QQGBhZ4zjPPPMOiRYuwWCwkJiZSq1YtAPz9/XF3d8disTBr1ix69+5NQkKCY0DApEmTSEtLw9/fn06dOjFp0qRC4xk9ejT+/v6OAQFZhgwZgr+/PxaLhbCwMGbMmMGdd97p9OcE3bT5ySefOJo0QXe+HzZsGF26dMl3QER2CxcuZMyYMVit1lsGHjj7HWU3ZcoUdu3ahb+/PxMmTCg0ObwdKucoiVsOKnWJvJMwBdQQkXI1m2pQUJAUZc6XciUzXc/785snoF4nV0djGBWCUmqXiAQVcs49wBLgOPrZdScQISK7CrimGzBFRPrZt18GEJE8q32UUtuAySKyXikVCfQSkafsx94HYkVkcUFxVujnV0EOzIFdz0PIQmjzxxIpcv/+/XTo0KFEyjKMspLX721+z7ACkysRMavPlhW3atDFnqWL6AlrW424ucqAYRjFIiI7lVLtgayONAdEJHc7zK2aAUezbSeja8Jysa813BrIGq6X17XN8rl2NPD/27vz8KjK8//j7zsJkLAJSFgDBCGALAo24IILiCJoUWutorVaa6VudddW21/9qq21tvXbal2+WtG6VG1dEBVZquAKKJvIIhgQJAFk37csz++PZ9IMkABJZnLOzHxe13WuTM45M7nPBTzceZb7GQXQsWPiFWI9JF2ugIxG0FmjuyKHqjbDmhIvG2fDpz+Dd4fA7vVBRyOS0MzsWqCRc26ec24e0NjMronhjxgJvFK+GrQ6nHNPOOfynXP52dnZMQwpBHZvgJLtPjHrcoX2FRapBiVnYdTiGDjpFZ+kTRoI25cHHZFIIrvSOfffyfrOuY3AlQd5TxEQXScgJ3KuMiOB6CHL6rw3ObkyP89s0kl+zpmIVIuSs7DqcB6cOgl2fgsTT4BNXwQdkUiiSjer6LaJTPY/WPXTz4A8M+tsZvXxCdjYfW+KDJc2B6ZGnZ4ADDWz5pGFAEMj51LHwj/BqvHQ9UpI23+SfCwcaL60SNhU9++rkrMwa3USnP4hpNWDbV8HHY1IohoPvGxmQ8xsCL6X650DvcE5VwJch0+qFgL/cs7NN7N7zOzsqFtHAi+5qJbXObcBuBef4H0G3BM5lxrWToXP74QO50PXq+LyIzIzM1m/fr0SNEkIzjnWr19frVpqB1ytmWiSdrVT6S5Ij/yhbv8GGiXpxGGRajrE1Zpp+En3QyKn5gJtnHPXxju+6kiK9mvPRhjXFywdhs+C+s3i8mOKi4spLCxk165dcfl8kVjLzMwkJydnvy2darRaU0KiPDH79n2YfDr0exC6XxdsTCIJwjlXZmbTgS7ABfgt4l4NNqokVbwNGnXwbVScEjPwhU8PtG2PSKJTcpZIDh8AbYfDzJ/DrlVw1G+1AkqkCmbWDbgocqwDXgZwzg0OMq6k1qgDnPah2iWRWtKcs0SSkQUnvQpdR8H8+2D6T6DsYOWaRFLWl8CpwHedcyc65x4GtHQwHjbMho8u9MOaSsxEak09Z4kmLQP6Pw5Z7eCL/4E2p0PuxUFHJRJG5+En7E82s/H4XQKUOcRa8Vb46AIo3amyGSIxouQsEZlBn7ug9WDIPsmfKyvxiZuIAOCcGwOMMbNGwDnAjUArM3sMeN05NzHQAJOBc75g9valMGQKZB58r0MROTgNayayVif7RG1rAbzVHVYesDqASEpyzm13zv3TOTcCXxB2NvCLgMNKDktHw/IXoc/dvvSPiMSEkrNk4BxkNIYpZ8KcO3wvmojsxzm3MbJl0pCD3y0HVLoH5v0OWg+BnncEHY1IUtE4WDJomgdDp8GsG2HB/bD2Ixj4IjTMCToyEUlW6fVh6CeAxW0XAJFUpZ6zZJGRBQP+D45/3u/JufDPQUckIsmq8A0/+T+rDWS1DjoakaSj5CzZdP4hDJsNR9/nv99RqGFOEYmdZf+ED86FpU8FHYlI0lJyloya5vmetNLd8O4QeHewT9JERGpjy1d+dWb2QDjiJ0FHI5K0lJwls/QGvuTGxjnwTj9YOT7oiEQkUZXuho8vhLT6cMKLKt0jEkdxS87MbLSZrTGzeVVc72FmU81st5ndus+1YWa2yMwKzOyX8YoxJeReDMNm+KK1U4ZrNaeI1MycX/j5rMc947dpEpG4iWfP2TPAsANc3wBcD/wp+qSZpQOPAMOBnsBFZtYzTjGmhqbd/WrOrqNgzfuACzoiEUk0nS6Go38POSOCjkQk6cWtX9o594GZ5R7g+hpgjZmdtc+lAUCBc24pgJm9hK/uvSBOoaaG8tWcJTshrR7sXg8bZkLboUFHJiJhVrrbT5FoOcAfIhJ3YZxz1h5YEfV9YeScxEJGlv86716YfIaGOUWkamXFfkHR578KOhKRlBLG5KxazGyUmc0wsxlr164NOpzEcfTv/TDngvu1mlNEKvf5r2HdVGjeN+hIRFJKGJOzIiB6tmlO5FylIlux5Dvn8rOzs+MeXNIoH+Y84YXIas6+sHZq0FGJSFisnAALH4CuV0HHHwQdjUhKCWNy9hmQZ2adzaw+MBIYG3BMyat8NWeL/tCkS9DRiEgYOAezb4GmPeCYB4OORiTlxG1BgJm9CAwCWppZIXAXUA/AOfe4mbUBZgBNgTIzuxHo6ZzbYmbXAROAdGC0c25+vOIU/GrOwe/412UlMPs2OPIW7c0pkqq2FsC2ryH/bxXzVEWkzsRzteZFB7m+Gj9kWdm1ccC4eMQlB7HpC1jyJCx7Do5/DtoNDzoiEalrTfPge0WQnhl0JCIpKYzDmhKkFv1g2MxI0doztZpTJNWU7vHDmvWbKTkTCYiSM9lf0+4wdHrFas5plwcdkYjUlS/uggkDfJImIoHQ5mhSufLVnK1O8cka+GKUafXBLNjYRCQ+Snb6aQ3ZJ0N6/aCjEUlZ6jmTA8u9GFp8x7+eeQO8NwQ2zAo2JhGJj+Uv+d1Duv886EhEUpqSMzl0zfv6BQPj82HqZSpcK5JMnIPFD8NhvaHVoKCjEUlpSs7k0OVdBSMK4Mjb/G/Yb3aDb14JOioRiYV1n8DG2dDtOk1dEAmYkjOpnvqHQb8/wHcXQYfvQ4t8f37XOq3qFElkzY+B456GzpcEHYlIylNyJjXTOBdOeM5/BZh6CbzTz2/5IiKJJyMLjvgxZDQKOhKRlKfkTGrPOejyUyjdAVOGweRhsGle0FGJyKFa/Ch8+Vf/b1lEAqfkTGrPDDqeD2ctgH5/hnXT4Z2jYcXrQUcmUmNmNszMFplZgZn9sop7LjCzBWY238z+GXW+1MzmRI5w7w1cuhvm3Q2r/6O5ZiIhoTpnEjvpDeDIm+GIy2Dhn6DNEH9+y2K/T2dGw2DjEzlEZpYOPAKcDhQCn5nZWOfcgqh78oA7gIHOuY1m1irqI3Y65/rWadA19c2/Ydcalc8QCRH1nEnsNTgc+v4e6jUFVwYffg/e6g5Ln/Xfi4TfAKDAObfUObcHeAk4Z597rgQecc5tBHDOranjGGNj8cO+0HSb04KOREQilJxJfFka9H8MMtvAtMtgfH/4dkrQUYkcTHtgRdT3hZFz0boB3czsYzObZmbDoq5lmtmMyPlz4x1sja37FNZ/CnnX+n+rIhIK+tco8dfqZDhjOhz/POxeC+8OhpXjg45KpLYygDxgEHAR8KSZNYtc6+ScywcuBv5iZl0q+wAzGxVJ4masXbu2LmLeNwBod5afiiAioaHkTOqGpUHnH/r6aP0fhTan+/NrPvA10kTCpQjoEPV9TuRctEJgrHOu2Dn3NbAYn6zhnCuKfF0KTAH6VfZDnHNPOOfynXP52dnZsX2CQ3F4fxj0lp+CICKhoeRM6lZGFuRdDWnpULoHProA3uwKC/4IpbuCjk6k3GdAnpl1NrP6wEhg31WXY/C9ZphZS/ww51Iza25mDaLODwQWEDbfvg87VgYdhYhUQsmZBCe9Pgx5D7IHwpzb4a0jYfnLqrUkgXPOlQDXAROAhcC/nHPzzeweMzs7ctsEYL2ZLQAmA7c559YDRwIzzOzzyPn7o1d5hkJZMXxyMUz/adCRiEglVEpDgnVYTxj0tq+xNOsW+HgkNOwA2ScEHZmkOOfcOGDcPud+E/XaATdHjuh7PgH61EWMNbbiNdi5EgY8EXQkIlIJ9ZxJOLQ5DYbNgkHjKhKz5S/D9hUHfp+IVN/ih6HxEdB22MHvFZE6p+RMwiMtHdoN96+Lt8Cno3x9tLl3Qcn2YGMTSRYbZsPaj335jLT0oKMRkUooOZNwqtcUhn8O7c+GeffAm91UxFYkFtZ94v99dflJ0JGISBXilpyZ2WgzW2Nmle6Abd5DkX3r5prZMVHXEmdfOomfxrlw4ktw+keQ1R6m/wS2LQ06KpHE1u1aOOcbqN/s4PeKSCDiuSDgGeBvwLNVXB+OrwmUBxwLPBb5Com0L53EX/ZAOGMabJgJTbr6c1/+BXLO9QmciBya4m1QrzHUP+yAtzkH//gHTJ0K7dv7Iyen4nWzZtojXSSe4pacOec+MLPcA9xyDvBsZMXTNDNrZmZtnXOr4hWTJDBL8wUzAXYUwee/gjm/hB43Q687oF6TYOMTCbuyEhjXB3IvhqN/V+VtK1bAFVfApElw2GGwefP+9zRsuH/CVv66/Gvr1pCuKW0iNRJkKY2q9q5bRWRfOqAEXyNoTADxSVg1bA8jFsGcO2HB72HpaDjqt3DE5ZrgLFKVojdh+zJokV/pZefg6afhppugtBQeewx+9jMoLoaVK6GoyB+FhXt//egj/7W4eO/PS0+Htm0PnsRlZsb/0UUSTVjrnHVyzhWZ2RHAe2b2hXNuSWU3mtkoYBRAx44d6zJGCVLDHDjhWeh2Hcy6CWbf6oc5M1sGHZlIOC1+GBp2hPYj9rtUVASjRsG4cTBoEIweDZ07+2v160Nurj+qUlYG69ZVJGzRyVtRESxY4HvitmzZ/73Z2fDb3/qfLyJekMlZlXvXRe9LZ2ZT8PvSVZqcOeeeAJ4AyM/PV2n5VNNygF8wsLXAJ2auzA93dh1VMT9NJNVtmgffToa+90NaRbPvHDz/PFx/PezZAw8/DNdcA2nVXCqWlgatWvnjmGOqvm/r1v2Tt0mTfA9dWRlcdVUNn08kyQSZnI0FrjOzl/ALATY751aZWXNgh3Nud9S+dA8EGKeEnRk0zfOvNy+Arx6DRX+BbtdD719rVZrIV49CeiZ0qdiuafVqnxSNHQsDB8Izz0DXOP8+06QJ9Ojhj3K33grnnw9XX+2TPPWgicS3lMaLwFSgu5kVmtkVZnaVmZX/bjQOWAoUAE8C10TOh39fOgmvZr1hxGLI/RF8+SC8mQeLH/WToUVSVZ974MRXocHhOAcvvgi9esHEifDgg/D++/FPzKpSvz78+99w1lk+Wfz734OJQyRMzCXRJtP5+fluxowZQYchYbFhtp+PtqMIzprvN1qXpGJmM51zlc9wTzB10X6tWeN7qF57DY47zveWde8e1x95yHbvhvPO8/PennoKfqIauZICqmrDtEOAJK8W/WDIZDj9Q5+YlWyHqT+GzV8GHZlI3SgrhY8ugNX/4d//9r1lb70Ff/iDX2UZlsQMoEEDePVVGDYMfvpTnziKpColZ5LczCCrjX+9cS4Uvg7jesOnV/v5aSLJbNU7rJv/HiOv7MYFF/gVl7Nnw+23h7MGWWYmvP46nH667zl7tqoS5iJJTsmZpI7s42FEgV/JufQpeLsXTDpZm6pL0hrz1Ax6/WIhr03owO9+5yv+9+wZdFQHlpkJY8bAkCHw4x/71aQiqUbJmaSWzGzo/yicWwR9H4CGHSCjkb+27J+wZXGw8YnEwIYNcMmFW/jeXf9D+3YlzJxp3HknZIS1suU+srLgjTfg1FPhssvghReCjujQLF/u5/WJ1JaSM0lNmdnQ8zYYGGn1S7bDp6Pgre7w7hBY/i8o3RNsjCI18NZbfm7Zy6824u7z72b6tDT69Ak6qupr2NCX+TjlFLj0Ur/CNKxKSuD++6FbN+jbFxbrdzypJSVnIuB7z0Z85fcc3LYEPr4Q3ugAK98JOjKRQ7Jpkx8GHDHCV93/9PV3+M3/K6Ve09ZBh1ZjDRvCm2/CySfDJZfAyy8HHdH+Fi70deLuuMMvZigp8bssLFoUdGSSyJSciZTLagu97oQRS2DQOGh5PDSJFLdd9ymsGKN6aRJK48dD795+ftavfw0zZkC/Ed+Fo+4JOrRaa9TI9waeeCL88Ie+JloYlJbCn/8M/fpBQYHv2RszBiZP9tcGDYIvtTBcakjJmci+0tKh3XA4eUzFFlAFj8OH34M3OsHcu2D7imBjFMHvVXnllTB8OBx2GEybBvfeU0b9wmegeGvQ4cVMo0bw9ttw/PFw0UW+5EaQFi/2vXm33up7y+bPh5Ej/eLwXr18guacT9AWLgw2VklMSs5EDsWAJ+DkN6DZ0TDvXhibC59qI0AJzn/+43vLRo+GX/wCZs6E/Hxg9X9g2uVQODboEGOqcWNfoPa443wi9NprdR9DWRn89a9+XtmCBfDcc770R5s2e9/XsydMmeKTtcGD/b1S94qLfemYxx+Hm2/2PZ1vvAHz5sGOHUFHd2AJsnZHJGBpGZBztj+2LYMlT0JmZC5PWTF8+RfofIkfGhWJo23b4Lbb/H843bvDxx/7hOW/Fj0Mma2g4/mBxRgvTZrAO+/43qoLL/RDnOeeWzc/e8kSX3vtgw/gzDPhySehXbuq7+/Rw/egDR7sj/fe871qEh/OwTffwPTpFcesWbBzp7/eoIHfhSJa27bQpYvfuqxLl72PFi18ch0Ubd8kUlvfvg/vDgJLh5xzoOvPoM1pYOqYjrdU3L5pzBi/zdHNN8O99/qyE/+1bSmM7Qq9f50U882qsmULnHGGn1v3yitwzjnx+1llZfDYY75wb0aG7zm77LJD/4970SKfnJWU+AStd+/4xZpKNm3yf/7lidinn8K33/prmZlwzDEwYAAce6w/cnNh40afZJcfBQUVr1eu3PvzmzXbP2ErT+LatYO0GDXvVbVhSs5EYmFrARQ8AUufht3roPERMGQKNOoQdGRJLRWTM/DzmI48spILs26BRQ/BOcuh4QG6dZLA5s0wdKgftnr1Vb9KNdaWLYMrrvBJ1Rln+N6yDjX4J714sU/Q9uzxnxX20iZ79sDdd/teyjZtoH17yMnZ+2v79tC8ed30LhUXw9y5FUnY9Ol7L7bo0cMnYOXJ2FFHQb161fsZO3bA0qV7J2/lCdzy5T65LpeZCZ07V97rlpsL9auxjbOSM5G6ULobVrwGK9+G45/1vWfz74ey3dD+bGjeN9i+8iSTqslZlaZ8FzIaw4kvxSaokNu0ySdon3/u56CddVZsPtc5n4jdcov//sEH/X6ftfmn+9VXPkHbvRvefdcnEGG0YIFfFTtnDpx0EmzfDoWFlRfXzcqqSNQqS95ycqB16+oVP3bOJ8XRPWKzZsGuXf56q1YVvWEDBkD//r6XK55KSvyQ6b69beVH9Py1Pn18InmolJyJBOXD78OK1wEHDXOg/QjoeCG0PiXoyBKekrNKlO6C9Mzaf06C2LTJ78U5d64f8h0+vHaft2KF7y2bNMnvUDB6NHTqFJtYCwp8grZzp0/Qjj46Np8bC87BI4/4+YyNG8Pf/773cPGePbBqlU/Uiooqvu77urh4789NS/O9b/smbeVf27b1PVPRydjatf69mZnwne/snYx16hSu32+dg9WrKxK1jAyf3B4qJWciQdq1BorehqKxsGoidL4UBjwGrgyWvwRthkJmy6CjTDhKziKcg93rU/bv0MaNcNppvqTFmDF+wUB1OQdPPw033eTrlP3xj/Czn8VublG5JUt8grZ9u0/Q+vaN7efXxKpVfrHD+PE+uR09ev8VqIeirAzWrds7YassiduypfL3H3nk3olYnz7VH55MNErORMKiZCeUbPNbSK2fARP6++HPlif4oc+cs6Fp96CjTAhKziK+nQyTh8Gpk6DVybENLEFs2OA3S1+40G/7NHToob+3qAhGjfKlOk45xScnRxwRv1iXLvUJ2tatPkHr1y9+P+tgXn/d18rbvt2Xmrj66vj3TG3dWpGwFRX5Cfb9+/tafammqjZMy8lE6lpGlk/MAFp8B4bNgF6/9vt7zrkd3urha1UBFG/TrgRycIsehnpNoEX/oCMJTIsWvvZbjx5+OG7SpIO/xzlfq6x3b1/24q9/9RP245mYgf/8KVN8aZAhQ/ycqrq2dasfvj3vPD9UOGsWXHNN3QwZNmni/5yGDPH7pp52WmomZgei5EwkSGY+QTvqbhg+y6+yy38EWg701xf+EV5rDZ9cCt+8klRV3yVGti+Hojegy0994p/CDj/cJ2jdusHZZ/teqaqsXu1rpF16qa8/9vnncP31sR/GrErnzj5Ba9rUJykzZ9bNzwWYOtUPpz79tN8TdOrUKlb/SmCUnImESaOO0O2aiv9kW50M7c7yqz8/+gG82hI+OM//yi9xZWbDzGyRmRWY2S+ruOcCM1tgZvPN7J9R5y8zs68ix2VxDfSrx/zXvGvi+mMSRcuWPinLy/PlNSZP3vu6c34fzF69YOJEP5T3/vv+/rpWnqA1a+Z7j+I9K6e4GO66y+9TWlrqn/u++6pX+kHqhnYIEAmzNkP8UVYC66b6BQWluyvGHj7+ITTu7HvfDuvt66ulpQcbcxIws3TgEeB0oBD4zMzGOucWRN2TB9wBDHTObTSzVpHzLYC7gHzAATMj790Y80DLimHJaMg51yf2AlQkaIMH+/Ia48b5fS7XrPFzql57zU86f+YZP7wWpNxcn6ANHuwTtEmT/PyrWPvqK/jRj/yKyEsvhYce0lBimCk5E0kEaRnQ6iR/lCvdBTtWwDcv+VWfAOlZ0OtOXyHelcGqSdCsN2S1C9f68/AbABQ455YCmNlLwDlA9C6JVwKPlCddzrnySlBnAJOccxsi750EDANejHmUafVg6CfgSmP+0YkuO9vPHytP0H71K/jf//UrBf/wB1/DLD0kv8d06rR3gjZxok8eY8E5eOopuPFG30P28stwwQWx+WyJn7gOa5rZaDNbY2bzqrhuZvZQZNhgrpkdE3Wt7oYFRBJReiac/gH8YAuc8RkcOxryroZmkfLj276GKcNgTA68ejhMOhk+u9avEJWDaQ+siPq+MHIuWjegm5l9bGbTzGxYNd4bO026anVvFVq18glap04+OcvN9RPfb789PIlZuY4dfYLWsqVfaTptWu0/c+1aP6/uyiv9/qtz5yoxSxTx7jl7Bvgb8GwV14cDeZHjWOAx4Ng6HRYQSXQZjeDwfH9Ey2rnt5DaPA82feG/LnvBD5Meng9rPoCPL/bJXLPefli0/Gt6g0AeJcFk4NuuQUAO8IGZVWtjHjMbBYwC6NixmsOSaz+GBQ9A/t+0TdgBtG7t51a9+y6cf371qtXXtQ4dfKyDBvkEbcIEOP74mn3WuHG+dtnGjX6HgxtuqLvFDlJ7cf1r6pz7wMxyD3DLOcCzzhdbm2ZmzcysLb6xq5thAZFklZHldyGI3onAuYoh0Iwm0PpU2PwFLJrst5gCGDoNWh4Laz6C1ZFh0cP6+B6atBD/zxZbRUB0xpMTORetEJjunCsGvjazxfhkrQjfhkW/d0plP8Q59wTwBPg6Z9WKcNFDPsFu0KJab0tF2dkwcmTQURyanJyKBO2MM3xh2BNOOPT379jhq/w/+qgvETJxYni3ipKqBd3SVtX9X7fDAiKpwgwsMp7Toh+cEOnULiuBbUt8D1uz3v7c+mkw/7d7z2dr3g8GvQ31m8GezX4fx+RcgPAZkGdmnfHJ1kjg4n3uGQNcBDxtZi3xw5xLgSXAfWbWPHLfUPzCgdjZUeT3cO1+ve85laTSvn3FHLTyBG3gwIO/b+ZMv3XQokV+p4P77vNbIEniCTo5q7VaDQuIiJeW4ectRc9dOvJWyLsWtiz0SdvGObDlS6gXWeI18wZY8Sq0OAZa5Pvj8P6+hy3BOedKzOw6YAKQDox2zs03s3uAGc65sZFrQ81sAVAK3OacWw9gZvfiEzyAe8pHAWKm4P/8IoBu18b0YyU8KkvQTjyx8ntLS+GBB+A3v/HDuJMm+YUFkriCTs6qGjqom2EBETmwjKxI8nUMsM+6nA7f91Xp18+AxY/4YdGmR8J3IwsalzzlE7kW+dAoZLsVHwLn3Dhg3D7nfhP12gE3R4593zsaGB2XwEp3++Ss3Vm+dIokrXbtKhK0YcPgnXfgpJP2vmfZMl8i46OP4Ac/gMcf97slSGILOjkbC1wXWaZ+LLDZObfKzCYQ72EBEamdnBH+AF9va/N82LOp4vrc38DOlf51g8N9ktbxQuhyed3HmkzKin3B2VaDgo5E6kDbthUJ2vDh8Pbbfv9P5+D55+HaSOfps8/CJZck3O9AUoW4Jmdm9iK+B6ylmRXiV2DWA3DOPY7/rfRMoADYAVweubYh7sMCIhI7afWged+9z5291A+Hbpjhj/UzYOtX/lrJTnirGzQ7umI4tEU+ZLWu+9gTTb3G0OeuoKOQOtSmjd/p4NRT4cwz4YUX/C4H//qXH+p87jlfJkSSR7xXa150kOsOqHTSRFyHBUQk/tIb7F/io3zbqZJt0HqIT9pWjsNXzMGXhdA8KpH9RCdo3/ueLwly333hrNkmtRf0sKaIpJLyMZfMbDj+Gf+6eJtfbLDhMw3ViRxA69Y+Qbv3XrjsMsjPP/h7JDEpORORYNVrDK1O9IeIHFCrVvDww0FHIfGmesEiIiIiIaLkTERERCRElJyJiIiIhIiSMxEREZEQUXImIiIiEiJKzkRERERCRMmZiIiISIgoORMREREJEXPl26kkATNbCyw/xNtbAuviGE5dSqZngeR6Hj1LfHVyzmUHHUQspHD7Bcn1PHqWcArrs1TahiV9Cgf0AAAFIklEQVRVclYdZjbDOZcUm18k07NAcj2PnkXiIdn+LJLpefQs4ZRoz6JhTREREZEQUXImIiIiEiKpnJw9EXQAMZRMzwLJ9Tx6FomHZPuzSKbn0bOEU0I9S8rOORMREREJo1TuORMREREJnZRMzsxsmJktMrMCM/tl0PHUlJl1MLPJZrbAzOab2Q1Bx1RbZpZuZrPN7K2gY6kNM2tmZq+Y2ZdmttDMjg86ptows5sif8fmmdmLZpYZdEypSu1XeCVL+wXJ1YYlYvuVcsmZmaUDjwDDgZ7ARWbWM9ioaqwEuMU51xM4Drg2gZ+l3A3AwqCDiIG/AuOdcz2Ao0ngZzKz9sD1QL5zrjeQDowMNqrUpPYr9JKl/YIkacMStf1KueQMGAAUOOeWOuf2AC8B5wQcU40451Y552ZFXm/F/+NpH2xUNWdmOcBZwN+DjqU2zOww4GTgKQDn3B7n3KZgo6q1DCDLzDKAhsDKgONJVWq/QipZ2i9IyjYs4dqvVEzO2gMror4vJIEbhHJmlgv0A6YHG0mt/AW4HSgLOpBa6gysBZ6ODHH83cwaBR1UTTnnioA/Ad8Aq4DNzrmJwUaVstR+hVeytF+QRG1YorZfqZicJR0zawy8CtzonNsSdDw1YWbfBdY452YGHUsMZADHAI855/oB24FEnhvUHN870xloBzQys0uCjUqShdqvUEqaNixR269UTM6KgA5R3+dEziUkM6uHb9hecM69FnQ8tTAQONvMluGHak41s+eDDanGCoFC51x5L8Ar+IYuUZ0GfO2cW+ucKwZeA04IOKZUpfYrnJKp/YLkasMSsv1KxeTsMyDPzDqbWX38xMCxAcdUI2Zm+DkBC51zDwYdT2045+5wzuU453LxfybvOedC/9tNZZxzq4EVZtY9cmoIsCDAkGrrG+A4M2sY+Ts3hASdHJwE1H6FUDK1X5B0bVhCtl8ZQQdQ15xzJWZ2HTABv2pjtHNufsBh1dRA4EfAF2Y2J3LuTufcuABjEu/nwAuR/0CXApcHHE+NOeemm9krwCz8CrvZJFi17WSh9kvqUFK0YYnafmmHABEREZEQScVhTREREZHQUnImIiIiEiJKzkRERERCRMmZiIiISIgoORMREREJESVnEhpmVmpmc6KOmFWkNrNcM5sXq88TEdmX2jCJlZSrcyahttM51zfoIEREakhtmMSEes4k9MxsmZk9YGZfmNmnZtY1cj7XzN4zs7lm9q6ZdYycb21mr5vZ55GjfKuOdDN70szmm9lEM8sK7KFEJGWoDZPqUnImYZK1z5DAhVHXNjvn+gB/A/4SOfcw8A/n3FHAC8BDkfMPAe87547G7wdXXkE9D3jEOdcL2AR8P87PIyKpRW2YxIR2CJDQMLNtzrnGlZxfBpzqnFsa2Sh5tXPucDNbB7R1zhVHzq9yzrU0s7VAjnNud9Rn5AKTnHN5ke9/AdRzzv02/k8mIqlAbZjEinrOJFG4Kl5Xx+6o16VozqWI1B21YXLIlJxJorgw6uvUyOtPgJGR1z8EPoy8fhe4GsDM0s3ssLoKUkSkCmrD5JAp65YwyTKzOVHfj3fOlS9Fb25mc/G/OV4UOfdz4Gkzuw1YC1weOX8D8ISZXYH/7fJqYFXcoxeRVKc2TGJCc84k9CLzNfKdc+uCjkVEpLrUhkl1aVhTREREJETUcyYiIiISIuo5ExEREQkRJWciIiIiIaLkTERERCRElJyJiIiIhIiSMxEREZEQUXImIiIiEiL/H3RJEaUZbPIgAAAAAElFTkSuQmCC\n"},"metadata":{"needs_background":"light"}}],"source":["plt.figure(figsize=(10,4))\n","plt.subplot(1,2,1)\n","plt.plot(cnn_bilstm_history.history[\"loss\"], label=\"BiLSTM-attention-training\", color=\"orange\", linestyle=\"dashed\")\n","plt.plot(cnn_bilstm_history.history[\"val_loss\"], label=\"BiLSTM-attention-validation\", color=\"blue\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","\n","plt.subplot(1,2,2)\n","plt.plot(cnn_bilstm_history.history[\"accuracy\"], label=\"BiLSTM-attention-training\", color=\"orange\", linestyle=\"dashed\")\n","plt.plot(cnn_bilstm_history.history[\"val_accuracy\"], label=\"BiLSTM-attention-validation\", color=\"blue\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\") # not representative of macro F1 though\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RXO1OP6HPugD"},"outputs":[],"source":["model = keras.models.load_model(os.path.join(\"models\", best_model_name),\n","                                custom_objects={\"macro_soft_f1\": macro_soft_f1, \n","                                                \"SeqWeightedAttention\": SeqWeightedAttention,\n","                                                'LDAM':LDAM})\n","\n","\n","\n","\n","\n","# actual test set predictions\n","ts_pred_ac = np.argmax(model.predict(test_feats_matrix_ac),\n","                    axis=1)\n","\n","write_predictions('data/lstm_3_test_pred.csv', ts_pred_ac+1, test_ids_ac) # change from 0-4 back to 1-5"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"RNN.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"nbformat":4,"nbformat_minor":0}